{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8f30050",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0db32867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import concurrent.futures\n",
    "import json\n",
    "import re\n",
    "from random import Random\n",
    "\n",
    "from openai import AsyncOpenAI, OpenAI\n",
    "from tqdm import tqdm\n",
    "\n",
    "from virtual_lab.agent import Agent\n",
    "from virtual_lab.constants import CONSISTENT_TEMPERATURE, DEFAULT_FINETUNING_EPOCHS\n",
    "from virtual_lab.run_meeting import run_meeting\n",
    "from virtual_lab.utils import (\n",
    "    async_get_messages,\n",
    "    compute_finetuning_cost,\n",
    "    compute_token_cost,\n",
    "    count_tokens,\n",
    "    get_pubmed_central_article,\n",
    ")\n",
    "\n",
    "from interpretability_constants import (\n",
    "    background_prompt,\n",
    "    project_specific_prompt,\n",
    "    discussions_phase_to_dir,\n",
    "    model as base_model,\n",
    "    model_mini as base_model_mini,\n",
    "    generic_agent,\n",
    "    computational_linguist,\n",
    "    clinical_informatics_specialist,\n",
    "    data_visualization_expert,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fad5e04",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2667637b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "finetuning_dir = discussions_phase_to_dir[\"finetuning\"]\n",
    "papers_dir = finetuning_dir / \"papers\"\n",
    "summaries_dir = finetuning_dir / \"summaries\"\n",
    "qa_dir = finetuning_dir / \"qa_pairs\"\n",
    "\n",
    "for dir_path in [finetuning_dir, papers_dir, summaries_dir, qa_dir]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "client = OpenAI()\n",
    "async_client = AsyncOpenAI()\n",
    "num_concurrent = 10\n",
    "max_tokens = 250000\n",
    "\n",
    "# Topic to agent mapping\n",
    "topic_to_agent = {\n",
    "    # Computational Linguist topics (for BioBERT/ClinicalBERT + SHAP/LIME)\n",
    "    \"BioBERT fine-tuning for clinical concept extraction\": computational_linguist,\n",
    "    \"ClinicalBERT performance on phenotype definition tasks\": computational_linguist,\n",
    "    \"Interpretability techniques in biomedical NLP\": computational_linguist,\n",
    "    \"Token-level attribution with SHAP and LIME in medical LLMs\": computational_linguist,\n",
    "    \"Few-shot learning and domain adaptation in clinical NLP\": computational_linguist,\n",
    "\n",
    "    # Clinical Informatics Specialist topics (for SNOMED CT)\n",
    "    \"SNOMED CT integration in phenotyping pipelines\": clinical_informatics_specialist,\n",
    "    \"Mapping free-text clinical concepts to standardized vocabularies\": clinical_informatics_specialist,\n",
    "    \"Use of terminology servers for phenotype grounding\": clinical_informatics_specialist,\n",
    "    \"Semantic drift and disambiguation in EHR concept mapping\": clinical_informatics_specialist,\n",
    "    \"Interoperability of phenotyping tools with EHR standards\": clinical_informatics_specialist,\n",
    "\n",
    "    # Data Visualization Expert topics (for Plotly/Dash)\n",
    "    \"Visual analytics for clinical decision support\": data_visualization_expert,\n",
    "    \"Dashboard design for LLM interpretability in healthcare\": data_visualization_expert,\n",
    "    \"Visualization of token attribution in NLP models\": data_visualization_expert,\n",
    "    \"Human-AI interaction through visual explanation tools\": data_visualization_expert,\n",
    "    \"Customizing interpretability interfaces for clinician roles\": data_visualization_expert,\n",
    "}\n",
    "\n",
    "# Agent to topics mapping\n",
    "agent_to_topics = {agent: [] for agent in topic_to_agent.values()}\n",
    "\n",
    "for topic, agent in topic_to_agent.items():\n",
    "    agent_to_topics[agent].append(topic)\n",
    "\n",
    "%autoawait asyncio\n",
    "\n",
    "async def run_query(\n",
    "    semaphore: asyncio.Semaphore, agent: Agent, query: str, model: str = base_model\n",
    ") -> str:\n",
    "    \"\"\"Run a query using the model.\n",
    "\n",
    "    :param semaphore: Semaphore to limit the number of concurrent requests.\n",
    "    :param agent: Agent to use for the query.\n",
    "    :param query: Query to run.\n",
    "    :param model: Model to use for the query.\n",
    "    :return: Response from the model.\n",
    "    \"\"\"\n",
    "    async with semaphore:\n",
    "        assistant = await async_client.beta.assistants.create(\n",
    "            name=agent.title, instructions=agent.prompt, model=model\n",
    "        )\n",
    "        thread = await async_client.beta.threads.create()\n",
    "        await async_client.beta.threads.messages.create(\n",
    "            thread_id=thread.id, role=\"user\", content=query\n",
    "        )\n",
    "        await async_client.beta.threads.runs.create_and_poll(\n",
    "            thread_id=thread.id,\n",
    "            assistant_id=assistant.id,\n",
    "            model=model,\n",
    "            temperature=CONSISTENT_TEMPERATURE,\n",
    "        )\n",
    "        messages = await async_get_messages(client=async_client, thread_id=thread.id)\n",
    "        response = messages[-1][\"content\"][0][\"text\"][\"value\"]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d82ef4",
   "metadata": {},
   "source": [
    "## PubMed Central search queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004c6c77",
   "metadata": {},
   "source": [
    "Use the agents to generate PubMed Central search queries by topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d05eabe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŸ¡ Starting meeting BioBERT_fine-tuning_for_clinical_concept_extraction_queries\n",
      "DEBUGGING: Individual meeting members = [Computational Linguist, Scientific Critic]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Team:   0%|          | 0/2 [00:07<?, ?it/s]1 [00:00<?, ?it/s]\n",
      "Rounds (+ Final Round): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input token count: 370\n",
      "Output token count: 218\n",
      "Tool token count: 0\n",
      "Max token length: 588\n",
      "Cost: $0.00\n",
      "Time: 0:09\n",
      "âœ… Finished meeting BioBERT_fine-tuning_for_clinical_concept_extraction_queries\n",
      "ðŸŸ¡ Starting meeting ClinicalBERT_performance_on_phenotype_definition_tasks_queries\n",
      "DEBUGGING: Individual meeting members = [Computational Linguist, Scientific Critic]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Team:   0%|          | 0/2 [00:08<?, ?it/s]1 [00:00<?, ?it/s]\n",
      "Rounds (+ Final Round): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:08<00:00,  8.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input token count: 358\n",
      "Output token count: 216\n",
      "Tool token count: 0\n",
      "Max token length: 574\n",
      "Cost: $0.00\n",
      "Time: 0:09\n",
      "âœ… Finished meeting ClinicalBERT_performance_on_phenotype_definition_tasks_queries\n",
      "ðŸŸ¡ Starting meeting Interpretability_techniques_in_biomedical_NLP_queries\n",
      "DEBUGGING: Individual meeting members = [Computational Linguist, Scientific Critic]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Team:   0%|          | 0/2 [00:09<?, ?it/s]1 [00:00<?, ?it/s]\n",
      "Rounds (+ Final Round): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input token count: 358\n",
      "Output token count: 178\n",
      "Tool token count: 0\n",
      "Max token length: 536\n",
      "Cost: $0.00\n",
      "Time: 0:10\n",
      "âœ… Finished meeting Interpretability_techniques_in_biomedical_NLP_queries\n",
      "ðŸŸ¡ Starting meeting Token-level_attribution_with_SHAP_and_LIME_in_medical_LLMs_queries\n",
      "DEBUGGING: Individual meeting members = [Computational Linguist, Scientific Critic]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Team:   0%|          | 0/2 [00:10<?, ?it/s]1 [00:00<?, ?it/s]\n",
      "Rounds (+ Final Round): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input token count: 400\n",
      "Output token count: 223\n",
      "Tool token count: 0\n",
      "Max token length: 623\n",
      "Cost: $0.00\n",
      "Time: 0:11\n",
      "âœ… Finished meeting Token-level_attribution_with_SHAP_and_LIME_in_medical_LLMs_queries\n",
      "ðŸŸ¡ Starting meeting Few-shot_learning_and_domain_adaptation_in_clinical_NLP_queries\n",
      "DEBUGGING: Individual meeting members = [Computational Linguist, Scientific Critic]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Team:   0%|          | 0/2 [00:06<?, ?it/s]1 [00:00<?, ?it/s]\n",
      "Rounds (+ Final Round): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input token count: 376\n",
      "Output token count: 204\n",
      "Tool token count: 0\n",
      "Max token length: 580\n",
      "Cost: $0.00\n",
      "Time: 0:08\n",
      "âœ… Finished meeting Few-shot_learning_and_domain_adaptation_in_clinical_NLP_queries\n",
      "ðŸŸ¡ Starting meeting SNOMED_CT_integration_in_phenotyping_pipelines_queries\n",
      "DEBUGGING: Individual meeting members = [Clinical Informatics Specialist, Scientific Critic]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Team:   0%|          | 0/2 [00:08<?, ?it/s]1 [00:00<?, ?it/s]\n",
      "Rounds (+ Final Round): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:08<00:00,  8.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input token count: 371\n",
      "Output token count: 217\n",
      "Tool token count: 0\n",
      "Max token length: 588\n",
      "Cost: $0.00\n",
      "Time: 0:09\n",
      "âœ… Finished meeting SNOMED_CT_integration_in_phenotyping_pipelines_queries\n",
      "ðŸŸ¡ Starting meeting Mapping_free-text_clinical_concepts_to_standardized_vocabularies_queries\n",
      "DEBUGGING: Individual meeting members = [Clinical Informatics Specialist, Scientific Critic]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Team:   0%|          | 0/2 [00:08<?, ?it/s]1 [00:00<?, ?it/s]\n",
      "Rounds (+ Final Round): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:08<00:00,  8.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input token count: 377\n",
      "Output token count: 188\n",
      "Tool token count: 0\n",
      "Max token length: 565\n",
      "Cost: $0.00\n",
      "Time: 0:09\n",
      "âœ… Finished meeting Mapping_free-text_clinical_concepts_to_standardized_vocabularies_queries\n",
      "ðŸŸ¡ Starting meeting Use_of_terminology_servers_for_phenotype_grounding_queries\n",
      "DEBUGGING: Individual meeting members = [Clinical Informatics Specialist, Scientific Critic]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Team:   0%|          | 0/2 [00:08<?, ?it/s]1 [00:00<?, ?it/s]\n",
      "Rounds (+ Final Round): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:08<00:00,  8.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input token count: 359\n",
      "Output token count: 204\n",
      "Tool token count: 0\n",
      "Max token length: 563\n",
      "Cost: $0.00\n",
      "Time: 0:11\n",
      "âœ… Finished meeting Use_of_terminology_servers_for_phenotype_grounding_queries\n",
      "ðŸŸ¡ Starting meeting Semantic_drift_and_disambiguation_in_EHR_concept_mapping_queries\n",
      "DEBUGGING: Individual meeting members = [Clinical Informatics Specialist, Scientific Critic]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Team:   0%|          | 0/2 [00:07<?, ?it/s]1 [00:00<?, ?it/s]\n",
      "Rounds (+ Final Round): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input token count: 389\n",
      "Output token count: 197\n",
      "Tool token count: 0\n",
      "Max token length: 586\n",
      "Cost: $0.00\n",
      "Time: 0:09\n",
      "âœ… Finished meeting Semantic_drift_and_disambiguation_in_EHR_concept_mapping_queries\n",
      "ðŸŸ¡ Starting meeting Interoperability_of_phenotyping_tools_with_EHR_standards_queries\n",
      "DEBUGGING: Individual meeting members = [Clinical Informatics Specialist, Scientific Critic]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Team:   0%|          | 0/2 [00:06<?, ?it/s]1 [00:00<?, ?it/s]\n",
      "Rounds (+ Final Round): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input token count: 383\n",
      "Output token count: 211\n",
      "Tool token count: 0\n",
      "Max token length: 594\n",
      "Cost: $0.00\n",
      "Time: 0:08\n",
      "âœ… Finished meeting Interoperability_of_phenotyping_tools_with_EHR_standards_queries\n",
      "ðŸŸ¡ Starting meeting Visual_analytics_for_clinical_decision_support_queries\n",
      "DEBUGGING: Individual meeting members = [Data Visualization Expert, Scientific Critic]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Team:   0%|          | 0/2 [00:07<?, ?it/s]1 [00:00<?, ?it/s]\n",
      "Rounds (+ Final Round): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input token count: 351\n",
      "Output token count: 173\n",
      "Tool token count: 0\n",
      "Max token length: 524\n",
      "Cost: $0.00\n",
      "Time: 0:08\n",
      "âœ… Finished meeting Visual_analytics_for_clinical_decision_support_queries\n",
      "ðŸŸ¡ Starting meeting Dashboard_design_for_LLM_interpretability_in_healthcare_queries\n",
      "DEBUGGING: Individual meeting members = [Data Visualization Expert, Scientific Critic]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Team:   0%|          | 0/2 [00:08<?, ?it/s]1 [00:00<?, ?it/s]\n",
      "Rounds (+ Final Round): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:08<00:00,  8.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input token count: 369\n",
      "Output token count: 245\n",
      "Tool token count: 0\n",
      "Max token length: 614\n",
      "Cost: $0.00\n",
      "Time: 0:09\n",
      "âœ… Finished meeting Dashboard_design_for_LLM_interpretability_in_healthcare_queries\n",
      "ðŸŸ¡ Starting meeting Visualization_of_token_attribution_in_NLP_models_queries\n",
      "DEBUGGING: Individual meeting members = [Data Visualization Expert, Scientific Critic]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Team:   0%|          | 0/2 [00:07<?, ?it/s]1 [00:00<?, ?it/s]\n",
      "Rounds (+ Final Round): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input token count: 363\n",
      "Output token count: 201\n",
      "Tool token count: 0\n",
      "Max token length: 564\n",
      "Cost: $0.00\n",
      "Time: 0:08\n",
      "âœ… Finished meeting Visualization_of_token_attribution_in_NLP_models_queries\n",
      "ðŸŸ¡ Starting meeting Human-AI_interaction_through_visual_explanation_tools_queries\n",
      "DEBUGGING: Individual meeting members = [Data Visualization Expert, Scientific Critic]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Team:   0%|          | 0/2 [00:07<?, ?it/s]1 [00:00<?, ?it/s]\n",
      "Rounds (+ Final Round): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input token count: 363\n",
      "Output token count: 183\n",
      "Tool token count: 0\n",
      "Max token length: 546\n",
      "Cost: $0.00\n",
      "Time: 0:08\n",
      "âœ… Finished meeting Human-AI_interaction_through_visual_explanation_tools_queries\n",
      "ðŸŸ¡ Starting meeting Customizing_interpretability_interfaces_for_clinician_roles_queries\n",
      "DEBUGGING: Individual meeting members = [Data Visualization Expert, Scientific Critic]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Team:   0%|          | 0/2 [00:08<?, ?it/s]1 [00:00<?, ?it/s]\n",
      "Rounds (+ Final Round): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:08<00:00,  8.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input token count: 369\n",
      "Output token count: 209\n",
      "Tool token count: 0\n",
      "Max token length: 578\n",
      "Cost: $0.00\n",
      "Time: 0:09\n",
      "âœ… Finished meeting Customizing_interpretability_interfaces_for_clinician_roles_queries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for topic, agent in topic_to_agent.items():\n",
    "    save_name = f\"{topic.replace(' ', '_')}_queries\"\n",
    "    save_path = finetuning_dir / f\"{save_name}.json\"\n",
    "\n",
    "    if save_path.exists():\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        print(f\"ðŸŸ¡ Starting meeting {save_name}\")\n",
    "        run_meeting(\n",
    "            meeting_type=\"individual\",\n",
    "            team_member=agent,\n",
    "            agenda=f\"\"\"{background_prompt} {project_specific_prompt}\n",
    "                You are responsible for understanding the topic {topic} in the context of designing an LLM-based interpretability pipeline for electronic phenotype definition.\n",
    "                You need to fine-tune yourself on the relevant literature on {topic} to improve your ability to contribute effectively to building a transparent, clinically grounded, and visually intuitive interpretability tool.\n",
    "                Please write out a series of five distinct search queries that you want to run to find relevant scientific papers on {topic}. Include both general queries about {topic} and queries that specifically relate {topic} to LLM interpretability, phenotype definition, clinical applications, and clinician trust.\n",
    "                Please provide the queries in Python syntax as a list of double-quoted strings.\"\"\",\n",
    "            agenda_questions=(\n",
    "                f\"What are the queries that you want to perform to identify the relevant literature on {topic} (as a list of double-quoted strings in Python syntax)?\",\n",
    "            ),\n",
    "            save_dir=finetuning_dir,\n",
    "            save_name=save_name,\n",
    "            temperature=CONSISTENT_TEMPERATURE,\n",
    "        )\n",
    "        print(f\"âœ… Finished meeting {save_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Meeting {save_name} failed with error: {e}\")\n",
    "\n",
    "\n",
    "#extract query from agent responses\n",
    "\n",
    "# Set up regex pattern for extracting queries\n",
    "query_list_pattern = re.compile(r'\\[\\s*(\".*?\"\\s*(,\\s*\".*?\"\\s*)*)?,?\\s*\\]')\n",
    "\n",
    "topic_to_queries = {}\n",
    "\n",
    "for topic, agent in topic_to_agent.items():\n",
    "    # Get query path for topic\n",
    "    query_path = finetuning_dir / f\"{topic.replace(' ', '_')}_queries.json\"\n",
    "\n",
    "    # Load query discussion\n",
    "    with open(query_path) as f:\n",
    "        query_discussion = json.load(f)\n",
    "\n",
    "    # Extract queries\n",
    "    query_message = query_discussion[-1][\"message\"]\n",
    "    pattern_result = query_list_pattern.search(query_message)\n",
    "\n",
    "    # Check if pattern is matched\n",
    "    if pattern_result is None:\n",
    "        print(f\"No queries found for {query_path}\")\n",
    "        continue\n",
    "\n",
    "    # Extract queries\n",
    "    queries = json.loads(pattern_result.group())\n",
    "    topic_to_queries[topic] = queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c7ce16",
   "metadata": {},
   "source": [
    "## PubMed Central papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c42a3c3",
   "metadata": {},
   "source": [
    "Have the agents find papers on PubMed Central using the search queries (100 papers per query) and evaluate them based on their abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb71d789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŸ¡ Starting meeting Interpretability_techniques_in_biomedical_NLP_papers_2\n",
      "DEBUGGING: Individual meeting members = [Computational Linguist, Scientific Critic]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rounds (+ Final Round):   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching PubMed Central for 5 articles (abstracts) with query: \"Interpretability techniques in biomedical NLP\"\n",
      "Found 5 articles on PubMed Central\n",
      "Searching PubMed Central for 5 articles (abstracts) with query: \"LLM interpretability in electronic phenotype definition\"\n",
      "Found 5 articles on PubMed Central\n",
      "Searching PubMed Central for 5 articles (abstracts) with query: \"Interpretability techniques in clinical NLP applications\"\n",
      "Found 5 articles on PubMed Central\n",
      "Searching PubMed Central for 5 articles (abstracts) with query: \"Building clinician trust with NLP interpretability\"\n",
      "Found 5 articles on PubMed Central\n",
      "Searching PubMed Central for 5 articles (abstracts) with query: \"SNOMED CT integration in NLP interpretability\"\n",
      "Found 5 articles on PubMed Central\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Team:   0%|          | 0/2 [00:28<?, ?it/s]\n",
      "Rounds (+ Final Round): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:28<00:00, 28.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input token count: 363\n",
      "Output token count: 327\n",
      "Tool token count: 10,146\n",
      "Max token length: 690\n",
      "Cost: $0.03\n",
      "Time: 0:30\n",
      "âœ… Finished meeting Interpretability_techniques_in_biomedical_NLP_papers_2\n",
      "ðŸŸ¡ Starting meeting Use_of_terminology_servers_for_phenotype_grounding_papers_2\n",
      "DEBUGGING: Individual meeting members = [Clinical Informatics Specialist, Scientific Critic]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rounds (+ Final Round):   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching PubMed Central for 5 articles (abstracts) with query: \"Use of terminology servers for phenotype grounding\"\n",
      "Found 5 articles on PubMed Central\n",
      "Searching PubMed Central for 5 articles (abstracts) with query: \"Terminology servers and LLM interpretability in phenotype definition\"\n",
      "Found 1 articles on PubMed Central\n",
      "Searching PubMed Central for 5 articles (abstracts) with query: \"Phenotype grounding using SNOMED CT in clinical applications\"\n",
      "Found 5 articles on PubMed Central\n",
      "Searching PubMed Central for 5 articles (abstracts) with query: \"Building clinician trust through visual interpretability tools in phenotype definitions\"\n",
      "Found 3 articles on PubMed Central\n",
      "Searching PubMed Central for 5 articles (abstracts) with query: \"Machine learning and terminology servers for electronic phenotype definitions\"\n",
      "Found 5 articles on PubMed Central\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Team:   0%|          | 0/2 [00:44<?, ?it/s]\n",
      "Rounds (+ Final Round): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:44<00:00, 44.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input token count: 912\n",
      "Output token count: 688\n",
      "Tool token count: 5,791\n",
      "Max token length: 1,052\n",
      "Cost: $0.02\n",
      "Time: 0:46\n",
      "âœ… Finished meeting Use_of_terminology_servers_for_phenotype_grounding_papers_2\n",
      "ðŸŸ¡ Starting meeting Use_of_terminology_servers_for_phenotype_grounding_papers_4\n",
      "DEBUGGING: Individual meeting members = [Clinical Informatics Specialist, Scientific Critic]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rounds (+ Final Round):   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching PubMed Central for 5 articles (abstracts) with query: \"\"terminology servers\" AND \"phenotype grounding\"\"\n",
      "Found 0 articles on PubMed Central\n",
      "Searching PubMed Central for 5 articles (abstracts) with query: \"\"terminology servers\" AND \"LLM interpretability\" AND \"phenotype definition\"\"\n",
      "Found 0 articles on PubMed Central\n",
      "Searching PubMed Central for 5 articles (abstracts) with query: \"\"terminology servers\" AND \"clinical applications\" AND \"phenotype grounding\"\"\n",
      "Found 0 articles on PubMed Central\n",
      "Searching PubMed Central for 5 articles (abstracts) with query: \"\"terminology servers\" AND \"clinician trust\" AND \"phenotype grounding\"\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Team:   0%|          | 0/2 [00:06<?, ?it/s]\n",
      "Rounds (+ Final Round):   0%|          | 0/1 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Meeting Use_of_terminology_servers_for_phenotype_grounding_papers_4 failed with error: 429 Client Error: Too Many Requests for url: https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pmc&term=%22terminology+servers%22+AND+%22clinician+trust%22+AND+%22phenotype+grounding%22&retmax=10&retmode=json&sort=relevance\n",
      "ðŸŸ¡ Starting meeting Semantic_drift_and_disambiguation_in_EHR_concept_mapping_papers_5\n",
      "DEBUGGING: Individual meeting members = [Clinical Informatics Specialist, Scientific Critic]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rounds (+ Final Round):   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching PubMed Central for 2 articles (abstracts) with query: \"Semantic drift in EHR concept mapping\"\n",
      "Found 2 articles on PubMed Central\n",
      "Searching PubMed Central for 2 articles (abstracts) with query: \"Disambiguation in electronic health records\"\n",
      "Found 2 articles on PubMed Central\n",
      "Searching PubMed Central for 2 articles (abstracts) with query: \"Semantic drift and LLM interpretability in phenotype definition\"\n",
      "Found 2 articles on PubMed Central\n",
      "Searching PubMed Central for 2 articles (abstracts) with query: \"EHR concept mapping and clinical applications\"\n",
      "Found 2 articles on PubMed Central\n",
      "Searching PubMed Central for 2 articles (abstracts) with query: \"Building clinician trust in LLM-based phenotype tools\"\n",
      "Found 1 articles on PubMed Central\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Team:   0%|          | 0/2 [00:25<?, ?it/s]\n",
      "Rounds (+ Final Round): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:25<00:00, 25.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input token count: 962\n",
      "Output token count: 506\n",
      "Tool token count: 3,646\n",
      "Max token length: 895\n",
      "Cost: $0.02\n",
      "Time: 0:27\n",
      "âœ… Finished meeting Semantic_drift_and_disambiguation_in_EHR_concept_mapping_papers_5\n",
      "ðŸŸ¡ Starting meeting Visual_analytics_for_clinical_decision_support_papers_4\n",
      "DEBUGGING: Individual meeting members = [Data Visualization Expert, Scientific Critic]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rounds (+ Final Round):   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching PubMed Central for 5 articles (abstracts) with query: \"\"Visual analytics for clinical decision support\"\"\n",
      "Found 0 articles on PubMed Central\n",
      "Searching PubMed Central for 5 articles (abstracts) with query: \"\"Visual analytics for clinical decision support\" AND \"LLM interpretability\"\"\n",
      "Found 0 articles on PubMed Central\n",
      "Searching PubMed Central for 5 articles (abstracts) with query: \"\"Visual analytics for clinical decision support\" AND \"phenotype definition\"\"\n",
      "Found 0 articles on PubMed Central\n",
      "Searching PubMed Central for 5 articles (abstracts) with query: \"\"Visual analytics for clinical decision support\" AND \"clinical applications\"\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Team:   0%|          | 0/2 [00:06<?, ?it/s]\n",
      "Rounds (+ Final Round):   0%|          | 0/1 [00:06<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Meeting Visual_analytics_for_clinical_decision_support_papers_4 failed with error: 429 Client Error: Too Many Requests for url: https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pmc&term=%22Visual+analytics+for+clinical+decision+support%22+AND+%22clinical+applications%22&retmax=10&retmode=json&sort=relevance\n",
      "No papers found for discussions/finetuning/BioBERT_fine-tuning_for_clinical_concept_extraction_papers_4.json\n",
      "Number of papers found for BioBERT fine-tuning for clinical concept extraction: 24\n",
      "No papers found for discussions/finetuning/ClinicalBERT_performance_on_phenotype_definition_tasks_papers_1.json\n",
      "No papers found for discussions/finetuning/ClinicalBERT_performance_on_phenotype_definition_tasks_papers_2.json\n",
      "No papers found for discussions/finetuning/ClinicalBERT_performance_on_phenotype_definition_tasks_papers_3.json\n",
      "No papers found for discussions/finetuning/ClinicalBERT_performance_on_phenotype_definition_tasks_papers_4.json\n",
      "No papers found for discussions/finetuning/ClinicalBERT_performance_on_phenotype_definition_tasks_papers_5.json\n",
      "Number of papers found for ClinicalBERT performance on phenotype definition tasks: 0\n",
      "No papers found for discussions/finetuning/Interpretability_techniques_in_biomedical_NLP_papers_3.json\n",
      "No papers found for discussions/finetuning/Interpretability_techniques_in_biomedical_NLP_papers_4.json\n",
      "No papers found for discussions/finetuning/Interpretability_techniques_in_biomedical_NLP_papers_5.json\n",
      "Number of papers found for Interpretability techniques in biomedical NLP: 23\n",
      "No papers found for discussions/finetuning/Token-level_attribution_with_SHAP_and_LIME_in_medical_LLMs_papers_2.json\n",
      "No papers found for discussions/finetuning/Token-level_attribution_with_SHAP_and_LIME_in_medical_LLMs_papers_5.json\n",
      "Number of papers found for Token-level attribution with SHAP and LIME in medical LLMs: 14\n",
      "No papers found for discussions/finetuning/Few-shot_learning_and_domain_adaptation_in_clinical_NLP_papers_1.json\n",
      "No papers found for discussions/finetuning/Few-shot_learning_and_domain_adaptation_in_clinical_NLP_papers_2.json\n",
      "No papers found for discussions/finetuning/Few-shot_learning_and_domain_adaptation_in_clinical_NLP_papers_3.json\n",
      "No papers found for discussions/finetuning/Few-shot_learning_and_domain_adaptation_in_clinical_NLP_papers_5.json\n",
      "Number of papers found for Few-shot learning and domain adaptation in clinical NLP: 22\n",
      "No papers found for discussions/finetuning/SNOMED_CT_integration_in_phenotyping_pipelines_papers_2.json\n",
      "No papers found for discussions/finetuning/SNOMED_CT_integration_in_phenotyping_pipelines_papers_3.json\n",
      "No papers found for discussions/finetuning/SNOMED_CT_integration_in_phenotyping_pipelines_papers_4.json\n",
      "Number of papers found for SNOMED CT integration in phenotyping pipelines: 18\n",
      "No papers found for discussions/finetuning/Mapping_free-text_clinical_concepts_to_standardized_vocabularies_papers_1.json\n",
      "No papers found for discussions/finetuning/Mapping_free-text_clinical_concepts_to_standardized_vocabularies_papers_2.json\n",
      "No papers found for discussions/finetuning/Mapping_free-text_clinical_concepts_to_standardized_vocabularies_papers_3.json\n",
      "No papers found for discussions/finetuning/Mapping_free-text_clinical_concepts_to_standardized_vocabularies_papers_4.json\n",
      "No papers found for discussions/finetuning/Mapping_free-text_clinical_concepts_to_standardized_vocabularies_papers_5.json\n",
      "Number of papers found for Mapping free-text clinical concepts to standardized vocabularies: 0\n",
      "Missing papers for Use of terminology servers for phenotype grounding\n",
      "No papers found for discussions/finetuning/Semantic_drift_and_disambiguation_in_EHR_concept_mapping_papers_1.json\n",
      "No papers found for discussions/finetuning/Semantic_drift_and_disambiguation_in_EHR_concept_mapping_papers_2.json\n",
      "No papers found for discussions/finetuning/Semantic_drift_and_disambiguation_in_EHR_concept_mapping_papers_3.json\n",
      "No papers found for discussions/finetuning/Semantic_drift_and_disambiguation_in_EHR_concept_mapping_papers_4.json\n",
      "No papers found for discussions/finetuning/Semantic_drift_and_disambiguation_in_EHR_concept_mapping_papers_5.json\n",
      "Number of papers found for Semantic drift and disambiguation in EHR concept mapping: 0\n",
      "No papers found for discussions/finetuning/Interoperability_of_phenotyping_tools_with_EHR_standards_papers_1.json\n",
      "No papers found for discussions/finetuning/Interoperability_of_phenotyping_tools_with_EHR_standards_papers_2.json\n",
      "No papers found for discussions/finetuning/Interoperability_of_phenotyping_tools_with_EHR_standards_papers_3.json\n",
      "No papers found for discussions/finetuning/Interoperability_of_phenotyping_tools_with_EHR_standards_papers_4.json\n",
      "No papers found for discussions/finetuning/Interoperability_of_phenotyping_tools_with_EHR_standards_papers_5.json\n",
      "Number of papers found for Interoperability of phenotyping tools with EHR standards: 0\n",
      "Missing papers for Visual analytics for clinical decision support\n",
      "No papers found for discussions/finetuning/Dashboard_design_for_LLM_interpretability_in_healthcare_papers_2.json\n",
      "No papers found for discussions/finetuning/Dashboard_design_for_LLM_interpretability_in_healthcare_papers_3.json\n",
      "Number of papers found for Dashboard design for LLM interpretability in healthcare: 12\n",
      "Number of papers found for Visualization of token attribution in NLP models: 16\n",
      "No papers found for discussions/finetuning/Human-AI_interaction_through_visual_explanation_tools_papers_1.json\n",
      "No papers found for discussions/finetuning/Human-AI_interaction_through_visual_explanation_tools_papers_4.json\n",
      "No papers found for discussions/finetuning/Human-AI_interaction_through_visual_explanation_tools_papers_5.json\n",
      "Number of papers found for Human-AI interaction through visual explanation tools: 19\n",
      "No papers found for discussions/finetuning/Customizing_interpretability_interfaces_for_clinician_roles_papers_2.json\n",
      "No papers found for discussions/finetuning/Customizing_interpretability_interfaces_for_clinician_roles_papers_5.json\n",
      "Number of papers found for Customizing interpretability interfaces for clinician roles: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for topic, agent in topic_to_agent.items():\n",
    "    for query_num, query in enumerate(topic_to_queries[topic]):\n",
    "        save_name = f\"{topic.replace(' ', '_')}_papers_{query_num + 1}\"\n",
    "        save_path = discussions_phase_to_dir[\"finetuning\"] / f\"{save_name}.json\"\n",
    "\n",
    "        if save_path.exists():\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            print(f\"ðŸŸ¡ Starting meeting {save_name}\")\n",
    "            run_meeting(\n",
    "                meeting_type=\"individual\",\n",
    "                team_member=agent,\n",
    "                agenda=f\"\"\"{background_prompt} {project_specific_prompt}\n",
    "                You are responsible for understanding the topic {topic} in the context of designing an LLM-based interpretability pipeline for electronic phenotype definition.\n",
    "                You need to fine-tune yourself on the relevant literature on {topic} to improve your ability to contribute effectively to building a transparent, clinically grounded, and visually intuitive interpretability tool.\n",
    "                Please write out a series of five distinct search queries that you want to run to find relevant scientific papers on {topic}. Include both general queries about {topic} and queries that specifically relate {topic} to LLM interpretability, phenotype definition, clinical applications, and clinician trust.\n",
    "                Please provide the queries in Python syntax as a list of double-quoted strings.\"\"\",\n",
    "                agenda_questions=(\n",
    "                    \"What are the PMCIDs and titles of the papers you wish to fine-tune yourself on (as a Python dictionary mapping PMCID as a double-quoted string to title as double-quoted string)?\",\n",
    "                ),\n",
    "                save_dir=discussions_phase_to_dir[\"finetuning\"],\n",
    "                save_name=save_name,\n",
    "                temperature=CONSISTENT_TEMPERATURE,\n",
    "                pubmed_search=True,\n",
    "            )\n",
    "            print(f\"âœ… Finished meeting {save_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Meeting {save_name} failed with error: {e}\")\n",
    "\n",
    "# Set up regex pattern for extracting queries\n",
    "pmcid_to_title_pattern = re.compile(\n",
    "    r'\\{\\s*(\".*?\"\\s*:\\s*\".*?\"\\s*(,\\s*\".*?\"\\s*:\\s*\".*?\"\\s*)*)?\\}'\n",
    ")\n",
    "\n",
    "for topic, agent in topic_to_agent.items():\n",
    "    # Set up title to PMC ID dictionary\n",
    "    title_to_pmcid = {}\n",
    "    titles_lower, pmcids = set(), set()\n",
    "    topic_name = topic.replace(\" \", \"_\")\n",
    "\n",
    "    # Get all paper paths for a topic\n",
    "    paper_paths = sorted(finetuning_dir.glob(f\"{topic_name}_papers_*.json\"))\n",
    "\n",
    "    # Check if all papers results are present\n",
    "    if len(paper_paths) != 5:\n",
    "        print(f\"Missing papers for {topic}\")\n",
    "        continue\n",
    "\n",
    "    # Extract PMC IDs and titles from each papers file\n",
    "    for paper_path in paper_paths:\n",
    "        # Load paper discussion\n",
    "        with open(paper_path) as f:\n",
    "            paper_discussion = json.load(f)\n",
    "\n",
    "        # Extract PMC IDs and titles dictionary\n",
    "        paper_message = paper_discussion[1][\"message\"]\n",
    "        pattern_result = pmcid_to_title_pattern.search(paper_message)\n",
    "\n",
    "        # Check if pattern is matched\n",
    "        if pattern_result is None:\n",
    "            print(f\"No papers found for {paper_path}\")\n",
    "            continue\n",
    "\n",
    "        # Extract PMC IDs and titles dictionary\n",
    "        pmcid_to_title = json.loads(pattern_result.group())\n",
    "\n",
    "        # Add PMC IDs and titles to dictionary, avoiding duplicates\n",
    "        for pmcid, title in pmcid_to_title.items():\n",
    "            # Replace en dash and em dash with a hyphen and convert to lowercase\n",
    "            title = title.replace(\"â€“\", \"-\").replace(\"â€”\", \"-\")\n",
    "            title_lower = title.lower()\n",
    "\n",
    "            if title_lower not in titles_lower and pmcid not in pmcids:\n",
    "                title_to_pmcid[title] = pmcid\n",
    "                titles_lower.add(title_lower)\n",
    "                pmcids.add(pmcid)\n",
    "\n",
    "    print(f\"Number of papers found for {topic}: {len(title_to_pmcid):,}\")\n",
    "\n",
    "    # Save title to PMC ID dictionary\n",
    "    with open(finetuning_dir / f\"{topic_name}_title_to_pmcid.json\", \"w\") as f:\n",
    "        json.dump(title_to_pmcid, f, indent=4, sort_keys=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9a7cf26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Skipping: discussions/finetuning/Use_of_terminology_servers_for_phenotype_grounding_title_to_pmcid.json does not exist.\n",
      "âš ï¸ Skipping: discussions/finetuning/Visual_analytics_for_clinical_decision_support_title_to_pmcid.json does not exist.\n",
      "ðŸ“„ Number of unique PMCIDs: 147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading papers: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 147/147 [01:12<00:00,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Number of papers downloaded: 147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assume these directories are already defined\n",
    "# finetuning_dir = Path(\"...\")  # directory where _title_to_pmcid.json files live\n",
    "# papers_dir = Path(\"...\")      # directory where downloaded papers should be saved\n",
    "\n",
    "pmcids = set()\n",
    "\n",
    "# Load all PMCIDs from available topic response files\n",
    "for topic in topic_to_agent:\n",
    "    topic_name = topic.replace(\" \", \"_\")\n",
    "    file_path = finetuning_dir / f\"{topic_name}_title_to_pmcid.json\"\n",
    "\n",
    "    if not file_path.exists():\n",
    "        print(f\"âš ï¸ Skipping: {file_path} does not exist.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        with open(file_path) as f:\n",
    "            title_to_pmcid = json.load(f)\n",
    "            pmcids.update(title_to_pmcid.values())\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error reading {file_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"ðŸ“„ Number of unique PMCIDs: {len(pmcids):,}\")\n",
    "\n",
    "# Download the papers\n",
    "paper_count = 0\n",
    "\n",
    "for pmcid in tqdm(sorted(pmcids), desc=\"Downloading papers\"):\n",
    "    try:\n",
    "        title, content = get_pubmed_central_article(pmcid=pmcid)\n",
    "        if not title:\n",
    "            print(f\"âš ï¸ Skipping PMCID {pmcid}: title is None or empty.\")\n",
    "            continue\n",
    "\n",
    "        with open(papers_dir / f\"{pmcid}.json\", \"w\") as f:\n",
    "            json.dump({\"title\": title, \"content\": content}, f, indent=4, sort_keys=True)\n",
    "\n",
    "        paper_count += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to download or save PMCID {pmcid}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"âœ… Number of papers downloaded: {paper_count:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26511496",
   "metadata": {},
   "source": [
    "## Summarize papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a967cf6e",
   "metadata": {},
   "source": [
    "Define a function to using an agent to summarize a paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879959b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def summarize_paper(\n",
    "    semaphore: asyncio.Semaphore,\n",
    "    agent: Agent,\n",
    "    topic: str,\n",
    "    pmcid: str,\n",
    "    title: str,\n",
    "    content: list[str],\n",
    ") -> tuple[str, str, str]:\n",
    "    \"\"\"Summarize a paper using the model.\n",
    "\n",
    "    :param semaphore: Semaphore to limit the number of concurrent requests.\n",
    "    :param agent: Agent to use for summarization.\n",
    "    :param topic: Topic of interest.\n",
    "    :param pmcid: PMC ID of the paper.\n",
    "    :param title: Title of the paper.\n",
    "    :param content: Content of the paper.\n",
    "    :return: Tuple of PMC ID, title, and summary of the paper.\n",
    "    \"\"\"\n",
    "    # Set up query with paper\n",
    "    query = \"\\n\\n\".join(\n",
    "    [\n",
    "        f'Please summarize in extreme detail the following paper titled \"{title}\". Focus especially on summarizing key insights about the topic \"{topic}\" as it relates to building an LLM-based interpretability pipeline for electronic phenotype definition.'\n",
    "    ]\n",
    "    + content\n",
    "    )\n",
    "\n",
    "    # Run query to get summary\n",
    "    summary = await run_query(\n",
    "        semaphore=semaphore,\n",
    "        agent=agent,\n",
    "        query=query,\n",
    "    )\n",
    "\n",
    "    return pmcid, title, summary\n",
    "\n",
    "#Use the agents to summarize each paper in parallel.\n",
    "for topic, agent in topic_to_agent.items():\n",
    "    topic_name = topic.replace(\" \", \"_\")\n",
    "\n",
    "    # Create save directory\n",
    "    topic_summary_dir = summaries_dir / topic_name\n",
    "    topic_summary_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Load title to PMC ID dictionary\n",
    "    with open(finetuning_dir / f\"{topic_name}_title_to_pmcid.json\") as f:\n",
    "        title_to_pmcid: dict[str, str] = json.load(f)\n",
    "\n",
    "    # Get unique PMC IDs\n",
    "    pmcids = sorted(set((title_to_pmcid.values())))\n",
    "\n",
    "    # Load papers\n",
    "    pmcid_to_paper = {}\n",
    "    for pmcid in pmcids:\n",
    "        paper_path = papers_dir / f\"{pmcid}.json\"\n",
    "\n",
    "        if paper_path.exists():\n",
    "            with open(paper_path) as f:\n",
    "                paper: dict[str, str | list[str]] = json.load(f)\n",
    "                pmcid_to_paper[pmcid] = paper\n",
    "\n",
    "    print(f\"Number of papers loaded for {topic}: {len(pmcid_to_paper):,}\")\n",
    "\n",
    "    # Limit papers by length\n",
    "    pmcid_to_paper = {\n",
    "        pmcid: paper\n",
    "        for pmcid, paper in pmcid_to_paper.items()\n",
    "        if sum(count_tokens(paragraph) for paragraph in paper[\"content\"]) <= max_tokens\n",
    "    }\n",
    "\n",
    "    print(\n",
    "        f\"Number of papers after token limit of {max_tokens:,} for {topic}: {len(pmcid_to_paper):,}\"\n",
    "    )\n",
    "\n",
    "    # Compute input token cost\n",
    "    input_token_count = sum(\n",
    "        count_tokens(paragraph)\n",
    "        for paper in pmcid_to_paper.values()\n",
    "        for paragraph in [paper[\"title\"]] + paper[\"content\"]\n",
    "    )\n",
    "\n",
    "    input_token_cost = compute_token_cost(\n",
    "        model=base_model,\n",
    "        input_token_count=input_token_count,\n",
    "        output_token_count=0,\n",
    "    )\n",
    "\n",
    "    print(f\"Approximate input token cost for {topic} = ${input_token_cost:.2f}\")\n",
    "\n",
    "    # Set up semaphore with the number of concurrent requests\n",
    "    semaphore = asyncio.Semaphore(num_concurrent)\n",
    "\n",
    "    # Create tasks for each paper\n",
    "    tasks = [\n",
    "        asyncio.create_task(\n",
    "            summarize_paper(\n",
    "                semaphore=semaphore,\n",
    "                agent=agent,\n",
    "                topic=topic,\n",
    "                pmcid=pmcid,\n",
    "                title=paper[\"title\"],\n",
    "                content=paper[\"content\"],\n",
    "            )\n",
    "        )\n",
    "        for pmcid, paper in pmcid_to_paper.items()\n",
    "    ]\n",
    "\n",
    "    # Run agent summary for each paper\n",
    "    results = [\n",
    "        (await task) for task in tqdm(asyncio.as_completed(tasks), total=len(tasks))\n",
    "    ]\n",
    "\n",
    "    # Save summaries\n",
    "    for pmcid, title, summary in results:\n",
    "        with open(topic_summary_dir / f\"{pmcid}.json\", \"w\") as f:\n",
    "            json.dump(\n",
    "                {\"pmcid\": pmcid, \"title\": title, \"summary\": summary},\n",
    "                f,\n",
    "                indent=4,\n",
    "                sort_keys=True,\n",
    "            )\n",
    "\n",
    "#Convert the summaries to the format required for fine-tuning, aggregated by topic.\n",
    "for topic, agent in topic_to_agent.items():\n",
    "    # Convert summaries to training data format\n",
    "    training_data = []\n",
    "\n",
    "    topic_name = topic.replace(\" \", \"_\")\n",
    "\n",
    "    # Get summary paths\n",
    "    topic_summary_dir = summaries_dir / topic_name\n",
    "    summary_paths = sorted(topic_summary_dir.glob(\"*.json\"))\n",
    "\n",
    "    for summary_path in summary_paths:\n",
    "        # Load paper summary data\n",
    "        with open(summary_path) as f:\n",
    "            summary_data = json.load(f)\n",
    "\n",
    "        # Extract title and summary\n",
    "        title, summary = summary_data[\"title\"], summary_data[\"summary\"]\n",
    "\n",
    "        # Add example to training data\n",
    "        training_data.append(\n",
    "        {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": agent.prompt},\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f'Please tell me about the paper \"{title}\" and its insights into \"{topic}\" in relation to designing an LLM-based interpretability pipeline for electronic phenotype definition.',\n",
    "                },\n",
    "                {\"role\": \"assistant\", \"content\": summary},\n",
    "            ]\n",
    "        }\n",
    "        )\n",
    "\n",
    "    # Shuffle training data\n",
    "    random = Random(0)\n",
    "    random.shuffle(training_data)\n",
    "\n",
    "    # Count tokens\n",
    "    token_count = sum(\n",
    "        count_tokens(message[\"content\"])\n",
    "        for data in training_data\n",
    "        for message in data[\"messages\"]\n",
    "    )\n",
    "\n",
    "    # Determine finetuning cost\n",
    "    finetuning_cost = compute_finetuning_cost(\n",
    "        model=base_model_mini,\n",
    "        token_count=token_count,\n",
    "        num_epochs=DEFAULT_FINETUNING_EPOCHS,\n",
    "    )\n",
    "\n",
    "    # Print stats\n",
    "    print(f\"Number of paper examples for {topic}: {len(training_data):,}\")\n",
    "    print(f\"Token count for {topic}: {token_count:,}\")\n",
    "    print(f\"Finetuning cost for {topic} using {base_model_mini}: ${finetuning_cost:.2f}\")\n",
    "    print()\n",
    "\n",
    "    # Save training data in jsonl format\n",
    "    with open(\n",
    "        finetuning_dir / f\"{topic.replace(' ', '_')}_training_data.jsonl\", \"w\"\n",
    "    ) as f:\n",
    "        f.write(\"\\n\".join(json.dumps(example) for example in training_data))\n",
    "\n",
    "#Convert the summaries to the format required for fine-tuning, aggregated by agent.\n",
    "for agent, topics in agent_to_topics.items():\n",
    "    # Convert summaries to training data format\n",
    "    training_data = []\n",
    "\n",
    "    for topic in topics:\n",
    "        topic_name = topic.replace(\" \", \"_\")\n",
    "\n",
    "        # Get summary paths\n",
    "        topic_summary_dir = summaries_dir / topic_name\n",
    "        summary_paths = sorted(topic_summary_dir.glob(\"*.json\"))\n",
    "\n",
    "        for summary_path in summary_paths:\n",
    "            # Load paper summary data\n",
    "            with open(summary_path) as f:\n",
    "                summary_data = json.load(f)\n",
    "\n",
    "            # Extract title and summary\n",
    "            title, summary = summary_data[\"title\"], summary_data[\"summary\"]\n",
    "\n",
    "            # Add example to training data\n",
    "            training_data.append(\n",
    "                {\n",
    "                    \"messages\": [\n",
    "                        {\"role\": \"system\", \"content\": agent.prompt},\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": f'Please tell me about the paper \"{title}\" and its insights into \"{topic}\" in relation to designing SARS-CoV-2 nanobody binders.',\n",
    "                        },\n",
    "                        {\"role\": \"assistant\", \"content\": summary},\n",
    "                    ]\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # Shuffle training data\n",
    "    random = Random(0)\n",
    "    random.shuffle(training_data)\n",
    "\n",
    "    # Count tokens\n",
    "    token_count = sum(\n",
    "        count_tokens(message[\"content\"])\n",
    "        for data in training_data\n",
    "        for message in data[\"messages\"]\n",
    "    )\n",
    "\n",
    "    # Determine finetuning cost\n",
    "    finetuning_cost = compute_finetuning_cost(\n",
    "        model=base_model_mini,\n",
    "        token_count=token_count,\n",
    "        num_epochs=DEFAULT_FINETUNING_EPOCHS,\n",
    "    )\n",
    "\n",
    "    # Print stats\n",
    "    print(f\"Number of paper examples for {agent.title}: {len(training_data):,}\")\n",
    "    print(f\"Token count for {agent.title}: {token_count:,}\")\n",
    "    print(f\"Finetuning cost for {agent.title} using {base_model_mini}: ${finetuning_cost:.2f}\")\n",
    "    print()\n",
    "\n",
    "    # Save training data in jsonl format\n",
    "    with open(\n",
    "        finetuning_dir / f\"{agent.title.replace(' ', '_')}_training_data.jsonl\", \"w\"\n",
    "    ) as f:\n",
    "        f.write(\"\\n\".join(json.dumps(example) for example in training_data))\n",
    "\n",
    "#Convert the summaries to the format required for fine-tuning, aggregated across all topics.\n",
    "# Convert summaries to training data format\n",
    "training_data = []\n",
    "\n",
    "for topic in topic_to_agent:\n",
    "    topic_name = topic.replace(\" \", \"_\")\n",
    "\n",
    "    # Get summary paths\n",
    "    topic_summary_dir = summaries_dir / topic_name\n",
    "    summary_paths = sorted(topic_summary_dir.glob(\"*.json\"))\n",
    "\n",
    "    for summary_path in summary_paths:\n",
    "        # Load paper summary data\n",
    "        with open(summary_path) as f:\n",
    "            summary_data = json.load(f)\n",
    "\n",
    "        # Extract title and summary\n",
    "        title, summary = summary_data[\"title\"], summary_data[\"summary\"]\n",
    "\n",
    "        # Add example to training data\n",
    "        training_data.append(\n",
    "            {\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": generic_agent.prompt},\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f'Please tell me about the paper \"{title}\" and its insights into \"{topic}\" in relation to designing SARS-CoV-2 nanobody binders.',\n",
    "                    },\n",
    "                    {\"role\": \"assistant\", \"content\": summary},\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "\n",
    "# Shuffle training data\n",
    "random = Random(0)\n",
    "random.shuffle(training_data)\n",
    "\n",
    "# Count tokens\n",
    "token_count = sum(\n",
    "    count_tokens(message[\"content\"])\n",
    "    for data in training_data\n",
    "    for message in data[\"messages\"]\n",
    ")\n",
    "\n",
    "# Determine finetuning cost\n",
    "finetuning_cost = compute_finetuning_cost(\n",
    "    model=base_model_mini,\n",
    "    token_count=token_count,\n",
    "    num_epochs=DEFAULT_FINETUNING_EPOCHS,\n",
    ")\n",
    "\n",
    "# Print stats\n",
    "print(f\"Number of paper examples: {len(training_data):,}\")\n",
    "print(f\"Token count: {token_count:,}\")\n",
    "print(f\"Finetuning cost using {base_model_mini}: ${finetuning_cost:.2f}\")\n",
    "print()\n",
    "\n",
    "# Save training data in jsonl format\n",
    "with open(\n",
    "    finetuning_dir / \"all_generic_training_data.jsonl\", \"w\"\n",
    ") as f:\n",
    "    f.write(\"\\n\".join(json.dumps(example) for example in training_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual_lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
