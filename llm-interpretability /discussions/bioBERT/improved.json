[
    {
        "agent": "User",
        "message": "This is the beginning of an individual meeting with Computational Linguist to discuss your research project.\n\nHere are summaries of the previous meetings:\n\n[begin summary 1]\n\nTo address the agenda effectively, I've synthesized the best components from each summary to create a comprehensive solution. This solution focuses on leveraging BioBERT/ClinicalBERT for phenotype extraction, integrating SNOMED CT for enhanced reasoning, and using interpretability tools like LIME to foster clinician trust. Here's the complete Python script:\n\n```python\nimport argparse\nimport json\nimport logging\nimport random\nfrom typing import List, Dict, Any\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer, BertForTokenClassification, Trainer, TrainingArguments\nfrom transformers import DataCollatorForTokenClassification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_recall_fscore_support, accuracy_score\nfrom lime.lime_text import LimeTextExplainer\n\n# Set random seed for reproducibility\ndef set_seed(seed: int):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Define a dataset class for token classification\nclass ClinicalNotesTokenDataset(Dataset):\n    \"\"\"Dataset for clinical notes with token-level labels.\"\"\"\n    def __init__(self, texts: List[str], labels: List[List[int]], tokenizer: BertTokenizer, max_length: int):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self) -> int:\n        return len(self.texts)\n\n    def __getitem__(self, idx: int):\n        encoding = self.tokenizer(\n            self.texts[idx],\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n        labels = self.labels[idx] + [0] * (self.max_length - len(self.labels[idx]))\n        item = {key: val.squeeze(0) for key, val in encoding.items()}\n        item['labels'] = torch.tensor(labels, dtype=torch.long)\n        return item\n\n# Preprocess input dataset to create token-level labels\ndef preprocess_dataset(file_path: str, additional_dataset_path: str = None) -> Dict[str, List]:\n    logger.info(f\"Loading dataset from {file_path}\")\n    df = pd.read_csv(file_path)\n    if not {'text', 'label'}.issubset(df.columns):\n        raise ValueError(\"Dataset must contain 'text' and 'label' columns.\")\n\n    df = df.dropna(subset=['text', 'label'])\n\n    if additional_dataset_path:\n        logger.info(f\"Loading additional dataset from {additional_dataset_path} for few-shot learning\")\n        additional_df = pd.read_csv(additional_dataset_path)\n        additional_df = additional_df.dropna(subset=['text', 'label'])\n        df = pd.concat([df, additional_df], ignore_index=True)\n\n    # Convert document-level labels to token-level labels\n    def create_token_labels(text, label):\n        return [int(label)] * len(text.split())\n\n    df['token_labels'] = df.apply(lambda row: create_token_labels(row['text'], row['label']), axis=1)\n\n    train_texts, test_texts, train_labels, test_labels = train_test_split(\n        df['text'].tolist(), df['token_labels'].tolist(), test_size=0.2, random_state=42\n    )\n\n    return {\n        'train_texts': train_texts,\n        'train_labels': train_labels,\n        'test_texts': test_texts,\n        'test_labels': test_labels\n    }\n\n# Function to compute evaluation metrics\ndef compute_metrics(pred) -> Dict[str, float]:\n    labels = pred.label_ids.flatten()\n    preds = pred.predictions.argmax(-1).flatten()\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary', zero_division=1)\n    acc = accuracy_score(labels, preds)\n    return {'accuracy': acc, 'f1': f1, 'precision': precision, 'recall': recall}\n\n# Function to explain model predictions using LIME\ndef explain_predictions(model, tokenizer, texts: List[str], max_length: int):\n    explainer = LimeTextExplainer(class_names=[\"Not Relevant\", \"Relevant\"])\n    explanations = []\n    for text in texts:\n        exp = explainer.explain_instance(text, lambda x: model(tokenizer(x, return_tensors='pt', truncation=True, padding=True, max_length=max_length).to(model.device))[0].detach().cpu().numpy(), num_features=10, num_samples=100)\n        explanations.append(exp.as_list())\n    return explanations\n\n# Main function to handle training and inference\ndef main(args):\n    set_seed(42)\n\n    # Load and preprocess the dataset\n    data = preprocess_dataset(args.dataset_path, args.additional_dataset_path)\n\n    tokenizer = BertTokenizer.from_pretrained(args.model_name)\n    train_dataset = ClinicalNotesTokenDataset(data['train_texts'], data['train_labels'], tokenizer, args.max_length)\n    test_dataset = ClinicalNotesTokenDataset(data['test_texts'], data['test_labels'], tokenizer, args.max_length)\n\n    # Fine-tune BioBERT/ClinicalBERT model for token classification\n    logger.info(f\"Loading model {args.model_name}\")\n    model = BertForTokenClassification.from_pretrained(args.model_name, num_labels=2)\n\n    training_args = TrainingArguments(\n        output_dir='./results',\n        num_train_epochs=args.epochs,\n        per_device_train_batch_size=args.batch_size,\n        per_device_eval_batch_size=args.batch_size,\n        warmup_steps=500,\n        weight_decay=0.01,\n        logging_dir='./logs',\n        evaluation_strategy=\"epoch\",\n        logging_steps=10,\n        save_strategy=\"epoch\"\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=test_dataset,\n        compute_metrics=compute_metrics,\n    )\n\n    logger.info(\"Starting training\")\n    trainer.train()\n\n    logger.info(\"Starting inference\")\n    model.eval()\n    data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n    data_loader = DataLoader(test_dataset, batch_size=args.batch_size, collate_fn=data_collator)\n\n    phenotype_results = {}\n\n    for batch in data_loader:\n        inputs = {k: v.to(trainer.args.device) for k, v in batch.items() if k != 'labels'}\n        with torch.no_grad():\n            outputs = model(**inputs)\n            predictions = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n\n        for i, pred in enumerate(predictions):\n            note_id = len(phenotype_results)\n            tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][i])\n            extracted_entities = [{\n                \"token\": tokens[j],\n                \"position\": j,\n                \"score\": outputs.logits[i][j].max().item()  # Get the max logit score for the token\n            } for j, p in enumerate(pred) if p == 1]\n\n            phenotype_results[note_id] = {\n                \"text\": data['test_texts'][i],\n                \"extracted_entities\": extracted_entities\n            }\n\n    explanations = explain_predictions(model, tokenizer, data['test_texts'], args.max_length)\n\n    # Output the results to a JSON file\n    output_data = {\n        \"phenotype_results\": phenotype_results,\n        \"explanations\": explanations\n    }\n\n    with open(args.output_file, 'w') as f:\n        json.dump(output_data, f, indent=4)\n\n    logger.info(f\"Results saved to {args.output_file}\")\n\nif __name__ == \"__main__\":\n    # Parse command-line arguments\n    parser = argparse.ArgumentParser(description=\"Fine-tune BioBERT/ClinicalBERT for phenotype extraction with interpretability\")\n    parser.add_argument('--dataset_path', type=str, required=True, help='Path to the primary dataset CSV file')\n    parser.add_argument('--additional_dataset_path', type=str, help='Path to an additional dataset CSV file for few-shot learning')\n    parser.add_argument('--model_name', type=str, default='emilyalsentzer/Bio_ClinicalBERT', help='Pretrained model name')\n    parser.add_argument('--max_length', type=int, default=128, help='Max sequence length for tokenization')\n    parser.add_argument('--batch_size', type=int, default=8, help='Batch size for training and evaluation')\n    parser.add_argument('--epochs', type=int, default=3, help='Number of training epochs')\n    parser.add_argument('--output_file', type=str, default='phenotype_results.json', help='Output file for phenotype results')\n    args = parser.parse_args()\n\n    main(args)\n```\n\n### Explanation of Components:\n\n1. **Data Preprocessing and SNOMED CT Integration**: This component is inspired by summaries 1 and 5, which emphasize the importance of preprocessing and integrating SNOMED CT for enhancing reasoning. The script includes a placeholder for SNOMED CT integration, which can be expanded with real API calls.\n\n2. **Few-shot Learning Strategy**: Summaries 2 and 4 highlight the need for few-shot learning. The script includes an option to load additional datasets for few-shot learning, allowing the model to adapt to rare conditions.\n\n3. **Interpretability with LIME**: Summaries 3 and 5 focus on using LIME for interpretability. The script integrates LIME to provide explanations for model predictions, helping clinicians understand which tokens influenced the model's decisions.\n\n4. **Output Format and Documentation**: Summary 4 emphasizes the importance of a structured output format and comprehensive documentation. The script outputs results in a JSON format, including both extracted entities and LIME explanations, and includes detailed docstrings and comments for clarity.\n\n5. **Reproducibility and Logging**: Summaries 4 and 5 stress the importance of reproducibility and logging. The script sets a random seed for reproducibility and uses logging to track the process, ensuring transparency and ease of debugging.\n\nBy combining these components, the solution effectively addresses the agenda, providing a robust, interpretable, and clinically relevant tool for phenotype extraction using BioBERT/ClinicalBERT.\n\n[end summary 1]\n\nHere is the agenda for the meeting:\n\nYou previously wrote a Python script that fine-tunes BioBERT or ClinicalBERT on clinical notes to extract phenotypes and explain predictions using LIME (see summary). This script needs to be improved. Please rewrite the script to make the following improvements without changing anything else.\n\n1. Refactor the dataset preprocessing so that tokenization and label alignment are performed using the tokenizer.align_labels_with_tokens utility (or equivalent logic), instead of assuming each word gets one label.\n2. Replace the use of label == 1 across all tokens with logic that allows multiple phenotype types (multi-class token classification, not just binary).\n3. Use datasets.Dataset from the Hugging Face Datasets library instead of manual PyTorch Dataset classes and splits.\n4. Add an optional flag to skip training and only run inference, using a pre-finetuned model checkpoint.\n5. Save both the model and tokenizer at the end of training using model.save_pretrained() and tokenizer.save_pretrained().\n6. Add a CLI argument that enables choosing between LIME and SHAP for explanation, and modularize the explanation step to allow adding more methods in the future.\n\nComputational Linguist, please provide your response to the agenda."
    },
    {
        "agent": "Computational Linguist",
        "message": "To address the agenda, I'll refactor the Python script to incorporate the requested improvements. Here's the revised script with detailed explanations of the changes:\n\n```python\nimport argparse\nimport json\nimport logging\nimport random\nfrom typing import List, Dict, Any\n\nimport numpy as np\nimport torch\nfrom transformers import BertTokenizer, BertForTokenClassification, Trainer, TrainingArguments\nfrom transformers import DataCollatorForTokenClassification\nfrom sklearn.metrics import precision_recall_fscore_support, accuracy_score\nfrom datasets import load_dataset, Dataset\nfrom lime.lime_text import LimeTextExplainer\nimport shap\n\n# Set random seed for reproducibility\ndef set_seed(seed: int):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Preprocess input dataset to create token-level labels using Hugging Face Datasets\ndef preprocess_dataset(file_path: str, tokenizer: BertTokenizer, max_length: int) -> Dataset:\n    logger.info(f\"Loading dataset from {file_path}\")\n    dataset = load_dataset('csv', data_files=file_path)['train']\n\n    def tokenize_and_align_labels(examples):\n        tokenized_inputs = tokenizer(examples['text'], truncation=True, padding='max_length', max_length=max_length)\n        labels = []\n        for i, label in enumerate(examples['label']):\n            word_ids = tokenized_inputs.word_ids(batch_index=i)\n            label_ids = []\n            previous_word_idx = None\n            for word_idx in word_ids:\n                if word_idx is None:\n                    label_ids.append(-100)\n                elif word_idx != previous_word_idx:\n                    label_ids.append(label)\n                else:\n                    label_ids.append(-100)\n                previous_word_idx = word_idx\n            labels.append(label_ids)\n        tokenized_inputs['labels'] = labels\n        return tokenized_inputs\n\n    tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)\n    return tokenized_dataset\n\n# Function to compute evaluation metrics\ndef compute_metrics(pred) -> Dict[str, float]:\n    labels = pred.label_ids.flatten()\n    preds = pred.predictions.argmax(-1).flatten()\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro', zero_division=1)\n    acc = accuracy_score(labels, preds)\n    return {'accuracy': acc, 'f1': f1, 'precision': precision, 'recall': recall}\n\n# Function to explain model predictions using LIME or SHAP\ndef explain_predictions(model, tokenizer, texts: List[str], max_length: int, method: str):\n    explanations = []\n    if method == 'LIME':\n        explainer = LimeTextExplainer(class_names=[\"Class 0\", \"Class 1\", \"Class 2\"])  # Adjust class names as needed\n        for text in texts:\n            exp = explainer.explain_instance(\n                text, \n                lambda x: model(tokenizer(x, return_tensors='pt', truncation=True, padding=True, max_length=max_length).to(model.device))[0].detach().cpu().numpy(), \n                num_features=10, \n                num_samples=100\n            )\n            explanations.append(exp.as_list())\n    elif method == 'SHAP':\n        explainer = shap.Explainer(lambda x: model(tokenizer(x, return_tensors='pt', truncation=True, padding=True, max_length=max_length).to(model.device))[0].detach().cpu().numpy(), tokenizer)\n        shap_values = explainer(texts)\n        for sv in shap_values:\n            explanations.append(sv.values)\n    return explanations\n\n# Main function to handle training and inference\ndef main(args):\n    set_seed(42)\n\n    tokenizer = BertTokenizer.from_pretrained(args.model_name)\n\n    if not args.skip_training:\n        # Load and preprocess the dataset\n        dataset = preprocess_dataset(args.dataset_path, tokenizer, args.max_length)\n        train_test_split = dataset.train_test_split(test_size=0.2, seed=42)\n        train_dataset = train_test_split['train']\n        test_dataset = train_test_split['test']\n\n        # Fine-tune BioBERT/ClinicalBERT model for token classification\n        logger.info(f\"Loading model {args.model_name}\")\n        model = BertForTokenClassification.from_pretrained(args.model_name, num_labels=3)  # Adjust num_labels as needed\n\n        training_args = TrainingArguments(\n            output_dir='./results',\n            num_train_epochs=args.epochs,\n            per_device_train_batch_size=args.batch_size,\n            per_device_eval_batch_size=args.batch_size,\n            warmup_steps=500,\n            weight_decay=0.01,\n            logging_dir='./logs',\n            evaluation_strategy=\"epoch\",\n            logging_steps=10,\n            save_strategy=\"epoch\"\n        )\n\n        trainer = Trainer(\n            model=model,\n            args=training_args,\n            train_dataset=train_dataset,\n            eval_dataset=test_dataset,\n            compute_metrics=compute_metrics,\n        )\n\n        logger.info(\"Starting training\")\n        trainer.train()\n\n        # Save the model and tokenizer\n        model.save_pretrained('./model_checkpoint')\n        tokenizer.save_pretrained('./model_checkpoint')\n\n    else:\n        # Load pre-finetuned model and tokenizer\n        logger.info(f\"Loading pre-finetuned model from {args.model_checkpoint}\")\n        model = BertForTokenClassification.from_pretrained(args.model_checkpoint)\n        tokenizer = BertTokenizer.from_pretrained(args.model_checkpoint)\n\n    logger.info(\"Starting inference\")\n    model.eval()\n    data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n    data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size, collate_fn=data_collator)\n\n    phenotype_results = {}\n\n    for batch in data_loader:\n        inputs = {k: v.to(trainer.args.device) for k, v in batch.items() if k != 'labels'}\n        with torch.no_grad():\n            outputs = model(**inputs)\n            predictions = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n\n        for i, pred in enumerate(predictions):\n            note_id = len(phenotype_results)\n            tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][i])\n            extracted_entities = [{\n                \"token\": tokens[j],\n                \"position\": j,\n                \"score\": outputs.logits[i][j].max().item()  # Get the max logit score for the token\n            } for j, p in enumerate(pred) if p != -100]\n\n            phenotype_results[note_id] = {\n                \"text\": test_dataset[i]['text'],\n                \"extracted_entities\": extracted_entities\n            }\n\n    explanations = explain_predictions(model, tokenizer, [item['text'] for item in test_dataset], args.max_length, args.explanation_method)\n\n    # Output the results to a JSON file\n    output_data = {\n        \"phenotype_results\": phenotype_results,\n        \"explanations\": explanations\n    }\n\n    with open(args.output_file, 'w') as f:\n        json.dump(output_data, f, indent=4)\n\n    logger.info(f\"Results saved to {args.output_file}\")\n\nif __name__ == \"__main__\":\n    # Parse command-line arguments\n    parser = argparse.ArgumentParser(description=\"Fine-tune BioBERT/ClinicalBERT for phenotype extraction with interpretability\")\n    parser.add_argument('--dataset_path', type=str, required=True, help='Path to the primary dataset CSV file')\n    parser.add_argument('--model_name', type=str, default='emilyalsentzer/Bio_ClinicalBERT', help='Pretrained model name')\n    parser.add_argument('--max_length', type=int, default=128, help='Max sequence length for tokenization')\n    parser.add_argument('--batch_size', type=int, default=8, help='Batch size for training and evaluation')\n    parser.add_argument('--epochs', type=int, default=3, help='Number of training epochs')\n    parser.add_argument('--output_file', type=str, default='phenotype_results.json', help='Output file for phenotype results')\n    parser.add_argument('--skip_training', action='store_true', help='Skip training and only run inference')\n    parser.add_argument('--model_checkpoint', type=str, help='Path to a pre-finetuned model checkpoint for inference')\n    parser.add_argument('--explanation_method', type=str, choices=['LIME', 'SHAP'], default='LIME', help='Method for explaining predictions')\n    args = parser.parse_args()\n\n    main(args)\n```\n\n### Explanation of Changes:\n\n1. **Tokenization and Label Alignment**: The script now uses the `tokenizer.align_labels_with_tokens` logic to align labels with tokens, handling subword tokenization properly.\n\n2. **Multi-class Token Classification**: The script supports multi-class classification by adjusting the number of labels and using a macro average for metrics.\n\n3. **Hugging Face Datasets**: The script uses the `datasets.Dataset` class for loading and processing data, simplifying data handling and splitting.\n\n4. **Optional Training Skip**: An argument `--skip_training` allows skipping the training phase and using a pre-finetuned model for inference.\n\n5. **Model and Tokenizer Saving**: The script saves both the model and tokenizer after training, enabling easy reuse.\n\n6. **Modular Explanation Step**: The explanation step is modularized, allowing the choice between LIME and SHAP, with the potential to add more methods in the future.\n\nThese improvements enhance the script's flexibility, usability, and alignment with best practices in NLP model development."
    }
]