[
    {
        "agent": "User",
        "message": "This is the beginning of an individual meeting with Data Visualization Expert to discuss your research project.\n\nHere is the agenda for the meeting:\n\nYou are working on a research project to use software techniques to develop a tool for large language model (LLM) interpretability in the context of creating electronic phenotype definitions, ideally, also incorporating a visual perspective to foster trust between the clinician and the model. To develop a tool for LLM interpretability in electronic phenotype definitions, we will adopt a machine learning approach, leveraging existing open-source interpretability libraries. Our focus will be on integrating SNOMED CT to enhance reasoning and reduce hallucinations, while incorporating visual elements to build clinician trust.\n                You are responsible for understanding the topic Customizing interpretability interfaces for clinician roles in the context of designing an LLM-based interpretability pipeline for electronic phenotype definition.\n                You need to fine-tune yourself on the relevant literature on Customizing interpretability interfaces for clinician roles to improve your ability to contribute effectively to building a transparent, clinically grounded, and visually intuitive interpretability tool.\n                Please write out a series of five distinct search queries that you want to run to find relevant scientific papers on Customizing interpretability interfaces for clinician roles. Include both general queries about Customizing interpretability interfaces for clinician roles and queries that specifically relate Customizing interpretability interfaces for clinician roles to LLM interpretability, phenotype definition, clinical applications, and clinician trust.\n                Please provide the queries in Python syntax as a list of double-quoted strings.\n\nHere are the agenda questions that must be answered:\n\n1. What are the PMCIDs and titles of the papers you wish to fine-tune yourself on (as a Python dictionary mapping PMCID as a double-quoted string to title as double-quoted string)?\n\nData Visualization Expert, please provide your response to the agenda."
    },
    {
        "agent": "Data Visualization Expert",
        "message": "To address the agenda, I'll first create a series of search queries to find relevant scientific papers on customizing interpretability interfaces for clinician roles. These queries will cover both general topics and specific intersections with LLM interpretability, phenotype definition, clinical applications, and clinician trust.\n\nHere are the search queries in Python syntax:\n\n```python\nqueries = [\n    \"Customizing interpretability interfaces for clinician roles\",\n    \"LLM interpretability in electronic phenotype definition\",\n    \"Customizing interpretability interfaces for clinical applications\",\n    \"Clinician trust in AI-driven phenotype definitions\",\n    \"Integrating SNOMED CT for interpretability in clinical settings\"\n]\n```\n\nNext, I'll use these queries to search for relevant scientific papers. Let's proceed with retrieving the abstracts or titles of the papers."
    },
    {
        "agent": "Data Visualization Expert",
        "message": "Here are the PMCIDs and titles of the papers that are relevant to the research project on customizing interpretability interfaces for clinician roles:\n\n```python\npapers = {\n    \"5938574\": \"Opportunities and obstacles for deep learning in biology and medicine\",\n    \"11523650\": \"A review on the efficacy of artificial intelligence for managing anxiety disorders\",\n    \"10956028\": \"GaNDLF: the generally nuanced deep learning framework for scalable end-to-end clinical workflows\",\n    \"11422740\": \"Prompt Engineering Paradigms for Medical Applications: Scoping Review\",\n    \"10689487\": \"Zero-shot interpretable phenotyping of postpartum hemorrhage using large language models\",\n    \"11047988\": \"The Role of AI in Hospitals and Clinics: Transforming Healthcare in the 21st Century\",\n    \"11848901\": \"Artificial intelligence to revolutionize IBD clinical trials: a comprehensive review\",\n    \"11494256\": \"Use of SNOMED CT in Large Language Models: Scoping Review\",\n    \"11336492\": \"Enhancing Clinical Relevance of Pretrained Language Models Through Integration of External Knowledge: Case Study on Cardiovascular Diagnosis From Electronic Health Records\"\n}\n```\n\nThese papers cover a range of topics including deep learning in medicine, AI in clinical workflows, LLM interpretability, and the integration of SNOMED CT for enhancing clinical relevance, which are all pertinent to developing a tool for LLM interpretability in electronic phenotype definitions."
    },
    {
        "agent": "User",
        "message": "Tool Output:\n\nHere are the top 5 articles on PubMed Central for the query \"Customizing interpretability interfaces for clinician roles\":\n\n[begin paper 1]\n\nPMCID = 5938574\n\nTitle = Opportunities and obstacles for deep learning in biology and medicine\n\nDeep learning describes a class of machine learning algorithms that are capable of combining raw inputs into layers of intermediate features. These algorithms have recently shown impressive results across a variety of domains. Biology and medicine are data-rich disciplines, but the data are complex and often ill-understood. Hence, deep learning techniques may be particularly well suited to solve problems of these fields. We examine applications of deep learning to a variety of biomedical problems\u2014patient classification, fundamental biological processes and treatment of patients\u2014and discuss whether deep learning will be able to transform these tasks or if the biomedical sphere poses unique challenges. Following from an extensive literature review, we find that deep learning has yet to revolutionize biomedicine or definitively resolve any of the most pressing challenges in the field, but promising advances have been made on the prior state of the art. Even though improvements over previous baselines have been modest in general, the recent progress indicates that deep learning methods will provide valuable means for speeding up or aiding human investigation. Though progress has been made linking a specific neural network's prediction to input features, understanding how users should interpret these models to make testable hypotheses about the system under study remains an open challenge. Furthermore, the limited amount of labelled data for training presents problems in some domains, as do legal and privacy constraints on work with sensitive health records. Nonetheless, we foresee deep learning enabling changes at both bench and bedside with the potential to transform several areas of biology and medicine.\n\n[end paper 1]\n\n[begin paper 2]\n\nPMCID = 11523650\n\nTitle = A review on the efficacy of artificial intelligence for managing anxiety disorders\n\nAnxiety disorders are psychiatric conditions characterized by prolonged and generalized anxiety experienced by individuals in response to various events or situations. At present, anxiety disorders are regarded as the most widespread psychiatric disorders globally. Medication and different types of psychotherapies are employed as the primary therapeutic modalities in clinical practice for the treatment of anxiety disorders. However, combining these two approaches is known to yield more significant benefits than medication alone. Nevertheless, there is a lack of resources and a limited availability of psychotherapy options in underdeveloped areas. Psychotherapy methods encompass relaxation techniques, controlled breathing exercises, visualization exercises, controlled exposure exercises, and cognitive interventions such as challenging negative thoughts. These methods are vital in the treatment of anxiety disorders, but executing them proficiently can be demanding. Moreover, individuals with distinct anxiety disorders are prescribed medications that may cause withdrawal symptoms in some instances. Additionally, there is inadequate availability of face-to-face psychotherapy and a restricted capacity to predict and monitor the health, behavioral, and environmental aspects of individuals with anxiety disorders during the initial phases. In recent years, there has been notable progress in developing and utilizing artificial intelligence (AI) based applications and environments to improve the precision and sensitivity of diagnosing and treating various categories of anxiety disorders. As a result, this study aims to establish the efficacy of AI-enabled environments in addressing the existing challenges in managing anxiety disorders, reducing reliance on medication, and investigating the potential advantages, issues, and opportunities of integrating AI-assisted healthcare for anxiety disorders and enabling personalized therapy.\n\n[end paper 2]\n\n[begin paper 3]\n\nPMCID = 6157254\n\nTitle = Proceedings of the 4th IPLeiria\u2019s International Health Congress\n\n\n\n[end paper 3]\n\n[begin paper 4]\n\nPMCID = 10956028\n\nTitle = GaNDLF: the generally nuanced deep learning framework for scalable end-to-end clinical workflows\n\nDeep Learning (DL) has the potential to optimize machine learning in both the scientific and clinical communities. However, greater expertise is required to develop DL algorithms, and the variability of implementations hinders their reproducibility, translation, and deployment. Here we present the community-driven Generally Nuanced Deep Learning Framework (GaNDLF), with the goal of lowering these barriers. GaNDLF makes the mechanism of DL development, training, and inference more stable, reproducible, interpretable, and scalable, without requiring an extensive technical background. GaNDLF aims to provide an end-to-end solution for all DL-related tasks in computational precision medicine. We demonstrate the ability of GaNDLF to analyze both radiology and histology images, with built-in support for k-fold cross-validation, data augmentation, multiple modalities and output classes. Our quantitative performance evaluation on numerous use cases, anatomies, and computational tasks supports GaNDLF as a robust application framework for deployment in clinical workflows.\n\nThe increasing complexity of the implementation and operation of deep learning techniques hinders their reproducibility and deployment at scale, especially in healthcare. Pati and colleagues introduce a deep learning framework to analyse healthcare data without requiring extensive computational experience, facilitating the integration of artificial intelligence in clinical workflows.\n\n[end paper 4]\n\n[begin paper 5]\n\nPMCID = 10956174\n\nTitle = Proceedings of the 2023 International Maternal Newborn Health Conference\n\n\n\n[end paper 5]\n\n\n\nHere are the top 5 articles on PubMed Central for the query \"LLM interpretability in electronic phenotype definition\":\n\n[begin paper 1]\n\nPMCID = 11422740\n\nTitle = Prompt Engineering Paradigms for Medical Applications: Scoping Review\n\nPrompt engineering, focusing on crafting effective prompts to large language models (LLMs), has garnered attention for its capabilities at harnessing the potential of LLMs. This is even more crucial in the medical domain due to its specialized terminology and language technicity. Clinical natural language processing applications must navigate complex language and ensure privacy compliance. Prompt engineering offers a novel approach by designing tailored prompts to guide models in exploiting clinically relevant information from complex medical texts. Despite its promise, the efficacy of prompt engineering in the medical domain remains to be fully explored.\n\nThe aim of the study is to review research efforts and technical approaches in prompt engineering for medical applications as well as provide an overview of opportunities and challenges for clinical practice.\n\nDatabases indexing the fields of medicine, computer science, and medical informatics were queried in order to identify relevant published papers. Since prompt engineering is an emerging field, preprint databases were also considered. Multiple data were extracted, such as the prompt paradigm, the involved LLMs, the languages of the study, the domain of the topic, the baselines, and several learning, design, and architecture strategies specific to prompt engineering. We include studies that apply prompt engineering\u2013based methods to the medical domain, published between 2022 and 2024, and covering multiple prompt paradigms such as prompt learning (PL), prompt tuning (PT), and prompt design (PD).\n\nWe included 114 recent prompt engineering studies. Among the 3 prompt paradigms, we have observed that PD is the most prevalent (78 papers). In 12 papers, PD, PL, and PT terms were used interchangeably. While ChatGPT is the most commonly used LLM, we have identified 7 studies using this LLM on a sensitive clinical data set. Chain-of-thought, present in 17 studies, emerges as the most frequent PD technique. While PL and PT papers typically provide a baseline for evaluating prompt-based approaches, 61% (48/78) of the PD studies do not report any nonprompt-related baseline. Finally, we individually examine each of the key prompt engineering\u2013specific information reported across papers and find that many studies neglect to explicitly mention them, posing a challenge for advancing prompt engineering research.\n\nIn addition to reporting on trends and the scientific landscape of prompt engineering, we provide reporting guidelines for future studies to help advance research in the medical field. We also disclose tables and figures summarizing medical prompt engineering papers available and hope that future contributions will leverage these existing works to better advance the field.\n\n[end paper 1]\n\n[begin paper 2]\n\nPMCID = 11480249\n\nTitle = Confronting the data deluge: How artificial intelligence can be used in the study of plant stress\n\nThe advent of the genomics era enabled the generation of high-throughput data and computational methods that serve as powerful hypothesis-generating tools to understand the genomic and gene functional basis of plant stress resilience. The proliferation of experimental and analytical methods used in biology has resulted in a situation where plentiful data exists, but the volume and heterogeneity of this data has made analysis a significant challenge. Current advanced deep-learning models have displayed an unprecedented level of comprehension and problem-solving ability, and have been used to predict gene structure, function and expression based on DNA or protein sequence, and prominently also their use in high-throughput phenomics in agriculture. However, the application of deep-learning models to understand gene regulatory and signalling behaviour is still in its infancy. We discuss in this review the availability of data resources and bioinformatic tools, and several applications of these advanced ML/AI models in the context of plant stress response, and demonstrate the use of a publicly available LLM (ChatGPT) to derive a knowledge graph of various experimental and computational methods used in the study of plant stress. We hope this will stimulate further interest in collaboration between computer scientists, computational biologists and plant scientists to distil the deluge of genomic, transcriptomic, proteomic, metabolomic and phenomic data into meaningful knowledge that can be used for the benefit of humanity.\n\n[end paper 2]\n\n[begin paper 3]\n\nPMCID = 11639975\n\nTitle = EXACT-Net: Framework for EHR-Guided Lung Tumor Auto-Segmentation for Non-Small Cell Lung Cancer Radiotherapy\n\nIn recent years, large language models have shown great potential to enhance traditional medical image processing by incorporating multimodality information into decision-making. Conventional artificial intelligence systems solely rely on images to make predictions or decisions. However, information from medical reports can provide invaluable information for the system to curate its decision. Here we are presenting a multimodality language-vision model and framework for accurate segmentation of medical images.\n\nBackground/Objectives: Lung cancer is a devastating disease with the highest mortality rate among cancer types. Over 60% of non-small cell lung cancer (NSCLC) patients, accounting for 87% of lung cancer diagnoses, require radiation therapy. Rapid treatment initiation significantly increases the patient\u2019s survival rate and reduces the mortality rate. Accurate tumor segmentation is a critical step in diagnosing and treating NSCLC. Manual segmentation is time- and labor-consuming and causes delays in treatment initiation. Although many lung nodule detection methods, including deep learning-based models, have been proposed. Most of these methods still have a long-standing problem of high false positives (FPs). Methods: Here, we developed an electronic health record (EHR)-guided lung tumor auto-segmentation called EXACT-Net (EHR-enhanced eXACtitude in Tumor segmentation), where the extracted information from EHRs using a pre-trained large language model (LLM) was used to remove the FPs and keep the TP nodules only. Results: The auto-segmentation model was trained on NSCLC patients\u2019 computed tomography (CT), and the pre-trained LLM was used with the zero-shot learning approach. Our approach resulted in a 250% boost in successful nodule detection using the data from ten NSCLC patients treated in our institution. Conclusions: We demonstrated that combining vision-language information in EXACT-Net multi-modal AI framework greatly enhances the performance of vision only models, paving the road to multimodal AI framework for medical image processing.\n\n[end paper 3]\n\n[begin paper 4]\n\nPMCID = 11545079\n\nTitle = The Evolution of Artificial Intelligence in Medical Imaging: From Computer Science to Machine and Deep Learning\n\nArtificial intelligence, now one of the most promising frontiers of medicine, has a long and tumultuous history punctuated by successes and failures. One of its successes was its application to medical images. We reconstruct the timeline of the advancements in this field, from its origins in the 1940s before crossing medical images to early applications of machine learning to radiology, to the present era where artificial intelligence is revolutionizing radiology.\n\nArtificial intelligence (AI), the wide spectrum of technologies aiming to give machines or computers the ability to perform human-like cognitive functions, began in the 1940s with the first abstract models of intelligent machines. Soon after, in the 1950s and 1960s, machine learning algorithms such as neural networks and decision trees ignited significant enthusiasm. More recent advancements include the refinement of learning algorithms, the development of convolutional neural networks to efficiently analyze images, and methods to synthesize new images. This renewed enthusiasm was also due to the increase in computational power with graphical processing units and the availability of large digital databases to be mined by neural networks. AI soon began to be applied in medicine, first through expert systems designed to support the clinician\u2019s decision and later with neural networks for the detection, classification, or segmentation of malignant lesions in medical images. A recent prospective clinical trial demonstrated the non-inferiority of AI alone compared with a double reading by two radiologists on screening mammography. Natural language processing, recurrent neural networks, transformers, and generative models have both improved the capabilities of making an automated reading of medical images and moved AI to new domains, including the text analysis of electronic health records, image self-labeling, and self-reporting. The availability of open-source and free libraries, as well as powerful computing resources, has greatly facilitated the adoption of deep learning by researchers and clinicians. Key concerns surrounding AI in healthcare include the need for clinical trials to demonstrate efficacy, the perception of AI tools as \u2018black boxes\u2019 that require greater interpretability and explainability, and ethical issues related to ensuring fairness and trustworthiness in AI systems. Thanks to its versatility and impressive results, AI is one of the most promising resources for frontier research and applications in medicine, in particular for oncological applications.\n\n[end paper 4]\n\n[begin paper 5]\n\nPMCID = 10689487\n\nTitle = Zero-shot interpretable phenotyping of postpartum hemorrhage using large language models\n\nMany areas of medicine would benefit from deeper, more accurate phenotyping, but there are limited approaches for phenotyping using clinical notes without substantial annotated data. Large language models (LLMs) have demonstrated immense potential to adapt to novel tasks with no additional training by specifying task-specific instructions. Here we report the performance of a publicly available LLM, Flan-T5, in phenotyping patients with postpartum hemorrhage (PPH) using discharge notes from electronic health records (n\u2009=\u2009271,081). The language model achieves strong performance in extracting 24 granular concepts associated with PPH. Identifying these granular concepts accurately allows the development of interpretable, complex phenotypes and subtypes. The Flan-T5 model achieves high fidelity in phenotyping PPH (positive predictive value of 0.95), identifying 47% more patients with this complication compared to the current standard of using claims codes. This LLM pipeline can be used reliably for subtyping PPH and outperforms a claims-based approach on the three most common PPH subtypes associated with uterine atony, abnormal placentation, and obstetric trauma. The advantage of this approach to subtyping is its interpretability, as each concept contributing to the subtype determination can be evaluated. Moreover, as definitions may change over time due to new guidelines, using granular concepts to create complex phenotypes enables prompt and efficient updating of the algorithm. Using this language modelling approach enables rapid phenotyping without the need for any manually annotated training data across multiple clinical use cases.\n\n[end paper 5]\n\n\n\nHere are the top 5 articles on PubMed Central for the query \"Customizing interpretability interfaces for clinical applications\":\n\n[begin paper 1]\n\nPMCID = 11047988\n\nTitle = The Role of AI in Hospitals and Clinics: Transforming Healthcare in the 21st Century\n\nAs healthcare systems around the world face challenges such as escalating costs, limited access, and growing demand for personalized care, artificial intelligence (AI) is emerging as a key force for transformation. This review is motivated by the urgent need to harness AI\u2019s potential to mitigate these issues and aims to critically assess AI\u2019s integration in different healthcare domains. We explore how AI empowers clinical decision-making, optimizes hospital operation and management, refines medical image analysis, and revolutionizes patient care and monitoring through AI-powered wearables. Through several case studies, we review how AI has transformed specific healthcare domains and discuss the remaining challenges and possible solutions. Additionally, we will discuss methodologies for assessing AI healthcare solutions, ethical challenges of AI deployment, and the importance of data privacy and bias mitigation for responsible technology use. By presenting a critical assessment of AI\u2019s transformative potential, this review equips researchers with a deeper understanding of AI\u2019s current and future impact on healthcare. It encourages an interdisciplinary dialogue between researchers, clinicians, and technologists to navigate the complexities of AI implementation, fostering the development of AI-driven solutions that prioritize ethical standards, equity, and a patient-centered approach. \n\n[end paper 1]\n\n[begin paper 2]\n\nPMCID = 11991189\n\nTitle = EEG Signal Prediction for Motor Imagery Classification in Brain\u2013Computer Interfaces\n\nBrain\u2013computer interfaces (BCIs) based on motor imagery (MI) generally require EEG signals recorded from a large number of electrodes distributed across the cranial surface to achieve accurate MI classification. Not only does this entail long preparation times and high costs, but it also carries the risk of losing valuable information when an electrode is damaged, further limiting its practical applicability. In this study, a signal prediction-based method is proposed to achieve high accuracy in MI classification using EEG signals recorded from only a small number of electrodes. The signal prediction model was constructed using the elastic net regression technique, allowing for the estimation of EEG signals from 22 complete channels based on just 8 centrally located channels. The predicted EEG signals from the complete channels were used for feature extraction and MI classification. The results obtained indicate a notable efficacy of the proposed prediction method, showing an average performance of 78.16% in classification accuracy. The proposed method demonstrated superior performance compared to the traditional approach that used few-channel EEG and also achieved better results than the traditional method based on full-channel EEG. Although accuracy varies among subjects, from 62.30% to an impressive 95.24%, these data indicate the capability of the method to provide accurate estimates from a reduced set of electrodes. This performance highlights its potential to be implemented in practical MI-based BCI applications, thereby mitigating the time and cost constraints associated with systems that require a high density of electrodes.\n\n[end paper 2]\n\n[begin paper 3]\n\nPMCID = 11745858\n\nTitle = Application of large language models in disease diagnosis and treatment\n\nLarge language models (LLMs) such as ChatGPT, Claude, Llama, and Qwen are emerging as transformative technologies for the diagnosis and treatment of various diseases. With their exceptional long-context reasoning capabilities, LLMs are proficient in clinically relevant tasks, particularly in medical text analysis and interactive dialogue. They can enhance diagnostic accuracy by processing vast amounts of patient data and medical literature and have demonstrated their utility in diagnosing common diseases and facilitating the identification of rare diseases by recognizing subtle patterns in symptoms and test results. Building on their image-recognition abilities, multimodal LLMs (MLLMs) show promising potential for diagnosis based on radiography, chest computed tomography (CT), electrocardiography (ECG), and common pathological images. These models can also assist in treatment planning by suggesting evidence-based interventions and improving clinical decision support systems through integrated analysis of patient records. Despite these promising developments, significant challenges persist regarding the use of LLMs in medicine, including concerns regarding algorithmic bias, the potential for hallucinations, and the need for rigorous clinical validation. Ethical considerations also underscore the importance of maintaining the function of supervision in clinical practice. This paper highlights the rapid advancements in research on the diagnostic and therapeutic applications of LLMs across different medical disciplines and emphasizes the importance of policymaking, ethical supervision, and multidisciplinary collaboration in promoting more effective and safer clinical applications of LLMs. Future directions include the integration of proprietary clinical knowledge, the investigation of open-source and customized models, and the evaluation of real-time effects in clinical diagnosis and treatment practices.\n\n[end paper 3]\n\n[begin paper 4]\n\nPMCID = 5938574\n\nTitle = Opportunities and obstacles for deep learning in biology and medicine\n\nDeep learning describes a class of machine learning algorithms that are capable of combining raw inputs into layers of intermediate features. These algorithms have recently shown impressive results across a variety of domains. Biology and medicine are data-rich disciplines, but the data are complex and often ill-understood. Hence, deep learning techniques may be particularly well suited to solve problems of these fields. We examine applications of deep learning to a variety of biomedical problems\u2014patient classification, fundamental biological processes and treatment of patients\u2014and discuss whether deep learning will be able to transform these tasks or if the biomedical sphere poses unique challenges. Following from an extensive literature review, we find that deep learning has yet to revolutionize biomedicine or definitively resolve any of the most pressing challenges in the field, but promising advances have been made on the prior state of the art. Even though improvements over previous baselines have been modest in general, the recent progress indicates that deep learning methods will provide valuable means for speeding up or aiding human investigation. Though progress has been made linking a specific neural network's prediction to input features, understanding how users should interpret these models to make testable hypotheses about the system under study remains an open challenge. Furthermore, the limited amount of labelled data for training presents problems in some domains, as do legal and privacy constraints on work with sensitive health records. Nonetheless, we foresee deep learning enabling changes at both bench and bedside with the potential to transform several areas of biology and medicine.\n\n[end paper 4]\n\n[begin paper 5]\n\nPMCID = 11992566\n\nTitle = From bytes to bites: application of large language models to enhance nutritional recommendations\n\nLarge language models (LLMs) such as ChatGPT are increasingly positioned to be integrated into various aspects of daily life, with promising applications in healthcare, including personalized nutritional guidance for patients with chronic kidney disease (CKD). However, for LLM-powered nutrition support tools to reach their full potential, active collaboration of healthcare professionals, patients, caregivers and LLM experts is crucial. We conducted a comprehensive review of the literature on the use of LLMs as tools to enhance nutrition recommendations for patients with CKD, curated by our expertise in the field. Additionally, we considered relevant findings from adjacent fields, including diabetes and obesity management. Currently, the application of LLMs for CKD-specific nutrition support remains limited and has room for improvement. Although LLMs can generate recipe ideas, their nutritional analyses often underestimate critical food components such as electrolytes and calories. Anticipated advancements in LLMs and other generative artificial intelligence (AI) technologies are expected to enhance these capabilities, potentially enabling accurate nutritional analysis, the generation of visual aids for cooking and identification of kidney-healthy options in restaurants. While LLM-based nutritional support for patients with CKD is still in its early stages, rapid advancements are expected in the near future. Engagement from the CKD community, including healthcare professionals, patients and caregivers, will be essential to harness AI-driven improvements in nutritional care with a balanced perspective that is both critical and optimistic.\n\n[end paper 5]\n\n\n\nHere are the top 2 articles on PubMed Central for the query \"Clinician trust in AI-driven phenotype definitions\":\n\n[begin paper 1]\n\nPMCID = 11848901\n\nTitle = Artificial intelligence to revolutionize IBD clinical trials: a comprehensive review\n\nIntegrating artificial intelligence (AI) into clinical trials for inflammatory bowel disease (IBD) has potential to be transformative to the field. This article explores how AI-driven technologies, including machine learning (ML), natural language processing, and predictive analytics, have the potential to enhance important aspects of IBD trials\u2014from patient recruitment and trial design to data analysis and personalized treatment strategies. As AI advances, it has potential to improve long-standing challenges in trial efficiency, accuracy, and personalization with the goal of accelerating the discovery of novel therapies and improve outcomes for people living with IBD. AI can streamline multiple trial phases, from target identification and patient recruitment to data analysis and monitoring. By integrating multi-omics data, electronic health records, and imaging repositories, AI can uncover molecular targets and personalize trial strategies, ultimately expediting drug development. However, the adoption of AI in IBD clinical trials encounters significant challenges. These include technical barriers in data integration, ethical concerns regarding patient privacy, and regulatory issues related to AI validation standards. Additionally, AI models risk producing biased outcomes if training datasets lack diversity, potentially impacting underrepresented populations in clinical trials. Addressing these limitations requires standardized data formats, interdisciplinary collaboration, and robust ethical frameworks to ensure inclusivity and accuracy. Continued partnerships among clinicians, researchers, data scientists, and regulators will be essential to establish transparent, patient-centered AI frameworks. By overcoming these obstacles, AI has the potential to enhance the efficiency, equity, and efficacy of IBD clinical trials, ultimately benefiting patient care.\n\n Artificial intelligence in IBD clinical trials \n\nInflammatory Bowel Disease (IBD), including Crohn\u2019s disease and ulcerative colitis, poses significant challenges for clinical trials, such as difficulties in recruiting participants, variations in disease presentation, and inconsistent treatment responses. Artificial intelligence (AI) is increasingly recognized as a solution to these challenges, improving recruitment, data analysis, personalized care, and trial design. AI can enhance recruitment by analyzing medical records to match patients to trials efficiently. AI tools can automate this process, improving both efficiency and diversity. Additionally, AI can predict dropout risks, helping researchers plan better and maintain trial integrity. IBD trials generate complex datasets that require advanced analysis. AI can process these large datasets to identify patterns in disease progression and treatment efficacy, also improving the accuracy of endoscopic and histological assessments, providing deeper insights into the disease. AI can enable personalized treatments by predicting responses based on genetics, biomarkers, and medical history. Real-time monitoring through wearable devices supports early interventions, improving patient outcomes and disease management. Adaptive trial designs might also benefit from AI, allowing protocols to adjust based on interim results. This enhances trial efficiency, ethical standards, and participant safety, while ensuring accurate data collection. However, implementing AI requires addressing data privacy, fairness, and regulatory compliance. Transparent, secure, and inclusive AI models are essential to build trust and ensure equitable benefits across all patient populations. AI is transforming IBD clinical trials by streamlining recruitment, improving data analysis, personalizing care, and optimizing trial design. By addressing challenges proactively, we can unlock AI\u2019s full potential, leading to more efficient trials and better outcomes for patients.\n\n[end paper 1]\n\n[begin paper 2]\n\nPMCID = 11893894\n\nTitle = Ethical considerations in AI for child health and recommendations for child-centered medical AI\n\nThere does not exist any previous comprehensive review on AI ethics in child health or any guidelines for management, unlike in adult medicine. This review describes ethical principles in AI for child health and provides recommendations for child-centered medical AI. We also introduce the Pediatrics EthicAl Recommendations List for AI (PEARL-AI) framework for clinicians and AI developers to ensure ethical AI enabled systems in healthcare for children.\n\n[end paper 2]\n\n\n\nHere are the top 5 articles on PubMed Central for the query \"Integrating SNOMED CT for interpretability in clinical settings\":\n\n[begin paper 1]\n\nPMCID = 11494256\n\nTitle = Use of SNOMED CT in Large Language Models: Scoping Review\n\nLarge language models (LLMs) have substantially advanced natural language processing (NLP) capabilities but often struggle with knowledge-driven tasks in specialized domains such as biomedicine. Integrating biomedical knowledge sources such as SNOMED CT into LLMs may enhance their performance on biomedical tasks. However, the methodologies and effectiveness of incorporating SNOMED CT into LLMs have not been systematically reviewed.\n\nThis scoping review aims to examine how SNOMED CT is integrated into LLMs, focusing on (1) the types and components of LLMs being integrated with SNOMED CT, (2) which contents of SNOMED CT are being integrated, and (3) whether this integration improves LLM performance on NLP tasks.\n\nFollowing the PRISMA-ScR (Preferred Reporting Items for Systematic Reviews and Meta-Analyses extension for Scoping Reviews) guidelines, we searched ACM Digital Library, ACL Anthology, IEEE Xplore, PubMed, and Embase for relevant studies published from 2018 to 2023. Studies were included if they incorporated SNOMED CT into LLM pipelines for natural language understanding or generation tasks. Data on LLM types, SNOMED CT integration methods, end tasks, and performance metrics were extracted and synthesized.\n\nThe review included 37 studies. Bidirectional Encoder Representations from Transformers and its biomedical variants were the most commonly used LLMs. Three main approaches for integrating SNOMED CT were identified: (1) incorporating SNOMED CT into LLM inputs (28/37, 76%), primarily using concept descriptions to expand training corpora; (2) integrating SNOMED CT into additional fusion modules (5/37, 14%); and (3) using SNOMED CT as an external knowledge retriever during inference (5/37, 14%). The most frequent end task was medical concept normalization (15/37, 41%), followed by entity extraction or typing and classification. While most studies (17/19, 89%) reported performance improvements after SNOMED CT integration, only a small fraction (19/37, 51%) provided direct comparisons. The reported gains varied widely across different metrics and tasks, ranging from 0.87% to 131.66%. However, some studies showed either no improvement or a decline in certain performance metrics.\n\nThis review demonstrates diverse approaches for integrating SNOMED CT into LLMs, with a focus on using concept descriptions to enhance biomedical language understanding and generation. While the results suggest potential benefits of SNOMED CT integration, the lack of standardized evaluation methods and comprehensive performance reporting hinders definitive conclusions about its effectiveness. Future research should prioritize consistent reporting of performance comparisons and explore more sophisticated methods for incorporating SNOMED CT\u2019s relational structure into LLMs. In addition, the biomedical NLP community should develop standardized evaluation frameworks to better assess the impact of ontology integration on LLM performance.\n\n[end paper 1]\n\n[begin paper 2]\n\nPMCID = 10031450\n\nTitle = Extracting Medical Information From Free-Text and Unstructured Patient-Generated Health Data Using Natural Language Processing Methods: Feasibility Study With Real-world Data\n\nPatient-generated health data (PGHD) captured via smart devices or digital health technologies can reflect an individual health journey. PGHD enables tracking and monitoring of personal health conditions, symptoms, and medications out of the clinic, which is crucial for self-care and shared clinical decisions. In addition to self-reported measures and structured PGHD (eg, self-screening, sensor-based biometric data), free-text and unstructured PGHD (eg, patient care note, medical diary) can provide a broader view of a patient\u2019s journey and health condition. Natural language processing (NLP) is used to process and analyze unstructured data to create meaningful summaries and insights, showing promise to improve the utilization of PGHD.\n\nOur aim is to understand and demonstrate the feasibility of an NLP pipeline to extract medication and symptom information from real-world patient and caregiver data.\n\nWe report a secondary data analysis, using a data set collected from 24 parents of children with special health care needs (CSHCN) who were recruited via a nonrandom sampling approach. Participants used a voice-interactive app for 2 weeks, generating free-text patient notes (audio transcription or text entry). We built an NLP pipeline using a zero-shot approach (adaptive to low-resource settings). We used named entity recognition (NER) and medical ontologies (RXNorm and SNOMED CT [Systematized Nomenclature of Medicine Clinical Terms]) to identify medication and symptoms. Sentence-level dependency parse trees and part-of-speech tags were used to extract additional entity information using the syntactic properties of a note. We assessed the data; evaluated the pipeline with the patient notes; and reported the precision, recall, and F1 scores.\n\nIn total, 87 patient notes are included (audio transcriptions n=78 and text entries n=9) from 24 parents who have at least one CSHCN. The participants were between the ages of 26 and 59 years. The majority were White (n=22, 92%), had more than one child (n=16, 67%), lived in Ohio (n=22, 92%), had mid- or upper-mid household income (n=15, 62.5%), and had higher level education (n=24, 58%). Out of 87 notes, 30 were drug and medication related, and 46 were symptom related. We captured medication instances (medication, unit, quantity, and date) and symptoms satisfactorily (precision >0.65, recall >0.77, F1>0.72). These results indicate the potential when using NER and dependency parsing through an NLP pipeline on information extraction from unstructured PGHD.\n\nThe proposed NLP pipeline was found to be feasible for use with real-world unstructured PGHD to accomplish medication and symptom extraction. Unstructured PGHD can be leveraged to inform clinical decision-making, remote monitoring, and self-care including medical adherence and chronic disease management. With customizable information extraction methods using NER and medical ontologies, NLP models can feasibly extract a broad range of clinical information from unstructured PGHD in low-resource settings (eg, a limited number of patient notes or training data).\n\n[end paper 2]\n\n[begin paper 3]\n\nPMCID = 6849172\n\nTitle = Exploring semantic deep learning for building reliable and reusable one health knowledge from PubMed systematic reviews and veterinary clinical notes\n\nDeep Learning opens up opportunities for routinely scanning large bodies of biomedical literature and clinical narratives to represent the meaning of biomedical and clinical terms. However, the validation and integration of this knowledge on a scale requires cross checking with ground truths (i.e. evidence-based resources) that are unavailable in an actionable or computable form. In this paper we explore how to turn information about diagnoses, prognoses, therapies and other clinical concepts into computable knowledge using free-text data about human and animal health. We used a Semantic Deep Learning approach that combines the Semantic Web technologies and Deep Learning to acquire and validate knowledge about 11 well-known medical conditions mined from two sets of unstructured free-text data: 300\u2009K PubMed Systematic Review articles (the PMSB dataset) and 2.5\u2009M veterinary clinical notes (the VetCN dataset). For each target condition we obtained 20 related clinical concepts using two deep learning methods applied separately on the two datasets, resulting in 880 term pairs (target term, candidate term). Each concept, represented by an n-gram, is mapped to UMLS using MetaMap; we also developed a bespoke method for mapping short forms (e.g. abbreviations and acronyms). Existing ontologies were used to formally represent associations. We also create ontological modules and illustrate how the extracted knowledge can be queried. The evaluation was performed using the content within BMJ Best Practice.\n\nMetaMap achieves an F measure of 88% (precision 85%, recall 91%) when applied directly to the total of 613 unique candidate terms for the 880 term pairs. When the processing of short forms is included, MetaMap achieves an F measure of 94% (precision 92%, recall 96%). Validation of the term pairs with BMJ Best Practice yields precision between 98 and 99%.\n\nThe Semantic Deep Learning approach can transform neural embeddings built from unstructured free-text data into reliable and reusable One Health knowledge using ontologies and content from BMJ Best Practice.\n\n[end paper 3]\n\n[begin paper 4]\n\nPMCID = 11336492\n\nTitle = Enhancing Clinical Relevance of Pretrained Language Models Through Integration of External Knowledge: Case Study on Cardiovascular Diagnosis From Electronic Health Records\n\nDespite their growing use in health care, pretrained language models (PLMs) often lack clinical relevance due to insufficient domain expertise and poor interpretability. A key strategy to overcome these challenges is integrating external knowledge into PLMs, enhancing their adaptability and clinical usefulness. Current biomedical knowledge graphs like UMLS (Unified Medical Language System), SNOMED CT (Systematized Medical Nomenclature for Medicine\u2013Clinical Terminology), and HPO (Human Phenotype Ontology), while comprehensive, fail to effectively connect general biomedical knowledge with physician insights. There is an equally important need for a model that integrates diverse knowledge in a way that is both unified and compartmentalized. This approach not only addresses the heterogeneous nature of domain knowledge but also recognizes the unique data and knowledge repositories of individual health care institutions, necessitating careful and respectful management of proprietary information.\n\nThis study aimed to enhance the clinical relevance and interpretability of PLMs by integrating external knowledge in a manner that respects the diversity and proprietary nature of health care data. We hypothesize that domain knowledge, when captured and distributed as stand-alone modules, can be effectively reintegrated into PLMs to significantly improve their adaptability and utility in clinical settings.\n\nWe demonstrate that through adapters, small and lightweight neural networks that enable the integration of extra information without full model fine-tuning, we can inject diverse sources of external domain knowledge into language models and improve the overall performance with an increased level of interpretability. As a practical application of this methodology, we introduce a novel task, structured as a case study, that endeavors to capture physician knowledge in assigning cardiovascular diagnoses from clinical narratives, where we extract diagnosis-comment pairs from electronic health records (EHRs) and cast the problem as text classification.\n\nThe study demonstrates that integrating domain knowledge into PLMs significantly improves their performance. While improvements with ClinicalBERT are more modest, likely due to its pretraining on clinical texts, BERT (bidirectional encoder representations from transformer) equipped with knowledge adapters surprisingly matches or exceeds ClinicalBERT in several metrics. This underscores the effectiveness of knowledge adapters and highlights their potential in settings with strict data privacy constraints. This approach also increases the level of interpretability of these models in a clinical context, which enhances our ability to precisely identify and apply the most relevant domain knowledge for specific tasks, thereby optimizing the model\u2019s performance and tailoring it to meet specific clinical needs.\n\nThis research provides a basis for creating health knowledge graphs infused with physician knowledge, marking a significant step forward for PLMs in health care. Notably, the model balances integrating knowledge both comprehensively and selectively, addressing the heterogeneous nature of medical knowledge and the privacy needs of health care institutions.\n\n[end paper 4]\n\n[begin paper 5]\n\nPMCID = 9931276\n\nTitle = Diseasomics: Actionable machine interpretable disease knowledge at the point-of-care\n\nPhysicians establish diagnosis by assessing a patient\u2019s signs, symptoms, age, sex, laboratory test findings and the disease history. All this must be done in limited time and against the backdrop of an increasing overall workload. In the era of evidence-based medicine it is utmost important for a clinician to be abreast of the latest guidelines and treatment protocols which are changing rapidly. In resource limited settings, the updated knowledge often does not reach the point-of-care. This paper presents an artificial intelligence (AI)-based approach for integrating comprehensive disease knowledge, to support physicians and healthcare workers in arriving at accurate diagnoses at the point-of-care. We integrated different disease-related knowledge bodies to construct a comprehensive, machine interpretable diseasomics knowledge-graph that includes the Disease Ontology, disease symptoms, SNOMED CT, DisGeNET, and PharmGKB data. The resulting disease-symptom network comprises knowledge from the Symptom Ontology, electronic health records (EHR), human symptom disease network, Disease Ontology, Wikipedia, PubMed, textbooks, and symptomology knowledge sources with 84.56% accuracy. We also integrated spatial and temporal comorbidity knowledge obtained from EHR for two population data sets from Spain and Sweden respectively. The knowledge graph is stored in a graph database as a digital twin of the disease knowledge. We use node2vec (node embedding) as digital triplet for link prediction in disease-symptom networks to identify missing associations. This diseasomics knowledge graph is expected to democratize the medical knowledge and empower non-specialist health workers to make evidence based informed decisions and help achieve the goal of universal health coverage (UHC). The machine interpretable knowledge graphs presented in this paper are associations between various entities and do not imply causation. Our differential diagnostic tool focusses on signs and symptoms and does not include a complete assessment of patient\u2019s lifestyle and health history which would typically be necessary to rule out conditions and to arrive at a final diagnosis. The predicted diseases are ordered according to the specific disease burden in South Asia. The knowledge graphs and the tools presented here can be used as a guide.\n\nA doctor at the point-of-care is expected to have the complete medical knowledge with latest updates in evidence-based medicine (EBM). The doctor is also expected to use this complete knowledge accurately during a medical interaction with a patient. In reality this is not the case\u2014there are gaps in knowledge acquisition and gaps in clinical decision making. To address these gaps, in the past AI based expert driven rule-based clinical decision support systems were developed. Rule-based systems are rigid and often fail in case of complex diseases. We therefore built an AI based evidence driven clinical decision support system. We mined PubMed, Wikipedia, textbooks, medical records, etc. to extract clinical knowledge. We used this clinical knowledge as glue to connect ontologies to construct a machine interpretable antireductionistic diseasomics knowledge graph. The diseasomics knowledge graph is stored in a Neo4j property graph database in a cloud for online and realtime access using JSON-RPC API and works like the physicians\u2019 brain digital twin. We used the digital triplet node2vec techniques to mine unknown knowledge and to create a learning healthcare system. The integrated diseasomics knowledge system is available for use at https://triage.cyberneticcare.com/diseasePrediction.\n\n[end paper 5]\n\n"
    }
]