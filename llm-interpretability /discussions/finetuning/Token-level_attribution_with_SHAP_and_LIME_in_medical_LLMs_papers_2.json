[
    {
        "agent": "User",
        "message": "This is the beginning of an individual meeting with Computational Linguist to discuss your research project.\n\nHere is the agenda for the meeting:\n\nYou are working on a research project to use software techniques to develop a tool for large language model (LLM) interpretability in the context of creating electronic phenotype definitions, ideally, also incorporating a visual perspective to foster trust between the clinician and the model. To develop a tool for LLM interpretability in electronic phenotype definitions, we will adopt a machine learning approach, leveraging existing open-source interpretability libraries. Our focus will be on integrating SNOMED CT to enhance reasoning and reduce hallucinations, while incorporating visual elements to build clinician trust.\n                You are responsible for understanding the topic Token-level attribution with SHAP and LIME in medical LLMs in the context of designing an LLM-based interpretability pipeline for electronic phenotype definition.\n                You need to fine-tune yourself on the relevant literature on Token-level attribution with SHAP and LIME in medical LLMs to improve your ability to contribute effectively to building a transparent, clinically grounded, and visually intuitive interpretability tool.\n                Please write out a series of five distinct search queries that you want to run to find relevant scientific papers on Token-level attribution with SHAP and LIME in medical LLMs. Include both general queries about Token-level attribution with SHAP and LIME in medical LLMs and queries that specifically relate Token-level attribution with SHAP and LIME in medical LLMs to LLM interpretability, phenotype definition, clinical applications, and clinician trust.\n                Please provide the queries in Python syntax as a list of double-quoted strings.\n\nHere are the agenda questions that must be answered:\n\n1. What are the PMCIDs and titles of the papers you wish to fine-tune yourself on (as a Python dictionary mapping PMCID as a double-quoted string to title as double-quoted string)?\n\nComputational Linguist, please provide your response to the agenda."
    },
    {
        "agent": "Computational Linguist",
        "message": "To address the agenda, I will first outline a series of search queries to find relevant scientific papers on Token-level attribution with SHAP and LIME in medical LLMs. These queries will cover both general aspects and specific applications related to LLM interpretability, phenotype definition, clinical applications, and clinician trust.\n\nHere are the search queries in Python syntax:\n\n```python\nsearch_queries = [\n    \"Token-level attribution SHAP LIME medical LLMs\",\n    \"SHAP LIME interpretability medical LLMs phenotype definition\",\n    \"Token-level attribution clinical applications SHAP LIME\",\n    \"LLM interpretability clinician trust SHAP LIME\",\n    \"Token-level attribution SHAP LIME electronic phenotype definitions\"\n]\n```\n\nNext, I will use these queries to search for relevant papers. Let's proceed with retrieving the PMCIDs and titles of the papers using these queries."
    },
    {
        "agent": "Computational Linguist",
        "message": "Here are the PMCIDs and titles of the papers I found relevant to the topic of Token-level attribution with SHAP and LIME in medical LLMs:\n\n```python\npapers = {\n    \"11213143\": \"Prospector Heads: Generalized Feature Attribution for Large Models & Data\",\n    \"11300430\": \"From outputs to insights: a survey of rationalization approaches for explainable text classification\",\n    \"10558383\": \"The benefits and pitfalls of machine learning for biomarker discovery\",\n    \"11848901\": \"Artificial intelligence to revolutionize IBD clinical trials: a comprehensive review\",\n    \"11157010\": \"Human-annotated rationales and explainable text classification: a survey\",\n    \"11436130\": \"Enhancing the Interpretability of Malaria and Typhoid Diagnosis with Explainable AI and Large Language Models\",\n    \"11877768\": \"The role of explainable artificial intelligence in disease prediction: a systematic literature review and future research directions\",\n    \"11972358\": \"Explainable artificial intelligence to diagnose early Parkinson\u2019s disease via voice analysis\",\n    \"11900311\": \"Shaping the Future of Healthcare: Ethical Clinical Challenges and Pathways to Trustworthy AI\",\n    \"11629193\": \"Bridging human and machine intelligence: Reverse-engineering radiologist intentions for clinical trust and adoption\"\n}\n```\n\nThese papers cover various aspects of interpretability, clinical applications, and the use of SHAP and LIME in medical contexts, which are relevant to our research project on LLM interpretability in electronic phenotype definitions."
    },
    {
        "agent": "User",
        "message": "Tool Output:\n\nHere are the top 2 articles on PubMed Central for the query \"Token-level attribution SHAP LIME medical LLMs\":\n\n[begin paper 1]\n\nPMCID = 11213143\n\nTitle = Prospector Heads: Generalized Feature Attribution for Large Models & Data\n\nFeature attribution, the ability to localize regions of the input data that are relevant for classification, is an important capability for ML models in scientific and biomedical domains. Current methods for feature attribution, which rely on \u201cexplaining\u201d the predictions of end-to-end classifiers, suffer from imprecise feature localization and are inadequate for use with small sample sizes and high-dimensional datasets due to computational challenges. We introduce prospector heads, an efficient and interpretable alternative to explanation-based attribution methods that can be applied to any encoder and any data modality. Prospector heads generalize across modalities through experiments on sequences (text), images (pathology), and graphs (protein structures), outperforming baseline attribution methods by up to 26.3 points in mean localization AUPRC. We also demonstrate how prospector heads enable improved interpretation and discovery of class-specific patterns in input data. Through their high performance, flexibility, and generalizability, prospectors provide a framework for improving trust and transparency for ML models in complex domains.\n\n[end paper 1]\n\n[begin paper 2]\n\nPMCID = 11300430\n\nTitle = From outputs to insights: a survey of rationalization approaches for explainable text classification\n\nDeep learning models have achieved state-of-the-art performance for text classification in the last two decades. However, this has come at the expense of models becoming less understandable, limiting their application scope in high-stakes domains. The increased interest in explainability has resulted in many proposed forms of explanation. Nevertheless, recent studies have shown that rationales, or language explanations, are more intuitive and human-understandable, especially for non-technical stakeholders. This survey provides an overview of the progress the community has achieved thus far in rationalization approaches for text classification. We first describe and compare techniques for producing extractive and abstractive rationales. Next, we present various rationale-annotated data sets that facilitate the training and evaluation of rationalization models. Then, we detail proxy-based and human-grounded metrics to evaluate machine-generated rationales. Finally, we outline current challenges and encourage directions for future work.\n\n[end paper 2]\n\n\n\nHere are the top 2 articles on PubMed Central for the query \"SHAP LIME interpretability medical LLMs phenotype definition\":\n\n[begin paper 1]\n\nPMCID = 10558383\n\nTitle = The benefits and pitfalls of machine learning for biomarker discovery\n\nProspects for the discovery of robust and reproducible biomarkers have improved considerably with the development of sensitive omics platforms that can enable measurement of biological molecules at an unprecedented scale. With technical barriers to success lowering, the challenge is now moving into the analytical domain. Genome-wide discovery presents a problem of scale and multiple testing as standard statistical methods struggle to distinguish signal from noise in increasingly complex biological systems. Machine learning and AI methods are good at finding answers in large datasets, but they have a tendency to overfit solutions. It may be possible to find a local answer or mechanism in a specific patient sample or small group of samples, but this may not generalise to wider patient populations due to the high likelihood of false discovery. The rise of explainable AI offers to improve the opportunity for true discovery by providing explanations for predictions that can be explored mechanistically before proceeding to costly and time-consuming validation studies. This review aims to introduce some of the basic concepts of machine learning and AI for biomarker discovery with a focus on post hoc explanation of predictions. To illustrate this, we consider how explainable AI has already been used successfully, and we explore a case study that applies AI to biomarker discovery in rheumatoid arthritis, demonstrating the accessibility of tools for AI and machine learning. We use this to illustrate and discuss some of the potential challenges and solutions that may enable AI to critically interrogate disease and response mechanisms.\n\n[end paper 1]\n\n[begin paper 2]\n\nPMCID = 11848901\n\nTitle = Artificial intelligence to revolutionize IBD clinical trials: a comprehensive review\n\nIntegrating artificial intelligence (AI) into clinical trials for inflammatory bowel disease (IBD) has potential to be transformative to the field. This article explores how AI-driven technologies, including machine learning (ML), natural language processing, and predictive analytics, have the potential to enhance important aspects of IBD trials\u2014from patient recruitment and trial design to data analysis and personalized treatment strategies. As AI advances, it has potential to improve long-standing challenges in trial efficiency, accuracy, and personalization with the goal of accelerating the discovery of novel therapies and improve outcomes for people living with IBD. AI can streamline multiple trial phases, from target identification and patient recruitment to data analysis and monitoring. By integrating multi-omics data, electronic health records, and imaging repositories, AI can uncover molecular targets and personalize trial strategies, ultimately expediting drug development. However, the adoption of AI in IBD clinical trials encounters significant challenges. These include technical barriers in data integration, ethical concerns regarding patient privacy, and regulatory issues related to AI validation standards. Additionally, AI models risk producing biased outcomes if training datasets lack diversity, potentially impacting underrepresented populations in clinical trials. Addressing these limitations requires standardized data formats, interdisciplinary collaboration, and robust ethical frameworks to ensure inclusivity and accuracy. Continued partnerships among clinicians, researchers, data scientists, and regulators will be essential to establish transparent, patient-centered AI frameworks. By overcoming these obstacles, AI has the potential to enhance the efficiency, equity, and efficacy of IBD clinical trials, ultimately benefiting patient care.\n\n Artificial intelligence in IBD clinical trials \n\nInflammatory Bowel Disease (IBD), including Crohn\u2019s disease and ulcerative colitis, poses significant challenges for clinical trials, such as difficulties in recruiting participants, variations in disease presentation, and inconsistent treatment responses. Artificial intelligence (AI) is increasingly recognized as a solution to these challenges, improving recruitment, data analysis, personalized care, and trial design. AI can enhance recruitment by analyzing medical records to match patients to trials efficiently. AI tools can automate this process, improving both efficiency and diversity. Additionally, AI can predict dropout risks, helping researchers plan better and maintain trial integrity. IBD trials generate complex datasets that require advanced analysis. AI can process these large datasets to identify patterns in disease progression and treatment efficacy, also improving the accuracy of endoscopic and histological assessments, providing deeper insights into the disease. AI can enable personalized treatments by predicting responses based on genetics, biomarkers, and medical history. Real-time monitoring through wearable devices supports early interventions, improving patient outcomes and disease management. Adaptive trial designs might also benefit from AI, allowing protocols to adjust based on interim results. This enhances trial efficiency, ethical standards, and participant safety, while ensuring accurate data collection. However, implementing AI requires addressing data privacy, fairness, and regulatory compliance. Transparent, secure, and inclusive AI models are essential to build trust and ensure equitable benefits across all patient populations. AI is transforming IBD clinical trials by streamlining recruitment, improving data analysis, personalizing care, and optimizing trial design. By addressing challenges proactively, we can unlock AI\u2019s full potential, leading to more efficient trials and better outcomes for patients.\n\n[end paper 2]\n\n\n\nHere are the top 2 articles on PubMed Central for the query \"Token-level attribution clinical applications SHAP LIME\":\n\n[begin paper 1]\n\nPMCID = 11213143\n\nTitle = Prospector Heads: Generalized Feature Attribution for Large Models & Data\n\nFeature attribution, the ability to localize regions of the input data that are relevant for classification, is an important capability for ML models in scientific and biomedical domains. Current methods for feature attribution, which rely on \u201cexplaining\u201d the predictions of end-to-end classifiers, suffer from imprecise feature localization and are inadequate for use with small sample sizes and high-dimensional datasets due to computational challenges. We introduce prospector heads, an efficient and interpretable alternative to explanation-based attribution methods that can be applied to any encoder and any data modality. Prospector heads generalize across modalities through experiments on sequences (text), images (pathology), and graphs (protein structures), outperforming baseline attribution methods by up to 26.3 points in mean localization AUPRC. We also demonstrate how prospector heads enable improved interpretation and discovery of class-specific patterns in input data. Through their high performance, flexibility, and generalizability, prospectors provide a framework for improving trust and transparency for ML models in complex domains.\n\n[end paper 1]\n\n[begin paper 2]\n\nPMCID = 11157010\n\nTitle = Human-annotated rationales and explainable text classification: a survey\n\nAsking annotators to explain \u201cwhy\u201d they labeled an instance yields annotator rationales: natural language explanations that provide reasons for classifications. In this work, we survey the collection and use of annotator rationales. Human-annotated rationales can improve data quality and form a valuable resource for improving machine learning models. Moreover, human-annotated rationales can inspire the construction and evaluation of model-annotated rationales, which can play an important role in explainable artificial intelligence.\n\n[end paper 2]\n\n\n\nHere are the top 5 articles on PubMed Central for the query \"LLM interpretability clinician trust SHAP LIME\":\n\n[begin paper 1]\n\nPMCID = 11436130\n\nTitle = Enhancing the Interpretability of Malaria and Typhoid Diagnosis with Explainable AI and Large Language Models\n\nMalaria and Typhoid fever are prevalent diseases in tropical regions, and both are exacerbated by unclear protocols, drug resistance, and environmental factors. Prompt and accurate diagnosis is crucial to improve accessibility and reduce mortality rates. Traditional diagnosis methods cannot effectively capture the complexities of these diseases due to the presence of similar symptoms. Although machine learning (ML) models offer accurate predictions, they operate as \u201cblack boxes\u201d with non-interpretable decision-making processes, making it challenging for healthcare providers to comprehend how the conclusions are reached. This study employs explainable AI (XAI) models such as Local Interpretable Model-agnostic Explanations (LIME), and Large Language Models (LLMs) like GPT to clarify diagnostic results for healthcare workers, building trust and transparency in medical diagnostics by describing which symptoms had the greatest impact on the model\u2019s decisions and providing clear, understandable explanations. The models were implemented on Google Colab and Visual Studio Code because of their rich libraries and extensions. Results showed that the Random Forest model outperformed the other tested models; in addition, important features were identified with the LIME plots while ChatGPT 3.5 had a comparative advantage over other LLMs. The study integrates RF, LIME, and GPT in building a mobile app to enhance the interpretability and transparency in malaria and typhoid diagnosis system. Despite its promising results, the system\u2019s performance is constrained by the quality of the dataset. Additionally, while LIME and GPT improve transparency, they may introduce complexities in real-time deployment due to computational demands and the need for internet service to maintain relevance and accuracy. The findings suggest that AI-driven diagnostic systems can significantly enhance healthcare delivery in environments with limited resources, and future works can explore the applicability of this framework to other medical conditions and datasets.\n\n[end paper 1]\n\n[begin paper 2]\n\nPMCID = 11877768\n\nTitle = The role of explainable artificial intelligence in disease prediction: a systematic literature review and future research directions\n\nExplainable Artificial Intelligence (XAI) enhances transparency and interpretability in AI models, which is crucial for trust and accountability in healthcare. A potential application of XAI is disease prediction using various data modalities. This study conducts a Systematic Literature Review (SLR) following the PRISMA protocol, synthesizing findings from 30 selected studies to examine XAI\u2019s evolving role in disease prediction. It explores commonly used XAI methods, such as Shapley Additive Explanations (SHAP) and Local Interpretable Model-agnostic Explanations (LIME), and their impact across medical fields in disease prediction. The review highlights key gaps, including limited dataset diversity, model complexity, and reliance on single data types, emphasizing the need for greater interpretability and data integration. Addressing these issues is crucial for advancing AI in healthcare. This study contributes by outlining current challenges and potential solutions, suggesting directions for future research to develop more reliable and robust XAI methods.\n\n[end paper 2]\n\n[begin paper 3]\n\nPMCID = 11972358\n\nTitle = Explainable artificial intelligence to diagnose early Parkinson\u2019s disease via voice analysis\n\nParkinson\u2019s disease (PD) is a neurodegenerative disorder affecting motor control, leading to symptoms such as tremors and stiffness. Early diagnosis is essential for effective treatment, but traditional methods are often time-consuming and expensive. This study leverages Artificial Intelligence (AI) and Machine Learning (ML) techniques, using voice analysis to detect early signs of PD. We applied a hybrid model combining Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), Multiple Kernel Learning (MKL), and Multilayer Perceptron (MLP) to a dataset of 81 voice recordings. Acoustic features such as Mel-Frequency Cepstral Coefficients (MFCCs), jitter, and shimmer were analyzed. The model achieved 91.11% accuracy, 92.50% recall, 89.84% precision, 91.13% F1 score, and an area-under-the-curve (AUC) of 0.9125. SHapley Additive exPlanations (SHAP) provided data explainability, identifying key features driving the PD diagnosis, thus enhancing AI interpretability and trustability. Furthermore, a probability-based scoring system was developed to enable PD patients and clinicians to track disease progression. This AI-driven approach offers a non-invasive, cost-effective, and rapid tool for early PD detection, facilitating personalized treatment through vocal biomarkers.\n\nThe online version contains supplementary material available at 10.1038/s41598-025-96575-6.\n\n[end paper 3]\n\n[begin paper 4]\n\nPMCID = 11900311\n\nTitle = Shaping the Future of Healthcare: Ethical Clinical Challenges and Pathways to Trustworthy AI\n\nBackground/Objectives: Artificial intelligence (AI) is transforming healthcare, enabling advances in diagnostics, treatment optimization, and patient care. Yet, its integration raises ethical, regulatory, and societal challenges. Key concerns include data privacy risks, algorithmic bias, and regulatory gaps that struggle to keep pace with AI advancements. This study aims to synthesize a multidisciplinary framework for trustworthy AI in healthcare, focusing on transparency, accountability, fairness, sustainability, and global collaboration. It moves beyond high-level ethical discussions to provide actionable strategies for implementing trustworthy AI in clinical contexts. Methods: A structured literature review was conducted using PubMed, Scopus, and Web of Science. Studies were selected based on relevance to AI ethics, governance, and policy in healthcare, prioritizing peer-reviewed articles, policy analyses, case studies, and ethical guidelines from authoritative sources published within the last decade. The conceptual approach integrates perspectives from clinicians, ethicists, policymakers, and technologists, offering a holistic \u201cecosystem\u201d view of AI. No clinical trials or patient-level interventions were conducted. Results: The analysis identifies key gaps in current AI governance and introduces the Regulatory Genome\u2014an adaptive AI oversight framework aligned with global policy trends and Sustainable Development Goals. It introduces quantifiable trustworthiness metrics, a comparative analysis of AI categories for clinical applications, and bias mitigation strategies. Additionally, it presents interdisciplinary policy recommendations for aligning AI deployment with ethical, regulatory, and environmental sustainability goals. This study emphasizes measurable standards, multi-stakeholder engagement strategies, and global partnerships to ensure that future AI innovations meet ethical and practical healthcare needs. Conclusions: Trustworthy AI in healthcare requires more than technical advancements\u2014it demands robust ethical safeguards, proactive regulation, and continuous collaboration. By adopting the recommended roadmap, stakeholders can foster responsible innovation, improve patient outcomes, and maintain public trust in AI-driven healthcare.\n\n[end paper 4]\n\n[begin paper 5]\n\nPMCID = 11629193\n\nTitle = Bridging human and machine intelligence: Reverse-engineering radiologist intentions for clinical trust and adoption\n\nIn the rapidly evolving landscape of medical imaging, the integration of artificial intelligence (AI) with clinical expertise offers unprecedented opportunities to enhance diagnostic precision and accuracy. Yet, the \"black box\" nature of AI models often limits their integration into clinical practice, where transparency and interpretability are important. This paper presents a novel system leveraging the Large Multimodal Model (LMM) to bridge the gap between AI predictions and the cognitive processes of radiologists. This system consists of two core modules, Temporally Grounded Intention Detection (TGID) and Region Extraction (RE). The TGID module predicts the radiologist's intentions by analyzing eye gaze fixation heatmap videos and corresponding radiology reports. Additionally, the RE module extracts regions of interest that align with these intentions, mirroring the radiologist\u2019s diagnostic focus. This approach introduces a new task, radiologist intention detection, and is the first application of Dense Video Captioning (DVC) in the medical domain. By making AI systems more interpretable and aligned with radiologist\u2019s cognitive processes, this proposed system aims to enhance trust, improve diagnostic accuracy, and support medical education. Additionally, it holds the potential for automated error correction, guiding junior radiologists, and fostering more effective training and feedback mechanisms. This work sets a precedent for future research in AI-driven healthcare, offering a pathway towards transparent, trustworthy, and human-centered AI systems. We evaluated this model using NLG(Natural Language Generation), time-related, and vision-based metrics, demonstrating superior performance in generating temporally grounded intentions on REFLACX and EGD-CXR datasets. This model also demonstrated strong predictive accuracy in overlap scores for medical abnormalities and effective region extraction with high IoU(Intersection over Union), especially in complex cases like cardiomegaly and edema. These results highlight the system's potential to enhance diagnostic accuracy and support continuous learning in radiology.\n\nOverview of our proposed system, comprising two key submodules: Temporally Grounded Intention Detection (TGID) and Region Extraction (RE). The system processes eye gaze fixation video overlaid on CXR images alongside the corresponding radiology report, ultimately identifying the intended diagnosis and highlighting the associated Regions of Interest (ROI).\n\n[end paper 5]\n\n\n\nNo articles found on PubMed Central for the query \"Token-level attribution SHAP LIME electronic phenotype definitions\"."
    }
]