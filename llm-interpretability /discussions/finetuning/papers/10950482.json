{
    "content": [
        "We explored how explainable artificial intelligence (XAI) can help to shed light into the inner workings of neural networks for protein function prediction, by extending the widely used XAI method of integrated gradients such that latent representations inside of transformer models, which were finetuned to Gene Ontology term and Enzyme Commission number prediction, can be inspected too.",
        "The approach enabled us to identify amino acids in the sequences that the transformers pay particular attention to, and to show that these relevant sequence parts reflect expectations from biology and chemistry, both in the embedding layer and inside of the model, where we identified transformer heads with a statistically significant correspondence of attribution maps with ground truth sequence annotations (e.g. transmembrane regions, active sites) across many proteins.",
        "Source code can be accessed at https://github.com/markuswenzel/xai-proteins.",
        "Proteins are versatile molecular machines, performing various tasks in basically all cells of every organism, and are modularly constructed from chains of amino acids. Inferring the function of a given protein merely from its amino acid sequence is a particularly interesting problem in bioinformatics research.",
        "Function prediction can help to rapidly provide valuable pointers in the face of so far unfamiliar proteins of understudied species, such as of emerging pathogens. Moreover, it makes the analysis of large, unlabeled protein datasets possible, which becomes more and more relevant against the backdrop of the massive and evermore growing databases of unlabeled nucleic acid sequences, which again can be translated into amino acid sequences. Next-generation DNA sequencers can read the nucleic acid sequences present in a sample or specimen at decreasing costs, much faster than experimenters can determine the function of the genes and corresponding proteins. Therefore, databases with genes and corresponding amino acid sequences grow much more rapidly than those of respective experimental gene and protein labels or annotations. Besides, gaining knowledge about the mapping between amino acid sequence and protein function can help to engineer proteins for dedicated purposes too.",
        "Machine learning approaches to protein function prediction can include inferring enzymatic function, Gene Ontology (GO) terms, protein\u2013protein/\u2013drug interaction, remote homology, stability, sub-cellular location, and other properties. For structure prediction, the objective is to infer how the amino acid sequence folds into the secondary and tertiary protein structure. Several of the prediction tasks can be approached as well by transferring labels from similar sequences obtained via multiple sequence alignment (MSA). Protein prediction models are compared by the scientific community in systematic performance benchmarks, e.g. for function annotation (CAFA,), for structure prediction (CASP,), or for several semi-supervised tasks. Machine learning methods are continuing to win ground in comparison to MSA-techniques with respect to performance, have a short inference time, and can process sequences from the so-called \u201cdark proteome\u201d too, where alignments are not possible.",
        "Amino acid sequences share some similarities with the sequences of letters and words occurring in written language, in particular with respect to the complex interrelationships between distant elements, which are arranged in one-dimensional chains. Thus, recent progress in research on natural language processing (NLP) employing language modeling in a transfer learning scheme has driven forward protein function prediction too (e.g.).",
        "Typically, a language model is first pretrained on large numbers of unlabeled sequences in an unsupervised fashion, e.g. by learning to predict masked tokens (cloze task) or the respective next token in the sequences (which is why this unsupervised approach is also dubbed self-supervised learning). In this way, the model learns useful representations of the sequence statistics (i.e. language). These statistics possibly arise because the amino acid chains need to be stable under physiological conditions and are subject to evolutionary pressure. The learned representations can be transferred to separate downstream tasks, where the pretrained model can be further finetuned in a supervised fashion on labeled data, which are usually available in smaller amounts, considering that sequence labeling by experimenters is costly and lengthy.",
        "Transformer models making use of the attention mechanism, such as bidirectional encoder representations from transformers (BERT,) are currently prevailing architectures in NLP. Transformers have been recently applied to the study of amino acid sequences too, pushing the state of the art in the field of proteomics as well. Recurrent neural networks (RNNs) using long short term memory (LSTM) cells are another model architecture that is particularly suited to process sequential data. RNNs have been successfully employed to protein and peptide property prediction as well, within the scheme of language modeling combined with transfer learning, as sketched out above.",
        "Transformers and other modern deep learning models are notorious for having often millions and sometimes billions of trainable parameters, and it can be very difficult to interpret the decision making logic or strategy of such complex models. The research field of explainable machine learning aims at developing methods that enable humans to better interpret\u2014or to a limited degree: understand\u2014such \u201copaque,\u201d complex models. In certain cases, it was demonstrated that the methods can even help to uncover flaws and unintended biases of the models, such as being mislead by spurious correlations in the data.",
        "Attribution methods, such as integrated gradients (IG), layerwise-relevance propagation or gradient-weighted class activation mapping, make it possible to identify those features in the input space that the model apparently focuses on, because these features turn out to be particular relevant for the final classification decision of the model. Further examples of model explainability methods include probing classifiers, testing with concept activation vectors, and studying the attention mechanism. Explainability methods have been employed in NLP too. Moreover, researchers have started to explore using explainability methods in the area of protein function prediction.",
        "Building upon this previous research on the interpretation of protein classification models, we aimed at exploring how explainability methods can further help to gain insights into the inner workings of the now often huge neural networks, and proceeded as follows.",
        "First, we finetuned pretrained transformers on selected prediction tasks and could push or reach the state-of-the-art (see Supplementary Appendix E). Then, we quantified the relevance of each amino acid of a protein for the function prediction model. Subsequently, we investigated if these relevant sequence regions match expectations informed by knowledge from biology or chemistry, by correlating the relevance attributions with annotations from sequence databases (see Fig.\u00a01). For instance, we addressed the question if a classification model that is able to infer if a protein is situated in the cell membrane does indeed focus systematically on transmembrane regions or not. We conducted this analysis on the embedding level and \u201cinside\u201d of the model with a novel adaptation of IG. In this way, we identified transformer heads with a statistically significant correspondence of the attribution maps with ground truth annotations, across many proteins and thus going beyond anecdotes of few selected cases.",
        "The prediction tasks of inferring GO terms and Enzyme Commission (EC) numbers, that the proteins are labeled with, from their amino acid sequence are detailed in Supplementary Appendix B. This Supplementary material also explains the finetuning of the transformers \u201cProtBert-BFD\u201d and \u201cProtT5-XL-UniRef50\u201d and \u201cESM-2\u201d on the GO and EC tasks, and contains statements about data availability and composition.",
        "We investigated whether specific positions or areas on the amino acid sequence that had been annotated in sequence data bases are particularly relevant for the classification decision of the model (see Fig.\u00a01). Annotations included UniProtKB/Swiss-Prot \u201cactive\u201d and \u201cbinding sites,\u201d \u201ctransmembrane regions,\u201d \u201cshort sequence motifs,\u201d and PROSITE patterns related to a GO term and its children terms in the ontology. Definitions of the aforementioned UniProt annotations (per amino acid) and matching GO terms (class labels of proteins) are compiled in Supplementary Table A.1 (tables/figures with prefix letters are shown in the Supplementary material). First, we attributed relevance indicative for a given class (either a selected GO term or EC number) to each amino acid of a protein. Then, we correlated the relevance heat map obtained for the amino acid chain of a protein with corresponding binary sequence annotations. To study the information representation within the model, the explainability analysis was conducted at the embedding layer and repeated \u201cinside\u201d of the model, separately for its different heads and layers, using a novel method building upon IG, described below in Section 3.",
        "For the experimental evaluation, we focus on the pretrained ProtBert model that was finetuned either to the multi-label GO-classification on the GO \u201c2016\u201d dataset, or to the multi-class EC number classification on the \u201cEC50 level L1\u201d dataset. We consider the comparatively narrow EC task in addition to the much more comprehensive GO prediction, because the test split of the EC dataset contains a larger number of samples that are both labeled per protein and annotated per amino acid, which is beneficial for the conducted explainability analysis. We observed that larger models tend to perform numerically better than smaller models (see Supplementary Appendix E). Given our focus on methodological matters of model interpretation, we deliberately studied ProtBert (420M parameters), because it is better manageable, due to its considerably smaller memory footprint, in comparison to the larger ProtT5 (1.2B).",
        "Integrated gradients represents a model-agnostic attribution method, which can be characterized as unique attribution method satisfying a set of four axioms (Invariance, Sensitivity, Linearity, and Completeness). In this formalism, the attribution for feature i is defined via the line integral (along a path, parameterized as  with , between some chosen baseline  and the sample to be explained ), where F is the function we aim to explain. Choosing  as straight line connecting  and x makes IG the unique method satisfying the four axioms from above and an additional symmetry axiom. This path is the typical choice in applications applied directly to the input layer for computer vision or to the embedding layer for NLP. The approach can be generalized to arbitrary layers if one replaces x and  by the hidden feature representation of the network up to this layer (referred to as \u201clayer IG\u201d in the popular \u201cCaptum\u201d library).",
        "To obtain attributions for individual heads, we have to target the output of the multi-head self-attention (MHSA) block of a particular layer; see Fig.\u00a02 for a visualization of the transformer architecture. Properly separating the attributions of the individual heads from the attribution contribution obtained from the skip connection necessitates to target directly the output of the MHSA. Now, one cannot just simply choose an integration path that connects baseline and sample as encoded by the MHSA block because the input for the skip connection has to be varied consistently. To keep an identical path in all cases, we fix the integration path as a straight line in the embedding layer, which then gets encoded into a, in general, curvilinear path seen as input for some intermediate layer. Choosing not a straight path only leads to the violation of the symmetry axiom, which is not of paramount practical importance in this application; see for other applications with IG applied to general paths. For every sample, this application of IG yields a relevance map of shape , where the first  entries in the last dimension correspond to the first head, followed by the second head etc. By summing over  entries in the last dimension, we can reduce the relevance map to a  attribution map, i.e. one relevance sequence per head.",
        "Each sequence of relevance attributions can then be correlated with sequence annotations to find out if the model focuses on the annotated amino acids. Coefficients of point biserial correlation, which is equivalent to Pearson correlation, were calculated between the continuous relevance values and the corresponding binary annotations per amino acid. This correlation analysis was conducted separately for each head in each transformer layer. The resulting correlation coefficients were then assembled into a  matrix per protein, which entered the subsequent statistical analysis across proteins. Summary statistics over all proteins (which belong to the respective GO or EC class, and, which are part of the respective test dataset split) were obtained by computing Wilcoxon signed-rank tests across the correlation coefficients. The resulting P-values were corrected for the multiple tests per condition (16 heads times 30 layers equals 480 hypothesis tests) by controlling the false discovery rate.",
        "In parallel to the correlation analysis, we furthermore sum the aforementioned attribution map along the sequence dimension, and obtain  entries that specify the relevance distribution onto the different heads. We can carry out the same procedure for every transformer layer and combine all results into a  relevance map of summed attributions. This map makes it possible to identify heads with a positive relevance with respect to the selected class. One map was obtained per protein. Heads with a significantly positive relevance were singled out by calculating a summary statistic across proteins with the Wilcoxon signed-rank test. Finally, the two parallel analysis tracks were combined by identifying transformer heads that feature both a significantly positive (A) relevance-annotation-correlation and (B) relevance (this overlay is displayed in the figures by masking A with B).",
        " Supplementary Appendix D shows implementation details.",
        "The performance results for the ProtT5, ProtBert, and ESM-2 transformers finetuned to the GO and EC protein function tasks are presented in Supplementary Tables E.1 to E.3 in Supplementary Appendix E. In summary, we show that finetuning pretrained large transformer models leads to competitive results, in particular in the most relevant comparison in the single-model category, often on par with MSA-approaches. Larger models lead the rankings, with ProtT5 competing with ESM-2. Finetuning the entire model including the encoder shows its particular strength in the \u201cCAFA3\u201d benchmark.",
        "Starting with embedding layer attribution maps, as the most widely considered type of attribution, we investigate whether there are significant correlations between attribution maps and sequence annotations from external sources (see Section 2.1). We aim to answer this question in a statistical fashion going beyond anecdotal evidence based on single examples, which can sometimes be encountered in the literature.",
        " Figure\u00a03 shows the results of the explainability analysis for the embedding layer of ProtBert finetuned to GO classification. The relevance of each amino acid indicative for selected GO terms was computed with IG, and then correlated with UniProt and PROSITE sequence annotations. Subsequently, it was tested whether the correlation coefficients across all annotated proteins from the test set were significantly positive (see Section 2.1). A significant correlation was observed when relevance attributions indicative for the GO label \u201cmembrane\u201d were correlated with UniProt \u201ctransmembrane regions\u201d (p.05). Correlation was not observed in the GO \u201ccatalytic activity\u201d and \u201cbinding\u201d cases.",
        "The pretrained model is expected to contain substantial information already prior to finetuning; e.g. had identified transmembrane regions using the pretrained ProtT5. Therefore, we inspected the GO membrane case in more detail. The pretrained but not finetuned ProtBert (combined with a classification head trained for the same number of epochs) resulted also in a significantly positive correlation of embedding level attributions to the GO term \u201cmembrane\u201d with transmembrane regions only. Thus common patterns emerge between the pretrained and the finetuned ProtBert.",
        " Figure\u00a04 shows the results of the explainability analysis for the embedding layer of ProtBert finetuned to EC number classification (\u201cEC50 level L1\u201d dataset; i.e. the differentiation between the six main enzyme classes). Relevance per amino acid for each of the six EC classes was correlated with the UniProt annotations as \u201cactive sites,\u201d \u201cbinding sites,\u201d \u201ctransmembrane regions,\u201d and \u201cshort sequence motifs.\u201d It can be observed that the relevance attributions correlated significantly (p.05) with \u201cactive site\u201d and \u201cbinding site\u201d annotations for five out of six EC classes, and with \u201ctransmembrane regions\u201d and \u201cshort sequence motifs\u201d for two, respectively, three EC classes. (Supplementary Figure E.2 shows that positive relevance-annotation-correlation was observed for all annotation types for \u201cEC40\u201d and \u201cEC50\u201d on both levels \u201cL1\u201d and \u201cL2\u201d for several enzyme (sub-) classes.)",
        "Attribution maps obtained for the embedding layer correlated with UniProt annotations on the amino acid level, in particular, in the EC case, but also for the GO term \u201cmembrane.\u201d To summarize, across two tasks, we provide first quantitative evidence for the meaningfulness and specificity of attribution maps beyond anecdotal evidence. Note that the EC case has the benefit of often several hundred annotated samples contained in the test split (except for \u201ctransmembrane regions\u201d and \u201cmotifs\u201d; see right panel of Fig.\u00a04). In comparison, the GO case provides fewer samples in the test split of the dataset that were also annotated on the amino acid level (see numbers in brackets below the x-axis in Fig.\u00a03).",
        "Given the encouraging results presented in Section 5.2, we aim to go one step further and try to answer the more specific question if there are specialized heads inside of the model architecture for specific prediction tasks, using our IG variant that calculates relevance on the amino acid level per transformer head and layer (see Section 3).",
        " Figure\u00a05 shows the results of the explainability analysis inspecting the latent representations inside of the ProtBert model focusing on the selected class of the GO term \u201cmembrane\u201d (GO:0016020). Relevance attributions indicative for GO \u201cmembrane\u201d per amino acid were correlated with the UniProt annotations as \u201ctransmembrane regions\u201d separately for each transformer head and layer (matrix plot pixels in Fig.\u00a05). In parallel, ProtBert heads were singled out with a significantly positive relevance (sum along the sequence) indicative for \u201cmembrane\u201d (see also Section 2.1 and Section 3). Both parallel analysis streams were combined by identifying ProtBert heads with both a significantly positive attribution-annotation-correlation and relevance. Several ProtBert heads in different layers feature a significantly positive correlation of relevance attributions per amino acid with the UniProt annotations as \u201ctransmembrane regions,\u201d going along with a significantly positive relevance for the GO class \u201cmembrane.\u201d In contrast, correlation of relevance attributions with UniProt \u201cactive\u201d or \u201cbinding sites\u201d or \u201cmotifs\u201d or PROSITE patterns accompanied by a positive relevance was not observed (hence these cases were not included in Fig.\u00a05).",
        " Supplementary Figure  E.3 (in Supplementary Appendix E) shows the results of the explainability analysis for the case where the GO term \u201ccatalytic activity\u201d was selected (GO:0003824). Different ProtBert heads stand out characterized by a positive relevance accompanied by a positive correlation of attributions with PROSITE patterns and with UniProt \u201cactive sites\u201d and \u201ctransmembrane regions\u201d (but neither with \u201cbinding sites\u201d nor \u201cmotifs\u201d).",
        " Supplementary Figure  E.4 (in Supplementary Appendix E) repeats the explainability analysis inside ProtBert for the GO term \u201cbinding\u201d (GO:0005488). For several transformer heads and layers, a positive relevance went along with a correlation of relevance attributions with corresponding PROSITE patterns, and with UniProt \u201ctransmembrane regions\u201d (but neither with UniProt \u201cactive\u201d nor \u201cbinding sites\u201d nor \u201cmotifs\u201d).",
        "Subsequently, we conducted the explainability analysis for the case where ProtBert had been finetuned to EC number classification on EC50 level L1. Here, the model had learned to differentiate between the six main enzyme classes. Supplementary Figure E.5 (in Supplementary Appendix E) identifies ProtBert heads characterized both by a positive relevance (sum along the sequence) with respect to the EC class, and by a positive attribution-annotation-correlation (on the amino acid level). The analysis was conducted separately for UniProt annotations as \u201cactive\u201d/\u201cbinding sites,\u201d \u201ctransmembrane regions,\u201d and \u201cmotifs.\u201d (The absence of identified heads for EC4, EC5, and EC6 in the \u201ctransmembrane regions\u201d rows and for EC1 and EC5 in the \u201cmotif\u201d rows of Supplementary Figure\u00a0E.5 goes along with the availability of relatively few \u201ctransmembrane\u201d and \u201cmotif\u201d annotations for these EC classes; see histogram in Fig.\u00a04.)",
        "In summary, we propose a constructive method suited to identify heads inside of the transformer architecture that are specialized for specific protein function or property prediction tasks. The proposed method comprises a novel adaptation of the explainable artificial intelligence (XAI) method of IG combined with a subsequent statistical analysis. We first attributed relevance to the single amino acids per protein (per GO term or EC class), separately for each transformer head and layer. Then, we inspected the correlation between relevance attributions and annotations, in a statistical analysis across the annotated proteins from the test split of the respective dataset. Apparently, different transformer heads are sensitive to different annotated and thus biologically and, respectively, chemically \u201cmeaningful\u201d sites, regions or patterns on the amino acid sequence.",
        "We discuss the benefits of finetuning a pretrained model from end-to-end, and evaluate the XAI method with a residue substitution experiment in Supplementary Appendix E. There, we also discuss the relation of XAI to homology, to the hydrophobicity and charge of residues in transmembrane regions, and to probing and in-silico mutagenesis.",
        "Finally, we studied collective dynamics potentially emerging among the transformer heads (ProtBert, EC50, level L1) by a visualization of the originally high-dimensional, summed attribution maps in two dimensions, taking their similarities into account. For this purpose, the attribution maps that were summed along the amino acid sequence and represented as  matrices (see Section 3) were flattened, resulting in one vector per protein. The dimensionality of these vectors was then reduced with principal component analysis to 50 dimensions, and subsequently to two dimensions with t-distributed stochastic neighbor embedding (t-SNE), using the default t-SNE parameters. The resulting 2D points were visualized as scatter plot and colored according to the corresponding six main enzyme classes (Fig.\u00a06).",
        "The points form distinctive clusters matching the EC labels. Apparently, a structure emerges in the attribution maps, that seems to indicate class-specific collective dynamics among several ProtBert heads. It is important to stress that the attribution map underlying the clustering no longer contains any reference to specific positions in the sequence but relies on the relevance distribution on the different heads through all layers of the model. The emergence of class-specific structures therefore indicates that there are specific combinations of heads that are relevant for a specific classification decision.",
        "This work provides additional evidence for the effectiveness of the currently predominant paradigm in deep-learning-based protein analysis through the finetuning of large protein language models from end-to-end (which brings additional benefits; see Supplementary Appendix E). For different protein function prediction tasks, this approach leads to best-performing models according to single-model performance. The performance level is in many cases on par with MSA-approaches. The proposed models can even be effectively combined with the latter through the formation of ensembles.",
        "Considering the ever increasing model complexity, XAI has started to gain traction in the field of protein analysis too, but quantitative evidence for its applicability beyond single examples was lacking up to now. We provide statistical evidence for the alignment of attribution maps with corresponding sequence annotations, both on the embedding level as well as for specific heads inside of the model architecture, which led to the identification of specialized heads for specific protein function prediction tasks. Emerging class-specific structures suggest that these specialized transformer heads act jointly to decide together in specific combinations. A further detailed analysis of the identified heads could be an interesting next step in future research, potentially based on the query/key/value (QKV) matrices. Internally to the multi-layered model, a direct correspondence between rows/columns of the QKV matrices and individual residues in the sequence is, however, not possible anymore. This limitation makes it, e.g. difficult to infer relations between residues from the QKV matrices.",
        "In summary, XAI promises to tap into the presumably substantial knowledge contained in large models pretrained on massive datasets of amino and/or nucleic acid sequences. Therefore, we expect that XAI will play an increasingly important role in the future of bioinformatics research. We see potential applications of XAI for model validation and for scientific discovery (e.g. of novel discriminative sequence patterns or motifs that have not been identified by experiments or MSA so far). Identifying specialized heads might also help to prune overly large models, making them smaller and more efficient."
    ],
    "title": "Insights into the inner workings of transformer models for protein function prediction"
}