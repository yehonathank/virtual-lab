{
    "content": [
        "Significant progress has been achieved in biomedical text mining using deep learning methods, which rely heavily on large amounts of high-quality data annotated by human experts. However, the reality is that obtaining high-quality annotated data is extremely challenging due to data scarcity (e.g. rare or new diseases), data privacy and security concerns, and the high cost of data annotation. Additionally, nearly all researches focus on predicting labels without providing corresponding explanations. Therefore, in this paper, we investigate a more realistic scenario, biomedical few-shot learning, and explore the impact of interpretability on biomedical few-shot learning.",
        "We present LetEx\u2014Learning to explain\u2014a novel multi-task generative approach that leverages reasoning explanations from large language models (LLMs) to enhance the inductive reasoning ability of few-shot learning. Our approach includes (1) collecting high-quality explanations by devising a suite of complete workflow based on LLMs through CoT prompting and self-training strategies, (2) converting various biomedical NLP tasks into a text-to-text generation task in a unified manner, where collected explanations serve as additional supervision between text-label pairs by multi-task training. Experiments are conducted on three few-shot settings across six biomedical benchmark datasets. The results show that learning to explain improves the performances of diverse biomedical NLP tasks in low-resource scenario, outperforming strong baseline models significantly by up to 6.41%. Notably, the proposed method makes the 220M LetEx perform superior reasoning explanation ability against LLMs.",
        "Our source code and data are available at https://github.com/cpmss521/LetEx.",
        "Biomedical literature and electronic medical records are crucial reservoirs of medical knowledge, essential for advancing scientific research and facilitating clinical applications. However, this medical information typically exists in unstructured or semi-structured natural language formats, posing challenges for physicians to effectively extract useful information from vast amounts of literature and clinical records. Biomedical text mining endeavors to automatically transform unstructured information from biomedical literature into structured knowledge, thereby expediting scientific discovery.",
        "Biomedical text mining, a sub-field of biomedical natural language processing (BioNLP), involves a wide range of subtasks such as biomedical named entity recognition (BioNER), biomedical relation extraction (BioRE), biomedical event extraction (BioEE), and more. Significant research efforts have been dedicated to developing deep learning models in the biomedical field, aiming to assist medical professionals. In recent years, pre-trained language models (PLMs), such as BioBert, PubMedBERT, and ClinicalT5, have demonstrated superior performance across diverse downstream tasks. These models initially were pre-trained on a large-scale corpus and then finetuned on a specific downstream task in a fully supervised manner. However, the transition of these deep learning models from laboratory to clinical applications presents significant challenges for clinical experts. Current state-of-the-art methods heavily rely on substantial amounts of high-quality data manually annotated by domain experts. The process of annotating datasets is both time-consuming and difficult, particularly in medical domains, where privacy and security constraints amplify the intricacies of this process. Thus, in real-world applications, the availability of high-quality labeled datasets is usually limited.",
        "Fortunately, aiming at low-resource scenarios, a new paradigm called few-shot learning (FSL) has been proposed. FSL refers to understanding the underlying patterns in the data from only a few training instances to learn new induction and reasoning. Some methods, such as prototype-based and data augmentation-based approaches, have been proposed for few-shot BioNLP tasks. For example, introduced META-DDIE to predict rare drug\u2013drug interaction events. Aiming to BioNER, proposed knowledge-guided data augmentation based on semantic relation network. However, almost all existing research focuses on predicting the model\u2019s outcome while neglecting the explainability of the result. Faithful explanations ensure that predicted results are accurate, transparent, and trustworthy. Understanding how and why model decisions are made can enhance the trust of medical professionals in the system, which is crucial for drug discovery and clinical decisions.",
        "Therefore, in this work, we focus on a more realistic scenario, i.e. biomedical few-shot learning. This setting is realistic, particularly in the clinical domain, where obtaining a few annotations dataset (e.g. 64-shot) is feasible and efficient. Inspired by LLM\u2019s recent outstanding performance in various zero-shot NLP tasks through step-by-step thinking, we empower this reasoning explanation ability to small-sized models to improve biomedical few-shot learning. Intuitively, reasoning explanations can help the model understand the underlying structure and patterns of the dataset, i.e. how input text is mapped to the label. Thus, we first devised a comprehensive LLM-based workflow using CoT prompting and self-training strategies to collect reasoning explanations corresponding to the labels in the training samples, and used these reasoning chains and labels as supervision signals for the model. Subsequently, we unified all biomedical tasks as text-to-text generation tasks and performed multi-task learning based on the FlanT5 language model to enhance the model\u2019s understanding of the logical mapping between input text and labels, thereby improving its ability to generalize and reason about new samples.",
        "To better validate the effectiveness of the proposed LetEx, this paper investigates from two perspectives: The quality of generated explanations and the performance of models on various few-shot tasks. Experiments are conducted on 3 few-shot settings across 6 biomedical benchmark datasets. The results show that learning from explanation improves the performance of few-shot tasks, outperforming strong baseline models significantly by up to 6.41%. Regarding the quality of reasoning explanations, LetEx demonstrates superior explanation capabilities compared to LLMs across four dimensions: semantic alignment, semantic similarity, logical reasoning, and linguistic coherence. To some extent, our approach can be seen as a small step towards helping biomedical experts make appropriate decisions and interpretable AI. To summarize, our contributions are as follows:",
        "A suite of complete workflow was designed to obtain high-quality reasoning explanations from LLMs based on CoT prompting and self-training strategies, yielding 32,211 interpretable biomedical samples corresponding to labels across 6 datasets. It is a small step toward interpretable AI in biomedical text mining.",
        "Compared to the current mainstream methods that only predict labels, our LetEx generates labels and explanations simultaneously. Moreover, the quality of explanations significantly exceeds that of LLMs such as GPT-3.5 and ChatGLM.",
        "Extensive experiments conducted on three few-shot settings across six biomedical benchmark datasets demonstrate that the proposed LetEx model outperforms strong baseline models by up to 6.41%.",
        " Few-shot learning in BioNLP. In traditional supervised learning, models are trained on large labeled datasets. However, in practical scenarios, access to large amounts of labeled data can be challenging due to factors such as data scarcity, time, or labor costs. In response, researchers have turned to few-shot learning (FSL) techniques. A core idea of FSL is to enable models to learn induction and generalization only from a few labeled instances. One main line in FSL is the utilization of transfer learning and pre-trained language models. Another avenue of research involves meta-learning approaches. For example, introduces META-DDIE to predict rare drug\u2013drug interaction events. improves the derived prototypes accordingly by prototypical network for few-shot biomedical event trigger identification. Additionally, data augmentation techniques are employed. Aiming to few-shot BioNER, proposed knowledge-guided data augmentation based on semantic relation network. Recently, LLMs have brought further performance improvements on few-shot clinical information extractors. Unlike the above methods, we handle all of BioNLP tasks in a unified manner.",
        " Learning from explanations. Explanations can help biomedical experts to better understand the behaviors of deep learning models, which are pivotal in practical applications such as medical decisions, drug discovery, and predicting diseases. Previous studies have demonstrated satisfactory performance, but cannot provide faithful explanations of the predicted results. Recently, learning with explanations has received increasing attention. For example,) used few-shot CoT prompting to elicit GPT-3 to generate answers step by step. fine-tuned reasoning chains to enhance the reasoning ability of language models. In the biomedical domain, proposed multi-task instruction tuning integrating positive examples with their explanations to enhance FSL. Different from these works, we first focus on reasoning explanations of small-sized language models. Secondly, we provide answers and explanations at the same time, rather than using reasoning explanations before predicting answers.",
        "A main challenge is that the current biomedical datasets lack annotation explanations of labels. To this end, we first collect the reasoning explanations corresponding to each sample label from the training set using an LLM, without accessing the model parameters. Figure\u00a01 illustrates a suite of complete explanation generation workflow, designed using LLM-based CoT prompting and self-training strategies. Concretely, it contains three consecutive stages: CoT prompt generation, discrimination and filtering, and self-training.",
        " CoT prompt generation. To generate high-quality explanations, we utilize CoT prompting to elicit LLMs to generate reasoning explanations step by step. Specifically, a demonstration pool is initially constructed. We randomly select a few (e.g. 20) examples (, ) from each training dataset, and manually craft a reasoning explanation  for each instance according to the annotation guidelines, thereby constituting the candidate demonstration pool . Then, given the training sample , we use k-nearest neighbors (KNN) to dynamically select one demonstration  from candidate demostrations pool that most closely resembles . Next, we append each demonstration  to the training sample pair  and , using it as input to prompt the LLM to generate a reasoning explanation , i.e. ",
        "An example of CoT prompting is presented in Fig.\u00a02. In contrast to, we leverage the input text  and label  to generate reasoning explanations , aiming to minimize potential errors in the explanation.",
        " Discrimination and filtering. However, the hallucination of LLMs is still inevitable. Prior work conducts a human evaluation to determine the quality of reasoning. Generally, a high-quality reasoning explanation implicitly includes propositional logic related to the input text. In other words, when there is an entailment relationship between the input text and the reasoning explanation, the generated explanation is considered reliable. This is similar to a text entailment task. Thus, the problem is converted into a text entailment task by mapping (, ) into (, ), where  and  are viewed as premise , and hypothesis , respectively, and the label is assigned as either entailment or contradiction. Technically, we train a nature language inference (NLI) discriminator to filter noise samples (, ), where explanations  are prone to hallucination. Specifically, we adopt pretrained PubMedBERT-MNLI-MedNLI, trained on the MultiNLI, and MedNLI datasets, to serve as NLI discriminators by further fine-tuning. The discriminator is also a few-shot learning. By comparing the final predicted label of the explanation  with the ground-truth label  of the input text , we identify evident error text-explanation pairs (i.e. premise-hypothesis pairs) and assign their label as a contradiction. Similarly, previously manually crafted explanations for each dataset are considered as entailment labels. Based on the converted premise-hypothesis samples, we further fine-tune the NLI discriminator.",
        " Self-training. Then we adopt a self-training strategy to improve discriminator. Concretely, the predictions for the entailment label with the highest confidence are collected as new training samples, which are then used to re-train an improved discriminator model. After two to three iterations, we ultimately filtered out 1100 error explanations samples and retained 32\u00a0211 high-quality interpretable training samples across six datasets. These retained samples are used to construct support sets for simulating few-shot scenarios in Section 4.2.",
        "Intuitively, reasoning explanations provide detailed information on how target label  can be derived from the input text , usually containing task-related knowledge that is difficult to obtain solely from the text . In other words, these reasoning explanations offer additional supervision for training samples, enhancing the understanding of the logical mapping between text-label pairs. Hence, we propose the LetEx model, which learns from explanations as illustrated in Fig.\u00a03. The LetEx model takes an input sentence with task-specific prefixes and simultaneously generates both labels and their corresponding explanations. The backbone of the model is based on the 220M parameter FlanT5-base. Different prefixes represent different tasks. The label prediction task uses [task_prefix: ] and the explanation generation task utilizes [explain task_prefix: ] as the model prefix, where task_prefix usually is the name of the dataset. In the case of BioRE (i2b2 2010 dataset), the model receives the text sequence \u2018Basilar artery stenosis with  syndrome.\u2019 and then is trained to generate the target label \u2018PIP\u2019 along with the corresponding reasoning explanation \u2018PIP explanation: The medical problem of  So the label is PIP.\u2019. The prefixes \u2018i2b2: \u2019 and \u2018explain i2b2: \u2019 indicate the relation extraction task and the explanation generation task, respectively. Learning to explain enables the model to better generalize the target labels rather than overly relying on memorizing the current input text. The training objective of LetEx model is to joint loss  and : where N represents the number of training samples,  (or ) is the ground truth label (or explanation) for the ith sample, and  (or ) is the corresponding prediction.  (or ) represents the cross-entropy loss for the generated label (or explanation) prediction,  is a hyper-parameters for the loss of label and explanation.",
        "The LetEx model is evaluated on 6 popular benchmark datasets involving three biomedical tasks.",
        " BioNER. NCBI and BC5CDR-disease are utilized to extract disease entities in this task. To identify the named entities of the biomedical text, we insert the corresponding entity labels around the entity spans as target output during the training phase, e.g. for the input text \u2018Genotype and phenotype in patients with dihydropyrimidine dehydrogenase deficiency\u2019, the target output is \u2018Genotype and phenotype in patients with disease* dihydropyrimidine dehydrogenase deficiency *disease\u2019.",
        " BioRE. The task of BioRE aims to extract the relation of entities. We conduct relation extract on the i2b2 2010, HPRD50, and AIMed. The former originates from clinical medical notes, whereas the latter two come from biomedical literature. For this task, we directly generate the corresponding label text.",
        " NLI. MedNLI dataset is employed for NLI task. The MedNLI consists of sentence pairs with premise and hypothesis, where the task is to determine the logical relationship between them, typically categorized as \u2018entailment\u2019, \u2018contradiction\u2019, or \u2018neutral\u2019. We packet the premise-hypothesis pair to a text sequence. More details about datasets can be found in Supplementary Appendix A.",
        "In our experiments, to evaluate the few-shot performance across three distinct tasks, micro-averaged F1 scores are utilized for entity and relation extraction, while accuracy is used for the NLI task. More details on the experimental implementation are provided in Supplementary Appendix B.",
        "In the few-shot setting, we follow N-way K-shot setting, where N classes are randomly sampled from the retained training samples, with K examples per class, to construct a support set  for training. In here,  and  denote input text and corresponding gold label,  is reasoning explanation related to lable , and all categories in the dataset are considered, i.e. . Then, the model is evaluated on all test (query) samples Q (). Although N*K samples are chosen for the support set S, the total number in S could exceed N*K (such as in BioNER task). Therefore, the same downsampling algorithm is adopted as by to construct support set. In the work, considering practical scenarios, we simulated three low-resource scenarios, namely 16-shot, 32-shot, and 64-shot.",
        "The primary goal of our study is to train a small-size model that can match or outperform LLMs in the performance of both domain-special tasks and generated explanations. In this way, in practical applications, we can avoid high computational costs when deploying LLMs. To measure this goal, the large language models by zero-shot CoT prompt and domain-special small-sized language models based on vanilla fine-tuning are considered as the baseline models, specifically (1). LLMs, ranked by their scale, include the 175B GPT-3.5-turbo, 6B ChatGLM, and 3B FlanT5-XL, all of which exhibit robust CoT reasoning capabilities. Following, \u2018Let\u2019s think step by step\u2019 is appended to the prompt instructions to predict target tasks with LLMs (detailed prompt instructions are given in Supplementary Appendix C). (2). Small-sized language models include encoder-only domain-special PLMs such as BioBERT and PuMedBERT, and encoder\u2013decoder PLMs like T5 and ClinicalT5.",
        " Table\u00a01 presents the experimental results in few-shot scenarios, specifically 16-shot, 32-shot, and 64-shot, across six biomedical benchmark datasets. Overall, the proposed LetEx demonstrates superior performance across three few-shot scenarios in all datasets. Firstly, LetEx demonstrates superior performance compared to current LLMs across six datasets in the 16-shot setting, significantly surpassing GPT-3.5-turbo with 175 billion parameters. Notably, LetEx achieves F1 score improvements of up to 17.15% on the HPRD50 dataset and 17.2% on the AIMed dataset. The same increasing trend is observed with LLMs having either 3B FlanT5-XL or 6B ChatGLM. Secondly, compared to T5 and ClinicalT5, LetEx with the same parameter scale achieves a 2.14% improvement in F1 score on the i2b2 2010 dataset, a 4.29% improvement in F1 score on the HPRD50 dataset, and a 6.41% increase in accuracy on the MedNLI dataset, under a 32-shot setting. These observations suggest that learning from explanations helps LetEx gain a deeper understanding of the underlying mappings between text-label pairs, enhancing its comprehension of the data rather than merely memorizing labels of training samples. This approach allows the model to capture complex patterns and intrinsic structures of the data, ultimately improving its generalization and reasoning ability when handling new samples. A possible reason for the significant improvement in MedNLI is that the base model of LetEx, FlanT5-base, has undergone specialized instruction fine-tuning on other textual entailment datasets, which has enhanced LetEx\u2019s generalization and reasoning capabilities for similar tasks. Additionally, LetEx provides corresponding explanations for the prediction results, which is crucial in clinical scenarios. This feature is not available in the same scale language models, such as T5, ClinicalT5, BioBERT, or PubMedBERT. We also find that although LLMs perform well in various tasks, fine-tuning task-specific small-size models can make their performance on target tasks still match or even surpass those LLMs. For example, under 16-shot setting, the vanilla fine-tuning based on encoder\u2013decoder PLMs achieved on NCBI and BC5CDR-disease comes close to 175B GPT3.5-turbo.",
        "A series of ablation studies are conducted to validate the effect of learning from explanations, as shown in Fig.\u00a04. It can be concluded that learning from explanations makes LetEx perform better. Refocusing the LetEx model solely on label predictions, without corresponding explanations, resulted in varying degrees of reduction in F1 scores. Except for HPRD50, reductions of 2.81%, 0.5%, and 4.55% were observed on NCBI, BC5CDR-disease, and i2b2 2010 dataset, respectively, under the 32-shot setting. The same declining trend can be observed in both the 16-shot and 64-shot settings. It demonstrates that most of the benefits come from the explanations. The main reason is that learning from explanations allows the model to induce and generalize training samples by understanding and reasoning the connection between input text and label pairs.",
        "We further investigate the gap between few-shot learning and full supervision. Figure\u00a05 illustrates the performance variation of LetEx from k\u2009=\u200916 to full supervision on the HPRD50 and i2b2 2010 datasets. Compared to the baseline model fine-tuned with the ClinicalT5 on the full dataset, LetEx achieves comparable performance on HPRD50 with only 64-shot, resulting in an 11-fold increase in data efficiency. Similar observation also appeared on i2b2 2010 dataset. As the training samples increase from low resources to the full dataset, the gap between the two continues to narrow. Meanwhile, LetEx also shows decent performance on both full datasets. In comparison to fine-tuning ClinicalT5, the proposed LetEx model outperforms 0.46% and 1.25% F1 score on HPRD50 and i2b2 2010, respectively. While falling behind by 1.42% compared to fine-tuned BioBert on the HPRD50 dataset, LetEx maintains a slight advantage on the i2b2 dataset.",
        "To objectively evaluate the quality of generated explanations, we utilize the ROSCOE, a suite of scoring metrics for reasoning chains, to measure the correlation between text-label pair and generated explanations across four dimensions, i.e. semantic alignment (ROSCOE-SA), semantic similarity (ROSCOE-SS), logical inference (ROSCOE-LI), and language coherence (ROSCOE-LC). Each metric is constrained within the range of [0, 1], where 1 signifies highest correlation, while 0 means irrelevant. We include more ROSCOE details in Supplementary Appendix D. We report the quality evaluation of the explanations generated from the HPRD50 test datasets, which are summarized in Table\u00a02 and Fig.\u00a06. Overall, compared to baseline models, LetEx performs better across most metrics after being trained on high-quality explainable datasets. We observe that 3B FlanT5-XL usually directly generates answers and does not provide corresponding reasoning explanations, resulting in lower semantic overlap with the input text. Consequently, the generated explanations have lower correlations on Faithfulness-*, Info-Step, Perplexity-Chain, and Perplexity-Step. Notably, the Repetition-* and Self-Consistency measure the correlations between reasoning steps in explanations. Therefore, there are no Repetition-* and self-consistency scores as there are no reasoning steps to compare. As scaling up model parameters, this trend diminishes gradually from 6B ChatGLM to 175B GPT-3.5-turbo, bringing higher quality explanations. Rather than pursuing unlimited expansion of model parameters, the quality of explanations generated by 220M LetEx models, trained to specific tasks, is higher, which enhances confidence in clinical medical decision-making and facilitates faster deployment model in practice. Additionally, we assess the explanation quality of the i2b2 2020 dataset, the detailed analysis results are presented in Supplementary Appendix D.",
        "The success of LetEx relies on the quality of the explanations collected from LLMs during the training phase. Therefore, we further explore how the quality of the collected generative explanations during training affects the performance of the LetEx model.",
        "Specifically, based on the complete workflow on explanation generation in Fig.\u00a01, we conduct ablation study on AIMed and MedNLI datasets by removing different settings. As shown in Table\u00a03, the test results for the AIMed and MedNLI datasets across various few-shot scenarios are reported using the F1 score and accuracy, respectively. We first removed only the Discriminator, leading to the most significant performance drop among all factors, ranging from 0.35 to 3.11. This indicates that the Discriminator plays a crucial role in filtering out erroneous explanation samples and retaining high-quality, interpretable training samples. When demos are ablated, i.e. without demonstration  in Equation (1), the performance of LetEx shows slight fluctuations across various few-shot settings. This is primarily because the GPT-3.5-turbo language model is sufficiently powerful to generate reasonable explanations for training sample labels even in zero-shot scenarios. When labels are removed, we observe a consistent decline in various few-shot scenarios. Additionally, when we replace GPT-3.5-turbo with the ChatGLM language model-to-generate label explanations for the training samples, the performance declines to varying extents, with reductions of up to 2.75 F1 score.",
        "Overall, all these settings ensure that the label explanations collected during training are high quality. High-quality explanations provide more accurate logical mapping between text-label pairs, which can significantly improve the performance of the LetEx model in few-shot learning. Conversely, poor-quality explanations may introduce ambiguity or misinformation, leading to suboptimal performance. Subsequently, we explored how the explanations collected under different settings during the training phase impact the quality of explanations generated by LetEx during the testing phase. More details are included in Supplementary Appendix E.",
        "In order to evaluate the advantages of the proposed LetEx method, we conduct further analysis by random sampling on the i2b2 2010 dataset, as shown in Table\u00a04. We find that FlanT5-XL and ChatGLM directly predict the answers without giving reasoning steps in case 1. Worse still, ChatGLM produces the wrong answer. Although GPT-3.5 provided corresponding reasoning explanations, the incorrect reasoning led to an ultimately wrong prediction result. In contrast, LetEx pointed out that the sentence did not mention the outcome of the treatment, and thus the relation labels is TrAP. A similar situation occurred in Case 2. Both FlanT5-XL and ChatGLM directly generated relation labels. Notably, ChatGLM repeated the answer during the explanation without providing any reasoning steps. Compared to GPT-3.5, ChatGLM, and FlanT5-XL, our LetEx generates correct answers and faithful explanations in both cases. By learning from the explanation, our LetEx gains the ability that understand the underlying structure and patterns of the dataset, i.e. how the input texts are mapped to pre-defined labels. Therefore, LetEx not only accurately predicts labels but also generates reliable explanations in low-resource settings. More case studies can be found in Supplementary Appendix F.",
        "As mentioned in Section 6.4, the success of the LetEx relies on the quality of the explanations collected from LLMs. Although we show great improvement on the six biomedical benchmark datasets, if the task involves complex knowledge reasoning, the explanation generated by LLMs will become less credible. Additionally, during filtering, NLI discriminators can only approximate human judgments regarding the plausibility of generated explanations, inevitably introducing additional noise. Combining biomedical domain knowledge with LLMs to generate robust reasoning explanations will be an interesting direction for future work.",
        "In this article, considering realistic scenarios, we propose a novel LetEx method that learns from the explanations to enhance the inductive reasoning ability of few-shot learning. Specifically, we initially gathered high-quality explanations by devising a suite of complete workflow based on LLMs through CoT prompting and self-training strategies. Then we convert all biomedical NLP tasks into a text-to-text generation task in a unified manner. These reasoning chains provide additional supervision for LetEx through multi-task training and prompt learning, enabling it to predict labels and corresponding explanations simultaneously. Experiments are conducted on 3 few-shot settings across six biomedical benchmark datasets and the results show that learning from explanation could improve the performance of few-shot tasks, outperforming strong baseline models significantly by up to 6.41%. In addition, the quality of generated explanations from LetEx is superior obviously to LLMs such as GPT-3.5-turbo and ChatGLM, which enhances confidence in clinical medical decision-making and facilitates faster deployment models in practice."
    ],
    "title": "Learning to explain is a good biomedical few-shot learner"
}