{
    "content": [
        "We introduce Medical evidence Dependency (MD)\u2013informed attention, a novel neuro-symbolic model for understanding free-text clinical trial publications with generalizability and interpretability.",
        "We trained one head in the multi-head self-attention model to attend to the Medical evidence Ddependency (MD) and to pass linguistic and domain knowledge on to later layers (MD informed). This MD-informed attention model was integrated into BioBERT and tested on 2 public machine reading comprehension benchmarks for clinical trial publications: Evidence Inference 2.0 and PubMedQA. We also curated a small set of recently published articles reporting randomized controlled trials on COVID-19 (coronavirus disease 2019) following the Evidence Inference 2.0 guidelines to evaluate the model\u2019s robustness to unseen data. ",
        "The integration of MD-informed attention head improves BioBERT substantially in both benchmark tasks\u2014as large as an increase of +30% in the F1 score\u2014and achieves the new state-of-the-art performance on the Evidence Inference 2.0. It achieves 84% and 82% in overall accuracy and F1 score, respectively, on the unseen COVID-19 data.",
        " MD-informed attention empowers neural reading comprehension models with interpretability and generalizability via reusable domain knowledge. Its compositionality can benefit any transformer-based architecture for machine reading comprehension of free-text medical evidence.",
        "Evidence-based medicine (EBM) calls for the incorporation of the best available medical evidence from systematic research into clinical decision making for principled patient care. Much medical evidence is locked in free-text randomized control trial (RCT) publications. As vast evidence bases such as PubMed grows exponentially and rapidly, evidence retrieval and appraisal become extremely difficult due to information overload. It usually takes more than 30 minutes for a clinician to search for evidence needed to answer one clinical question encountered during patient care. In practice, however, their busy clinical routines can only spare less than 2 minutes for such laborious searches, resulting in limited translation of evidence from research to practice. Therefore, it is imperative to develop scalable and automated medical evidence extraction and comprehension methods. Methods have been developed for evidence retrieval, data elements extraction, automated systematic review, and clinical question answering (QA). In this study, we focus on machine reading comprehension (MRC).",
        "MRC is the technology that teaches a machine to read unstructured text, mimic the inference process of human readers, and then answer questions about it. Efficient comprehension and synthesis of medical evidence in the literature is no trivial task\u2014even for medical experts. An example abstract from and a related clinical question are shown in Figure\u00a01. The abstract reports an interventional study that assessed the effectiveness of respiratory rehabilitation for elderly coronavirus disease 2019 (COVID-19) patients. The clinical question asks whether respiratory rehabilitation can significantly improve 2 outcomes: anxiety and depression. We highlight the text in which inference is made to answer \u201cyes\u201d for anxiety and \u201cno\u201d for depression based on the following rationale: (1) the answer comes from the conclusion about the interventional group (not control); (2) anxiety and depression are measured by Self-Rating Depression Scale (SDS) and Self Rating Anxiety Scale (SAS) scores; and (3) in the interventional group, both scores decreased but only the difference in SAS is statistically significant.",
        "Early QA systems for improving patientcare relied heavily on biomedical ontologies\u2014such as the UMLS Metathesaurus\u2014and lexico-syntactic patterns to extract biomedical concepts as candidate answers, followed by a scoring function (eg, TF-IDF, LexRank) for answer ranking. Generating answers from automatically constructed knowledge graphs is another technique for answering clinical questions. A factorized Markov network was used to construct a clinical knowledge base from clinical notes. Recent breakthroughs of pretrained language models such as ELMo and BERT show significant performance improvement on multiple tasks including QA and MRC. Neural approaches in the biomedical domain have benefited from these advances. It is common practice in biomedical MRC to introduce attention variants from general NLP applications, followed by domain adaptation by fine tuning on a biomedical corpus. For example, Du et al used biomedical word embedding and a hierarchical multilayer transfer learning model with a co-attention mechanism by Xiong et al and Wiese et al to concatenate general word embeddings with biomedical embeddings and adopt FastQA in the attention layer to develop an extractive QA system. While most of the prior work in the biomedical domain only incorporates biomedical concepts through concept embeddings or general transfer learning from large biomedical corpora, we dedicate our efforts to design an efficient neural approach to make use of relevant domain knowledge and improve the model\u2019s reasoning capability over medical evidence text.",
        "All previous work can be categorized as either symbolic or statistical. The idea behind a symbolic approach is to teach machines to understand language in the same top-down manner that humans do\u2014learning and using rules as well as symbolic representations of knowledge\u2014which is explainable and offers good performance in reasoning tasks as expert systems do. However, this technique relies heavily on human-driven knowledge engineering and has had limited success in understanding and deciphering contextual information. Recent state-of-the-art results in natural language processing (NLP) have been achieved predominantly by statistical methods, particularly the deep learning models. These bottom-up data-driven approaches have shown significant advantages in learning latent and sophisticated representations probabilistically. However, their reasoning capabilities are still rather limited when compared with symbolic AI. In addition, the lack of transparency and the requirement for extensive training data to fit these models become 2 severe drawbacks. These challenges are exacerbated in the healthcare domain by the lack of trust in machines among clinicians. Other challenges facing text comprehension for medical literature have also been identified, such as that (1) models suffer from lengthy text and the long distance dependencies throughout the articles and (2) the complexities in clinical studies limit the neural models\u2019 ability to efficiently incorporate domain knowledge and develop clear intuitions around strong patterns denoting complex concepts. The attention mechanism and its variants conditioned on question text have been applied to such problems and only achieve modest predictive gains.",
        "Therefore, in this study, we aim to design a neuro-symbolic MRC model to understand free-text medical evidence (eg, RCT publications) by leveraging both the high capacity of neural networks as well as the expressiveness of symbolic methods. The traditional technique used to combine the 2 approaches is multitask learning with hard parameter sharing between symbolic knowledge representations for medical evidence and neural reading comprehension models. Their potential shortcomings include overfitting and dependency on the quality of the parser. By synergizing neural and symbolic methods, our goal is to improve the interpretability, reasoning ability, and task generalizability of neural networks by adding reusable domain knowledge. Our contributions are 3-fold: (1) we propose a symbolic representation, called medical evidence dependency (MD), to represent the compositional elements of medical evidence; (2) we propose a novel attention mechanism, MD-informed attention, which provides compositional submodel for any Transformer-based language models and is able to pass linguistic and domain knowledge onto later layers; and (3) we integrate MD-informed attention into BioBERT to evaluate the model\u2019s ability to understand and synthesize unstructured medical evidence on 2 public benchmarks. MD-informed attention substantially improves BioBERT performance and achieves new state-of-the-art performance.",
        "First, we define a simple and computable representation for medical evidence, which represents compositional evidence elements and relations among them. A medical evidence element is an atomic entity in a finding. We adopt the PICO framework developed for formulating clinical questions to retrieve evidence from literature to define 4 types elements (P, I, C, and O):",
        " Population the characteristics of the study population",
        " Intervention the primary intervention considered",
        " Comparator comparison for the intervention",
        " Outcome the anticipated measures, improvements, or effects",
        "Additionally, we define 2 new attribute classes to represent necessary context: observation (quantitative or qualitative results with respect to an outcome measure) and count (the count of participants observed to have the same result for an outcome measure) Then we define the directional relationships between a pair of evidence elements, called MD, with one element being the governor and the other being the dependent. The directions are fixed to Intervention(Comparator)\u2192Observation \u2192Outcome. Example MD-structured text is shown in Figure\u00a02. Using the elements and the dependency, we can construct a \u201cmedical evidence proposition,\u201d a compositional unit of medical evidence. In the example, 2 medical evidence propositions are formulated from the extracted intervention, outcome, and observation elements. Both represent an observed clinical fact with respect to the outcomes (cardiac index became higher; vascular resistance was decreased) after the intervention is applied.",
        "Most of the current neural NLP models use the Transformer introduced by Vaswani et al as their backbone, such as BERT, XL-Net, and GPT-2. The multihead attention mechanism is used to capture global interactions across the text in multiple \u201crepresentation subspaces.\u201d Such an architecture offers flexibility and potential to teach the model to learn a \u201csubspace\u201d in the medical domain. The conventional neural attention mechanism is unsupervised when learning to attend to relevant inputs. In this study, we train the self-attention to attend to the MD as a mechanism for passing both linguistic and domain knowledge to subsequent layers, and we hypothesize that our model can better attend to relevant text and improve reasoning capability over long-distance evidence for clinical questions (Figure 3).",
        "Inspired by Strubell et al, in which syntactic dependency is integrated into attention for semantic role labeling, we design an MD matrix, a specialized adjacency matrix for the directed graph induced by MDs from text. The MD matrix, like self-attention, captures global dependencies within text segments (Figure\u00a04). When a MD is identified between a pair of terms, 1 is assigned to the corresponding slot in the matrix; otherwise, 0 is assigned. In addition, because intervention elements are in the top hierarchy in the MDs among all others, we define every recognized intervention element as dependent on itself and assign 1 to the corresponding slot in the matrix. Figure\u00a04 shows an MD matrix for the example text.",
        "Conventional self-attention adopted the scaled dot-product attention, in which the attention is weighted sum of the values (Value). The weight assigned to each value is determined by the dot-product of the query (Query, ie, the information we are looking for) with all the keys (Key [ie, the relevance to the query]). The detailed explanation is given in Vaswani et al.",
        "Here, we modify the weights (scaled dot-product of Query and Key) to make it relevant to medical evidence. In one self-attention head from the Transformer, we drop in the MD matrix to replace the scaled attention score generated from the dot product of Query and Key (Figure\u00a04), and take its Softmax to compute new weights. Then by computing a new weighted sum of Value, we get a context representation ZMD specialized to attend to medical evidence: ",
        " Figure\u00a03 depicts the overall architecture of one multihead self-attention module with MD-informed attention. The attention heads within black boxes in Figure\u00a03 contain the MD-informed attention values. We leave the other attention heads in multihead self-attention as default to learn their own attention representation Z from their Query, Value, and Key, and concatenate the learned ZMD from MD-informed attention head with the rest of the conventional context layers to obtain Z as the final output of one attention module in the Transformer (the top layers in Figure\u00a03). By introducing MD-informed attention, the neural reading comprehension model can make efficient end-to-end use of domain knowledge. In addition, because MD is global, the model can efficiently capture and reason over long-distance evidence relations.",
        "The parser extends our previous work and annotated dataset. We module the task of extracting Medical Evidence Elements as named entity recognition, and parsing Medical evidence Dependency as relation extraction. Both named entity recognition and relation extraction models are trained by modifying the last layer and fine-tuning a biomedical version of BERT on the dataset. It achieves a micro-F1 score of 0.72 for 5-class named entity recognition and 0.92 for extracting MDs among PICO elements (details in Supplementary Appendix). We apply this parser to construct the MD matrix and MD-informed attention head. It is worth noting that this can be replaced when a more advanced method or tool is available.",
        " MD-informed self-attention is compatible with any Transformer-based model and can support various natural language understanding tasks on unstructured medical literature. In this study, we evaluate its effectiveness under the BioBERT architecture and present results on 2 shared benchmark datasets for text comprehension for the medical literature, Evidence Inference 2.0 and PubMedQA.",
        "  Evidence Inference 2.0    :  Evidence inference and synthesis is a key task in practicing EBM. Entries in this dataset consist of an intervention (eg, chemotherapy), a comparator (eg, surgery), and an outcome (eg, 5-year survival rate of operable cancers), along with an associated article. The task is to infer the comparative performance of the 2 treatments with respect to the outcome based on the article to tell if there was a significant increase, a significant decrease, or no significant change between the intervention and comparator. The prompts labeled as invalid or whose answers cannot be found in the article abstract are filtered out before training.",
        "  PubMedQA    :  This is a machine reading comprehension dataset for biomedical research questions. The task is, given a question and a relevant piece of medical literature (a context), predict an answer of yes, no, or maybe. The questions in the dataset are constructed from the titles of PubMed articles, while the context is a structured abstract with the Conclusion sentences omitted. No filtering is done on this dataset. During preprocessing, each question-context pair is separated by the special token [SEP]. Particularly in Evidence Inference 2.0, questions are given in terms of \u201cprompts,\u201d each specifying an Intervention, a Comparator, and an Outcome. Every prompt and context are processed as:",
        "[O] Outcome [I] Intervention [C] Comparator [SEP] abstract text",
        "Basic statistics and examples for 2 benchmark datasets after preparation are provided in Figure\u00a05.",
        "We tested MD-informed attention on 2 benchmarks. When constructing the MD matrix for the prompts or questions, the questions from PubMedQA are processed as the same way as the abstracts: first we identify the Medical evidence Dependencies and then construct the MD matrix accordingly. For Evidence Inference prompts, because the element types are given, we assign 1 to all pairs of words in Intervention and Comparator. Special tokens like [O], [SEP] are left as 0 in the matrix. The model is then trained to select one correct answer from multiple choice options (Evidence Inference 2.0: \u201csignificantly increased,\u201d \u201csignificantly decreased,\u201d and \u201cno significant difference\u201d; PubMedQA: \u201cyes,\u201d \u201cno,\u201d and \u201cmaybe\u201d).",
        " MD-informed attention is integrated into BioBERT and pretrained on biomedical corpus and SQUAD 2.0 for biomedical QA, by replacing one conventional Self-Attention head in the Transformer Encoder (henceforth, such systems referred as BioBERT-MDAtt). To evaluate the robustness of MD-informed attention, we also apply an attention mask on this attention head to randomly remove part of learned dependencies (BioBERT-MDAtt-masked), by setting each pair of words in MD matrix assigned 1 to 0 with a probability of p.",
        "We compare BioBERT-MDAtt results to the 2 baselines on both tasks.",
        " State-of-the-art performance: For the Evidence Inference 2.0 dataset, we compare our results to the best performance reported in, and the top system on the leaderboard. In, the best model predicts answers using a BERT to BERT, 2-stage pipeline. A variant of RoBERTa pretrained over scientific corpora serves as the base model. The first BERT identifies evidence bearing sentences within an article for given PICO elements. The second then classifies the answer using the evidence extracted from the first stage. In the up-to-date leaderboard, the top system applies a similar strategy and outperforms the original system by 2%. The state-of-the-art system for PubMedQA, reported in Jin et al\u2014which is also the top performer on their leaderboard\u2014adopts a multiphase fine-tuning of BioBERT on both labeled and unlabeled data collections. In our experiments on PubMedQA, only labeled QA pairs are used.",
        " BioBERT for QA: Additionally, we implement BioBERT for biomedical QA as another strong baseline, with all attention heads left on their own to learn. The last layer is modified to adapt to our question type and fine-tuned on both datasets (referred as BioBERT).",
        "Given that the information necessary to answer the question might be scattered throughout the abstract, we fix a large number, 384, as the maximum sequence length while training all the models. All BERT models deployed in this study are BERT-Base, with 12 attention heads. If MD-informed attention is applied, one head will be replaced. We fine-tuned all other underlying parameters. We trained all models using the Adam optimizer with a learning rate 2e-5. All systems are implemented in TensorFlow 1.14.0 and trained on 4x NVIDIA GeForce RTX 2080 Ti GPUs.",
        " Table\u00a01 lists the main results on the Evidence Inference 2.0 test set. Our proposed BioBERT-MDAtt model achieves the new state of the art (macro-F1: 0.843, micro-F1: 0.844, accuracy: 0.84), over 4% absolute macro-F1 higher than previously reported best models. The baseline model that fine-tunes on BioBERT achieves 0.55 macro-F1, comparable to the reported performance (0.51 macro-F1) of the BERT Pipeline without conditioning on recognized PICO elements in DeYoung et al. We report per-class performance from BioBERT and BioBERT-MDAtt in Table\u00a02. Simple addition of MD-informed attention brings substantial improvement\u2014almost a +0.30 increase in macro-F1 score and accuracy.",
        " Table\u00a01 shows the performance of the model with P\u2009=\u2009.4. The performance drops slightly compared with the model with the attention mask, but MD-informed attention still outperforms the previous state of the art. Compared with the prior models, in which the final label is predicted based upon evidence sentence extracted in the prior stages, BioBERT-MDAtt is conducted as a completely end-to-end pipeline and leverages knowledge in a domain-agnostic way rather than running the risk of overfitting to the training data.",
        "There are multiple data collections in PubMedQA, and only PQA-L(abeled) includes human-curated answers. However, it is an extremely low resource setting in which there are 1000 abstracts and only 450 training question-answer pairs in each fold of cross-validation. All evaluations in Tables\u00a02 and 3 are carried out on the PQA-L test set of 500 QA pairs by 10-fold cross validation. Two systems from Jin et al are compared against ours. The multiphase system in PubMedQA paper achieves the state-of-the-art performance by multiphase fine-tuning BioBERT, first on a large unlabeled corpus and then on PQA-L (over 200\u00a0000 abstracts in total). The Final Phase system in Table\u00a03 is trained by fine-tuning BioBERT only on PQA-L (1000 abstracts). To evaluate the effectiveness of the MD-informed attention, we also only use PQA-L data to train our MRC models in this study, thus our models are comparable to the Final Phase Only model.",
        "Our baseline model, fine-tuning BioBERT on PQA-L, achieves comparable results to Final Phase Only system from the PubMedQA article. The effects of incorporating the MD-informed attention head into BioBERT are reported in Table\u00a02. The BioBERT-MDAtt model achieves near state-of-the-art performance with substantially less data (macro-F1: SOTA 0.527 using 200 000 abstracts vs BioBERT-MDAtt 0.482 on 1000 abstracts) and showed considerable improvement upon the counterpart models (+0.17 in macro-F1 to the BioBERT baseline, and +0.19 to Final Phase Only model). Of note, the strategy adopted in the state-of-the-art (SOTA) system does not contradict with ours. It is easy to combine the two (ie, training MD-informed attention on PQA-L data after multiphase fine-tuning on large unlabeled corpus) and benefit from both strategies. This combination theoretically can achieve better results than individual approaches. Additionally, we notice both models have low performance for this class given the inherent ambiguity of \u201cmaybe\u201d class (Table\u00a02). Consistent with what we observe in the Evidence Inference 2.0 task, when masking is applied at P\u2009=\u2009.4 the performance drops slightly, but the addition of MD-informed attention head still results in a significant improvement in the model\u2019s performance. The results on PubMedQA task show that, by applying neuro-symbolic approach, the model can generalize over tasks via reusable knowledge and achieve better results with less data. We believe that our model has great potential to excel when a larger dataset is available.",
        "For both tasks, the evaluations in Table\u00a02 reveal that, replacing one conventional attention head with MD-informed attention in BioBERT results in extensive improvement in all measures. The MD-informed attention helps BioBERT further generalize over different tasks by reusable domain knowledge. More importantly, this improvement is understandable via human-readable symbolic form introduced by Medical evidence Dependency. In addition, because MD-informed attention is adaptable to any Transformer-based model (ie, most of the state-of-the-art language models), it provides a beneficial feature as being compositional and easy to be integrated. Therefore, MD-informed attention can serve as a reusable submodel to benefit any Transformer-based architecture and improve their abilities in understanding free-text medical evidence.",
        "To further evaluate the robustness of MD-informed attention, we curate a small set of recently published PubMed abstracts reporting clinical trials on COVID-19. We selected this disease domain for evaluation because the studies in this domain have only started to accumulate recently, which provides us unseen examples for both the MD parser and the MRC model. Following the annotation guidelines from Evidence Inference 2.0, we create 50 \u201cprompt-abstract\u201d pairs from 10 abstracts that report RCTs of COVID-19 and make it available in the Supplementary Appendix. BioBERT-MDAtt trained on Evidence Inference 2.0 (performance reported in Tables\u00a01 and 2) is applied to predict the 50 pairs.",
        "We evaluate the model from 3 aspects: (1) performance on unseen data, (2) reasoning capabilities over variance of the expressions for Intervention/Comparator/Outcome, and (3) reasoning capabilities over long-distance evidence relationships. To do so, while creating prompt-abstract pairs, we intentionally replicate the original pair and replace elements in the prompts with their variants occurring in the other sections in abstract\u2014a model with good reasoning capability should predict the same results for the pairs. For instance, consider the 2 pairs of prompts created from the article shown in Figure\u00a01:",
        " [O] anxiety [I] respiratory rehabilitation [C] without any rehabilitation intervention ",
        " [O] SAS score [I] respiratory rehabilitation [C] without any rehabilitation intervention ",
        "The 2 are asking the same question: if the intervention has significant effect on anxiety compared with the comparator, which we should infer from the abstract, then it should significantly affect the SAS score (which is used to quantify anxieties). We report the BioBERT-MDAtt model performance in Table\u00a04. Overall, even though this is an unseen dataset for both the parser and MRC model, the F1 score only drops slightly compared with original evaluation on the Evidence Inference 2.0 test set (from 0.84 to 0.82). A total of 42 of 50 pairs are answered correctly, indicating that our proposed model is robust. From examining the detailed results, we find that the model can answer both variants correctly for the created prompt pairs. The most common error that it makes is to misclassify \u201cno significant difference\u201d as 1 of other 2 labels. For example, from the example in Figure\u00a01, \u201cSAS and SDS scores in the intervention group decreased after the intervention, but only anxiety had significant statistical significance within and between the 2 groups,\u201d the model misclassified \u201cdepression\u201d as significantly decreased instead of correctly reasoning over the adversative transition.",
        "By visualizing the MD-informed attention head for the example text just mentioned (Figure\u00a06), the isolated attention is visualized, showing connecting from the word \u201cscore\u201d to all the words or tokens in a separated sentence that generates separated medical evidence propositions. The MD-informed attention head learns to highlight the relevant evidence components like \u201canxiety,\u201d \u201cstatistical significance\u201d and \u201cgroups,\u201d and the highest weight comes the pair \u201cscore\u201d to \u201canxiety,\u201d congruent with the facts that they both belong to outcome class and \u201c(SAS) score\u201d is the quantified measure for \u201canxiety.\u201d This shows that MD-informed attention is able to capture clinically meaningful or understandable interactions across different medical evidence propositions, instead of being a \u201cblack box\u201d for practitioners. In future work, we would like to incorporate MD-informed attention into more advanced models to further test its effectiveness.",
        "In this study, we present and evaluate a novel attention mechanism, MD-informed self-attention, for understanding and reasoning over free-text medical evidence such as RCT publications. By integrating MD-informed self-attention into BioBERT, and evaluating on 2 benchmarking tasks, we gain substantial improvement over BioBERT with the conventional multihead attention. We also outperform the prior state of the art on one task, and achieve near state-of-the-art performance with considerably less data on the other. By synergizing neural and symbolic methods, we introduce reusable knowledge and empower existing neural reading comprehension models with better understandability, reasoning ability, and task generalizability. In addition, because MD-informed attention is adaptable to any Transformer-based model (ie, most of the state-of-the-art language models), its compositionality is a beneficial feature to any Transformer-based architecture and can improve their abilities in understanding free-text medical evidence."
    ],
    "title": "A neuro-symbolic method for understanding free-text medical evidence"
}