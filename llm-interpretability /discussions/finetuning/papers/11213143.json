{
    "content": [
        "Feature attribution, the ability to localize regions of the input data that are relevant for classification, is an important capability for ML models in scientific and biomedical domains. Current methods for feature attribution, which rely on \u201cexplaining\u201d the predictions of end-to-end classifiers, suffer from imprecise feature localization and are inadequate for use with small sample sizes and high-dimensional datasets due to computational challenges. We introduce prospector heads, an efficient and interpretable alternative to explanation-based attribution methods that can be applied to any encoder and any data modality. Prospector heads generalize across modalities through experiments on sequences (text), images (pathology), and graphs (protein structures), outperforming baseline attribution methods by up to 26.3 points in mean localization AUPRC. We also demonstrate how prospector heads enable improved interpretation and discovery of class-specific patterns in input data. Through their high performance, flexibility, and generalizability, prospectors provide a framework for improving trust and transparency for ML models in complex domains.",
        "Most ML models are optimized solely for predictive performance, but many applications also necessitate models that provide insight into features of the data that are unique to a particular class. This capability is known as feature attribution, which in unstructured data (e.g., text, images, graphs) consists of identifying subsets of the input datum most responsible for that datum\u2019s class membership (e.g., pixels or patches of an image, often represented as a heatmap). Feature attribution is especially important for scientific and biomedical applications. For example, for a model to assist a pathologist in making a cancer diagnosis, it ideally should not only accurately classify which images contain tumors, but also precisely locate the tumors in each image.",
        "Unfortunately, modern ML systems can struggle to perform feature attribution. Most existing attribution techniques attempt to provide \u201cexplanations\u201d for trained classifiers (Figure 1) \u2014 descriptions of how model weights interact with different input features (e.g., gradients, attention) or of how each feature contributes to prediction (e.g., SHAP, LIME). Explanation-based attribution methods are inherently (a) data-inefficient as they require ample labeled training data to train underlying classifiers. Additionally, methods producing explanations can themselves be (b) computationally inefficient and thus may not actually improve tractability relative to annotation by domain experts, particularly for large inputs. Finally, (c) the attributed features are often found to be inaccurate and irrelevant to target classes.",
        "We explore whether foundation models (FMs) can be used to solve challenges (a\u2013c) without traditional explanations. Prior work demonstrates that FMs learn high quality data representations and can learn class-specific properties through a few labeled examples. However, it is unclear whether FM representations can be used to perform feature attribution in a scalable and accurate manner. Our key insight is to build on top of FM representations, rather than explain an FM fine-tuned as an end-to-end classifier.",
        "In this work we present prospector heads (a.k.a. \u201cprospectors\u201d), simple modules that aim to equip feature attribution to any encoder \u2014 including FMs \u2014 just as one would equip classification heads. Prospectors inductively reason over two layers: layer (I) categorizes learned representations into a finite set of \u201cconcepts\u201d and layer (II) learns concepts\u2019 spatial associations and how those associations correlate with a target class. To (a) enable data efficiency, prospectors are parameter-efficient and with only hundreds of parameters. To (b) limit time complexity, prospectors operate with efficient data structures and linear-time convolutions, all without model backpropagation. To (c) improve attribution accuracy, prospector heads are explicitly trained to perform feature attribution, unlike explanation methods.",
        "We show that prospector heads outperform attribution baselines over multiple challenging data modalities. Prospector-equipped models achieve gains in mean area under the precision-recall curve (AUPRC) of 8.5 points in sequences (text documents), 26.3 points in images (pathology slides), and 22.0 points in graphs (protein structures) over the top modality-specific baselines. Additionally, we show that prospector-equipped FMs are particularly robust to variation in the prevalence and dispersion of class-specific features. Finally, we also present visualizations of prospectors\u2019 internals and outputs to demonstrate their interpretability in complex domain applications.",
        "To adequately motivate our approach (Section 3), this section focuses on central methods ideas. We present a full version of Related Work, including baselines, in Appendix A.",
        "In the current explanation-based paradigm, feature attribution is performed by (1) training a supervised model before (2) interrogating the model\u2019s behavior (e.g., via internals, forward or backward passes, or input perturbations) and inferring class-specific features. Both model-specific (e.g., gradients, attention maps) and model-agnostic (e.g., SHAP, LIME) methods of today are either computationally prohibitive or poor localizers of class-specific features.",
        "Most modern encoders for unstructured data operate on tokens, or relatively small pieces of a datum, and their representations. Tokens can be user-prespecified and/or constructed by the encoder itself (potentially with help from a tokenizer), where these encoders are respectively referred to as partial-context and full-context (Figure 2). Due to computational constraints, high-dimensional unstructured data (e.g., gigapixel images) often require user-prespecified tokens (i.e., patches) and partial-context encoders that embed each token.",
        "Gradient-based saliency and attention maps have been used to explain partial-context classifiers for high-dimensional unstructured data like gigapixel imagery. However, studies report low specificity and sensitivity in part because attribution for the entire datum is built by concatenating attributions across prespecified tokens. Partial-context strategies incorrectly assume prespecified tokens are independent and identically distributed (IID).",
        "The use of concepts in ML inherently increases model interpretability by forcing models to reason over unstructured data with respect to said concepts. Concepts themselves can be human-derived, machine-derived, or co-derived with humans in the loop.",
        "Early concept-based methods examine models\u2019 use of concepts in prediction, while recent methods can also attribute concept importance in situ. Sets of concepts can also form a hidden layer, i.e., \u201cbottleneck\u201d, offering a form of multimodal grounding when concepts are human-derived. More recently, concepts are being assigned to pre-specified tokens in high-dimensional data, e.g., subsequences and sentences. These \u201cspatially resolved\u201d concepts have allowed for hierarchical concept formation when paired with LLMs.",
        "Prospectors are designed to perform few-shot feature attribution for high-dimensional data while meeting challenges (a-c). Instead of explaining end-to-end classifiers, prospectors interface with encoders by adapting their token embeddings. Crucially, prospectors foster a form of inductive reasoning over token embeddings to learn class-specific patterns. The use of tokens as the core unit of analysis depends on the key assumption that the equipped encoders have learned adequate distributional semantics in large-scale pretraining. Prospectors can then learn class-specific patterns in small labeled datasets via a simple two-layer module. In layer (I), prospectors transform token embeddings into spatially resolved concepts learned from the training set, constructing a parsimonious \u201cvocabulary\u201d or \u201ccodebook\u201d that can be user-verified and/or user-defined. Layer (II) then attributes scores to each token using a novel form of graph convolution that operates on concept frequencies and co-occurrences. The following sections describe the inference and fitting procedures of each layer.",
        "To enable any encoder to perform feature attribution regardless of input modality, we first define a generalized language for unstructured data. Any unstructured datum can be represented by a map graph  where each vertex  represents a discrete token, or piece of that datum in Euclidean space (Definition C.2).  is composed of  tokens. For example, in image data, tokens can be defined as pixels or patches. An edge  connects vertex  to . Both \u2019s token resolution and token connectivity are defined based on data modality (Figure S1).",
        "Suppose we have a dataset containing map graphs  and binary class labels . We assume that a classy graph  contains a set of classy-specific vertices , with . One main goal of feature attribution is to locate  in each datum given a set of  pairs as a training dataset. This task is inherently coarsely supervised and is discussed further in Appendix A.",
        "Prospectors receive token embeddings  from an equipped encoder and update map graph  such that each vertex  is featurized by an embedding . This vertex-specific \u201cfeature loading\u201d uses the notation: . Details for partial- and full-context encoders are specified in Appendix C.1.",
        "Next, prospectors use an encoder\u2019s learned semantics to define  spatially resolved concepts . This is achieved by quantizing each token embedding  as a scalar concept . When the quantize layer (Section 3.4.1) is applied over the full graph , it is transformed into graph  with the same topology as , but with categorical vertex features . We refer to  as a data sprite due to its low feature dimensionality compared to  (a data compression ratio of ). Intuitively, the heterogeneity of  is parameterized by the choice of . This layer is depicted in Figure 4.",
        "Prospectors next perform feature attribution using a form of graph convolution over sprite . This convolution requires a global kernel  that computes an attribution score  for each vertex  based on the concepts  (i.e., monograms) and co-occurrences  (i.e., skip-bigrams) present within the graph neighborhood defined by receptive field . The kernel  serves as a form of associative memory and can be conceptualized as a dictionary, scoring each concept monogram or skip-bigram in the combinatorial space , where  is the Cartesian product. The global kernel is fit over the training set (Section 3.4.2).",
        "To perform feature attribution at inference time, we apply the fitted kernel over each vertex in a datum to produce a prospect map  is a map graph with the same topology as  and  but featurized by scalar continuous attribution scores . We call this layer K2conv in reference to kernel \u2019s implicit structure (Definition C.5). An attribution score  is computed for each vertex  in , where  represents all vertices within the -neighborhood of  (including  itself):  where  denotes dictionary lookup. The above expression resembles the energy function for 2D Markov random fields, but adjusted to allow for longer-range dependencies in the second term via skip-bigrams. The resulting prospect map  targets class-specific region  by assigning high absolute positive or negative values to each token. Intuitively,  parameterizes the level of smoothing over  by modulating the number of neighboring tokens used to compute a token\u2019s importance. This layer is depicted in Figure 5.",
        "In our implementation, layers (I) and (II) are fitted separately and sequentially using the procedures below. Further details for each layer are found in Appendix C.3.",
        "Token embeddings sampled from across the training set are partitioned into  subspaces using an unsupervised algorithm (e.g., -means clustering). Afterward, each subspace represents a semantic concept  discovered in the corpus. To reduce computation, clustering can be performed over a representative sample (> 103) of the token embedding space, randomly sampled without replacement. Fitting is depicted in Figure 4.",
        "Fitting the K2conv kernel involves computing the class-attribution weights for each monogram and skip-bigram in  across the training set. These weights represent the only learnable parameters of a prospector head. The total number of parameters  is thus dependent on  and is at maximum (Appendix C.3.2): . The kernel is fit in two steps, as outlined below.",
        "For each sprite  in the training set, prospectors first build a datum-level representation in order to learn dataset-wide patterns. This is performed by the rollup operator, which traverses \u2019s vertices, tracks concept monogram and skip-bigrams , and counts their frequencies over all -neighborhoods. This operation constructs a sprite embedding , which resemble \u201cbag-of-words\u201d vectors with longer-range \u201cskip\u201d interactions. Sprite embeddings are rescaled to account for differences in baseline frequencies (e.g., using TF-IDF) and thus can be viewed as probabilities:  for monograms and  for skip-bigrams. The rollup operator and this step as a whole are described in Algorithm 1 and Figure S2.",
        "Prospectors next use the datum-level sprite embeddings  to learn a vector  of class-specific weights for each monogram and skipbigram across the entire training set. After fitting , we construct  as a dictionary mapping each element in  to its corresponding weight in . We implement two approaches to learning weights, which make up the two main prospector variants: a linear classifier  and a parameter-free fold-change computation. These variants are discussed further in Appendix C.3.3 and depicted graphically in Figure 5.",
        "This variant trains a linear classifier  to learn a mapping from  over the training dataset. The learned coefficients  then represent the class-specific importance of each index in . We implement this as a logistic regression with elastic net regularization with the mixing hyperparameter .",
        "Inspired by bioinformatics, this variant involves first computing mean sprite embeddings for each class over the training data. For example, for the negative class, , where  is the subset of the training dataset  for which . This mirrors the \u201cbaseline vector\u201d commonly used by popular feature attribution methods. Then, we compute  as a fold-changes  and select significant weights using a hypothesis test for independent means. The latter step serves as a form of regularization.",
        "Prospectors overcome the limitations of current feature attribution methods by observing the following design principles. Firstly, for (a) data efficiency and few-shot capabilities, prospectors are parameter efficient due to the sole use of concept monograms and skip-bigrams to build its kernel \u2014 at maximum only requiring  parameters. Both variants for computing importance weights  are thus data efficient due to their parsimony. Secondly, prospectors are (b) computationally efficient: by operating as an equippable head, prospectors are \u201cplug-in-ready\u201d without encoder retraining and or backpropagation. The combination of efficient data structures and modeling primitives such as dictionaries and convolutions allow prospectors to efficiently scale feature attribution to high-dimensional data: namely, linear-time with respect to the tunable number of tokens . We outline runtime complexity and speed benchmarking in Sections C.4 and D.1. Finally, prospectors achieve (c) improved localization and class-relevance by explicitly training on token embeddings to learn  instead of using end-to-end classifiers to identify  post hoc. We detail other favorable model properties in Appendix C.6.",
        "We evaluate prospectors using three primary tasks, each representing a different data modality (sequences, images, and graphs). Each also poses unique challenges for prospector training and feature attribution: class imbalance (sequences), high input dimensionality with few examples (images), and very coarse supervision (graphs). As is common in scientific and biomedical data, all three datasets are amenable to the multiple instance assumption (MIA) \u2014 that class1 data largely resemble class0 data with the exception of tokens only found in class1 data. Details for each dataset\u2019s construction are shared in Appendix D.5. For each task, we select representative encoders to which we equip prospector heads and relevant baseline attribution methods. We summarize encoders in Table 1 (and Appendix D), baselines in Appendix D.9, and ruled-out baselines in Appendix B.",
        "For both baselines and prospectors, we perform a grid-search over tunable hyperparameters. Due to the MIA, the best models were selected based on their ability to localize ground truth class1 regions in the training set, since these were not seen by prospectors during training. We use a sequential ranking criteria over four token-level metrics: precision, dice coefficient, Matthews correlation coefficient, and AUPRC. Details of hyperparameter tuning and model selection are found in Appendix D.2 and D.3. The results in the remainder of this paper present the localization AUPRC and average precision (AP) over a set of thresholds, for class1 regions in our held-out test data.",
        "Retrieval is an important task in language modeling that provides in-text answers to user queries. For this task, we use the WikiSection benchmark dataset created for paragraph-level classification. We repurpose WikiSection to assess the ability to retrieve target sentences specific to a queried class. We specifically use the \u201cgenetics\u201d section label as a query, and class1 data are defined as documents in the English-language \u201cdisease\u201d subset that contain this section label. Our goal is to identify sentences that contain genetics-related information given only coarse supervision from document-level labels. After preprocessing the pre-split dataset, our dataset contained 2513 training examples (2177 in class0, 336 in class1) and 712 test examples. The relationship between sentences in each document is represented as a graph with 2-hop connectivity (Figure S1).",
        "We assess two pretrained language models, MiniLM and DeBERTa, used at partial-context. While DeBERTA is an off-the-shelf LLM for zero-shot classification (ZSC) and natural language inference (NLI), MiniLM is a sentence and paragraph embedding model \u2014 thus requiring prospectors to perform feature attribution at the sentence-level.",
        "For baselines attribution methods, we present a mix of (1) supervised heads and (2) off-the-shelf LLM inference. Firstly, supervised heads train on token-level class labels to identify class-specific sentences in testing. Specifically, we train a multi-layer perception (MLP) on labeled token embeddings and a one-class support vector machine (SVM) trained solely on class0 token embeddings. In the latter case, we perform novelty detection to identify class1 tokens. While not traditional explanation methods, the MLP and SVM heads are given a large advantage as semi- and fully supervised baselines (as opposed to prospectors, which are coarsely supervised at the datum-level). For LLM inference, we used DeBERTA to output sentence-level ZSC probabilities (i.e., logits), NLI entailment scores, NLI entailment attention, and pooled Shapley values for ZSC. Implementation details are listed in Appendix D.9.",
        "Identifying tumors is an important task in clinical pathology, where manual annotation is standard practice. We evaluate prospectors on Camelyon16, a benchmark of gigapixel pathology images, each presenting either healthy tissue or cancer metastases. All images are partitioned into prespecified 224 \u00d7 224 patch tokens and filtered for foreground tissue regions. After preprocessing the pre-split dataset, our dataset contained 218 images for training (111 for class0 and 107 for class1) and 123 images for testing. The relationship between patches in each image is represented as a graph using up to 8-way connectivity (Figure S1).",
        "We equip prospectors to four encoders: tile2vec, ViT, CLIP, and PLIP. The first two encoders are trained with partial context, where tile2vec is unsupervised while ViT is weakly supervised with image-level label inheritance. Details on encoder training are provided in Appendix D. CLIP serves as a general-domain vision-language foundation model (VLM) and PLIP serves as a domain-specific version of CLIP for pathology images. Both VLM encoders are pretrained and used for partial-context inference on prespecified image patches. We choose two popular and computationally feasible explanation-based attribution baselines (Section 2): concatenated mean attention for ViT and concatenated prediction probability for ViT, CLIP, and PLIP.",
        "Many proteins rely on binding to metal ions in order to perform their biological functions, such as reaction catalysis in enzymes, and identifying the binding-specific amino acids is important for engineering and design applications. We generated a dataset of metal binding sites in enzymes using MetalPDB, a curated dataset derived from the Protein Data Bank (PDB). Focusing on zinc, the most common metal in the PDB, we generate a gold standard dataset of 610 zinc-binding (class1) enzymes and 653 non-binding (class0) enzymes (see Appendix D.5.3). Each protein structure is defined using the positions of its atoms in 3D space and subdivided into tokens representing amino acids (a.k.a. \u201cresidues\u201d). The relationship between residues is represented as a graph with edges defined by inter-atomic distance (Figure S1). This task is particularly challenging due to potentially overlapping class-specific features (i.e., proteins of both classes are metal-binders), highly heterogeneous background data (proteins in train and test sets adopt a wide variety of structural folds), and relatively small target regions, making this an example of a \u201cneedle-in-the-haystack\u201d task.",
        "We apply prospector heads to three encoders: COLLAPSE, an FM which produces embeddings of the local 3D structure surrounding each residue; ESM2, a protein LLM which produces embeddings for each residue based on 1D sequence; and a simple amino acid encoder (AA), where each residue is one-hot encoded by amino acid identity. By construction, ESM2 is a full-context encoder while COLLAPSE and AA are partial-context encoders. We present three baselines built on top of a supervised GAT classifier head (trained on protein-level labels) to identify binding residues: Attention, Shapley values (SHAP), and GNNExplainer. Implementation details are listed in Appendix D.9.",
        "In all tasks, prospectors achieve higher AUPRC and AP than baseline methods, often with large improvements (Figure 6). For text retrieval, we improve mean test-set AUPRC to 0.711 from 0.626 (i.e., 8.5-point gain) with the top supervised baseline (MLP head) and from 0.584 with the top LLM inference baseline (NLI entailment) \u2014 in summary, MiniLM with an equipped prospector head is able to outperform DeBERTa\u2019s baselines by 12.7 points in AUPRC despite being 5\u00d7 smaller in size and with relatively limited pretraining (Table S5). We also observe improved localization over baselines for Camelyon16 (26.3 points in AUPRC and 8.8 points in AP) and MetalPDB (22.0 points in AUPRC and 8.8 points in AP). For the MetalPDB dataset, the optimal methods tend to exhibit bimodal performance, with almost perfect predictions for a subset of the test dataset (particularly cysteine-dependent binding motifs, see Figure 9) and poor performance on other subsets, resulting in the clustering of points around 0.5 and 1.0 AUPRC. This behavior suggests that AP more clearly reflects task performance, highlighting the ability of prospectors to identify small conserved binding patterns.",
        "While prospectors overall improve localization performance over baselines regardless of the chosen encoder, the performance gain is maximized by choosing domain-specialized encoders for each dataset. For Camelyon16 and MetalPDB, the combination of prospectors with FM encoders (CLIP, PLIP, COLLAPSE) showed the strongest localization results, as shown in Figure 6. Among FMs, the best-performing encoders are those with the most task-specificity \u2014 PLIP has a domain advantage by virtue of being a CLIP-style encoder trained on pathology images, while COLLAPSE accounts for complex 3D atomic geometry rather than simply amino acid sequence (as in ESM2) or one-hot encoding (AA). Interestingly, we note that the AA encoder presents an exception to encoder generalization, supporting that prospectors themselves can identify salient patterns with rudimentary encoder semantics. This is likely due to the fact that many zinc-binding motifs rely on atomic coordination by three to four cysteine residues, which are otherwise rarely found in such arrangements. For tasks which require the detection of less amino acid-dependent structural patterns, we expect the COLLAPSE encoder to result in optimal prospector performance.",
        "Next, we explore the relationship between the properties of class-specific regions and localization performance. To characterize class-specific regions, we compute two metrics acting as proxies for coarse supervision (Section 3.2): region prevalence (# class1 tokens / # tokens) and mean region dispersion (# connected components / mean component size). For Camelyon16, we plot the relationship between test-set AUPRC and both metrics in Figure 7. Full results over all datasets are presented in Appendix D.11. For each plot, we also display the top baseline method.",
        "Firstly, we observe that most encoders exhibit a positive correlation between region prevalence and localization AUPRC across all modalities. However, some encoders are particularly robust to region prevalance and achieve high AURPC despite low prevalence (MiniLM, PLIP, COLLAPSE), and prospectors are consistently more robust than top baselines over all data modalities. Secondly, mean region dispersion and localization performance (both AUPRC and AP) demonstrate a parabolic relationship \u2014 indicating that some level of dispersion is needed for detectable regions, while too much dispersion makes the task challenging. These results recapitulate each task\u2019s challenges: the pathology task contains a wide range of dispersion values, while the protein task contains the lowest levels of prevalence and highest levels of dispersion (Appendix C.6). Despite these task differences, prospector-equipped FMs demonstrate an high levels of robustness to coarse supervision across modalities.",
        "In addition to improved localization performance, prospectors are inherently interpretable because their parameters provide insights into invariant class-specific patterns. Prospect maps visualize the feature attribution outputs in the input token space \u2014 but importantly, these maps can be further contextualized by visualizing prospector internals themselves. Due to the use of learned semantic concepts, the global convolutional kernel can be represented as a semantic network or as a heatmap (Appendix C.2), along with each input example as it passes through layers of the prospector head.",
        "We illustrate this interpretability for pathology images (Figure 8) and protein structure (Figure 9) using two test-set examples. We first visualize data sprites, which reflect the learned concepts mapped onto data inputs (from layer (I)). By analyzing semantic concepts on the data sprite, it is possible to assign domain-specific meaning to each concept. Additionally, by visualizing concept and co-occurrence frequencies in the sprite embedding, we can identify over- or under-represented patterns within each input. By visualizing the global kernel, which captures dataset-wide concept associations and their correlations with class labels, it is possible to cross-reference between the sprite and the class-specific regions of the resulting prospect map. The ability to visualize the internals of a prospector head in terms of concepts facilitates human-in-the-loop model development and the incorporation of domain knowledge, a major advantage relative to \u201cblack box\u201d models.",
        "Our pathology visualization (Figure 8) demonstrates a kernel with \u201chub,\u201d or densely connected and highly predictive concepts: concept #4 is indicative of class1 while concept #9 is indicative of class0. Such kernels demonstrate how prospectors do not detract from the rich semantics offered by FM encoders like PLIP for pathology data.",
        "Visualizing kernels for protein structures outlines prevalent class1-specific concepts in training data (e.g., concepts #7, #17) that are rare in the test set but nonetheless are critical for classification. Despite their low frequency, top prospectors achieved performant localization for this task. The distributional shift between train and test set is a likely explanation for the bimodal localization performance on this task, and suggests that improvements to kernel design and fitting (e.g., feature scaling and choice of ) along with constructing optimally representative training datasets (e.g., for a more varied class0) would improve prospector performance on more difficult data subsets.",
        "Further analysis of learned parameters can also help to better understand the nature of discovered patterns. For example, there may be more than one pattern which results in a particular class label, and differentiating examples that exhibit each pattern can uncover mechanistic subgroups of the data. To demonstrate this, we hierarchically cluster the sample-level sprite embeddings in the MetalPDB test set. This identified two major subgroups of zinc binding sites (Figure S7) defined by the number of cysteine residues coordinating the bound ion. One subgroup is enriched for proteins which contain four coordinating cysteines, while in the other there are one or more histidine residues involved in the binding interaction. Figure 9 shows an example from each cluster, including a visualization of the zinc-binding site on the far right. This finding recapitulates known subtypes of zinc binding motifs, and more broadly demonstrates the potential for prospectors to discover biological mechanisms when applied to less well-studied phenomena.",
        "This work presents prospector heads, encoder-equippable modules for (a) data-efficient, (b) time-efficient, and (c) performant feature attribution. We show that prospectors are both modality-generalizable and encoder-agnostic with particularly dominant performance when equipped to domain-specialized FMs. Finally, we show that prospectors are interpretable through their use of concept-based kernels.",
        "Prospectors\u2019 improved localization performance over explanation-based baselines calls into question the underlying assumption of explanations themselves: that end-to-end classifiers implicitly \u201csegment\u201d data in the input token space en route to making class predictions, and that these \u201csegmentations\u201d can be extracted post hoc. Our results suggest that using machine-derived concepts and modeling class-specific associations directly in the input token space helps to avoid such modeling assumptions.",
        "We believe a key driver of prospectors\u2019 performance is the combination of token-level representations with the local inductive bias provided by convolution. This combination fosters a form of inductive reasoning through \u201ctoken mixing\u201d and kernel construction. Several other aspects of prospector design draw inspiration from ideas across ML research (Appendix A.3), giving insights into their performance characteristics. Our results suggest that FMs in particular contain strong distributional semantics which yield precise feature attribution even with partial-context encoders and coarse levels of supervision. In other words, FMs (in tandem with quantization) remove the burden of long-context reasoning by reducing input data to mosaics of concepts (i.e., sprites). Prospectors can thus functionally operate over long-range dependencies even with a local inductive bias. This claim of capturing short- and long-range dependencies between tokens is backed by prospectors\u2019 localization robustness to region prevalence and dispersion. Additionally, because domain-specific FMs do improve performance when they are available (e.g., PLIP vs. CLIP), we hypothesize that as FMs continue to improve and be adapted to new applications and data modalities, so will the utility of prospectors across diverse domains.",
        "Prospectors are flexible and modular by design, enabling not only variable encoders but also simple changes in their fitting. Of the two variants we fitted, the non-trainable fold-change variant was superior for almost all evaluated settings (Appendix D.3). This may be because the variant explicitly learns dataset-wide concept associations and deviations from a class0 \u201cbaseline vector\u201d \u2014 which closely reflects the MIA (Section 4.1). It is possible that different kernel fitting methods may be better suited to detecting different types of class-specific patterns, but further investigation is needed to explore this question.",
        "One limitation of this work is the lack of sensitivity analysis for all design choices and hyperparameters. For example, due to time and compute constraints, we relied on domain knowledge to select token resolution and connectivity for each task instead of testing their impacts on performance. Furthermore, we did not study the choice of clustering method nor embedding sample size in the quantization step, and we limited our experimentation to open-source encoders only. Future work involves Pytorch implementation for GPU acceleration, enabling kernels to learn higher-order -grams, adding new variants for kernel fitting, deployments on varied data modalities, and exploring prospector utility with frontier non-Transformer architectures (e.g., state-space models and their attention hybrids) and API-locked LLMs.",
        "We anticipate many potential use cases for prospectors, particularly in tandem with vector databases and in other compound AI systems and agentic workflows. One particular use case is to screen or classify data with FMs equipped with performant classifier heads, and then swap in prospector heads when feature attribution is required. This process can enable users to investigate multiple class labels (e.g., scientific phenomena) without encoder retraining. Another use case is to use prospector-generated attributions to train downstream rationale models. In general, we believe that prospectors expands the toolkit for improving the transparency and utility of large FMs, high-dimensional data, and large-scale datasets \u2014 ultimately inspiring new few-shot inference modes for FMs. For scientific and biomedical applications, including in data-scarce settings, prospectors have the potential to provide mechanistic insights and discover phenomena in complex data.",
        "Trust and safety considerations are increasingly important as AI becomes an increasingly prominent part of high-impact disciplines such as science and biomedicine. This concern is particularly relevant for large \u201cblack box\u201d foundation models. The goal of this work is to provide a new approach to feature attribution for large models and complex datasets to improve transparency of AI systems. It is important to note that that our method is specifically not designed to be an explanation of a model\u2019s reasoning, and any feature attributions made by prospector heads should be carefully interpreted by the user in the context of the data modality.",
        "In the current explanation-based paradigm, feature attributions are referred to as \u201cexplanations\u201d and are performed by (1) training a supervised model before (2) interrogating the model\u2019s behavior (e.g., via internals, forward or backward passes, or input perturbations) and inferring class-specific features. This framework can be described as weak or coarse supervision due to the sole use of class labels as a supervisory signal in combination with a low signal-to-noise ratio in the datum-label pairs \u2014 particularly when the prevalence of class-specific features is low.",
        "Explanations, and feature attribution more broadly, can be categorized as either model-specific methods, which aim to describe how model weights interact with different input features), or model-agnostic methods, which aim to describe how each feature contributes to prediction. Explanation-based attribution methods in general are inherently data-inefficient as they require ample labeled training data to train underlying classifiers. It should be noted that few methods can also be applied to all data modalities.",
        "Model-specific methods like gradient-based saliency maps, class-activation maps (CAMs), and attention maps use a classifier\u2019s internals (e.g., weights, layer outputs), forward passes, and/or backpropagated gradients for attribution. Recent work has demonstrated gradients serve as poor localizers \u2014 potentially due to their high sensitivity to inputs, unfaithfulness in reflecting classifiers\u2019 reasoning processes and propensity to identify spurious correlations despite classifier non-reliance. Furthermore, CAMs pose high computational costs with multiple forward and backward passes. Finally, attention maps are demonstrably poor localizers also perhaps due to their unfaithfulness and difficulties in assigning class membership to input features.",
        "On the other hand, model-agnostic methods like SHAP and LIME perturb input features to determine their differential contribution to classification. Recent work has shown SHAP struggles to localize class-specific regions and is provably no better than random guessing for inferring model behavior or for downstream tasks. Furthermore, SHAP-style methods can be computationally expensive for a variety of reasons. Some methods face exponential or quadratic time complexities with respect to the number of input features (e.g., pixels in an image) and are thus infeasible for high-dimensional data, while others require multiple forward and/or backward passes or require training additional comparably sized deep networks along with the original classifier.",
        "Most modern encoders for unstructured data operate on tokens, or relatively small pieces of a datum, and their representations. Tokens can be user-prespecified and/or constructed by the encoder itself (potentially with help from a tokenizer) \u2014 where these encoders are respectively referred to as partial-context and full-context (Figure 2). Due to computational constraints, high-dimensional unstructured data (e.g., gigapixel images) often require user-prespecified tokens (i.e., patches) and partial-context encoders that embed each token.",
        "We provide an illustrative example for the image modality. In this setting, determining encoder context is based on practical modeling constraints: computational complexity of an architecture\u2019s modeling primitives, input data dimensionality, and hardware. For example, an attention-based Vision Transformer experiences quadratic time complexity with respect to input dimension. Standard images (e.g., 224 \u00d7 224 pixels) easily fit in modern GPU memory, enabling us to train full-context encoders that construct token embeddings via intermediary layers. However, gigapixel images require user-prespecified tokens (i.e., patches) and partial-context encoders.",
        "Full-context encoders now include foundation models (FMs) for a variety of data modalities and domains, including natural imagery, radiology images (;), pathology images, protein sequences, and molecular graphs like protein structures. On the other hand, partial-context encoders train on prespecified tokens like document sentences and image patches. In image domains like histopathology, remote sensing, and cosmology, numerous encoders have been proposed with varying training regimes: unsupervised encoders, weakly supervised classifiers, and FMs all build representations for patches.",
        "Regarding feature attribution for partial context models, gradient-based saliency and attention maps have been used to explain class predictions for high-dimensional unstructured data like gigapixel imagery. However, studies report low specificity and sensitivity in part because attribution for the entire datum is built by concatenating attributions across prespecified tokens (e.g., image patches). Partial-context strategies incorrectly assume prespecified tokens are independent and identically distributed (IID).",
        "Our work hinges on the assumption that FMs learn particularly rich embeddings and distributional semantics \u2014 and thus, sets of concepts \u2014 by virtue of their representational power. While feature attribution has not been explored by adapting FM embeddings, this work is inspired by the recent efforts to perform object detection and visual grounding via FM adaptation.",
        "Prospectors bring together ideas from many classical and modern works in adaptation, interpretability, memory augmentation, information retrieval, and language modeling. On the surface, prospectors resemble probing models, but the fact that they learn token associations between multiple token embeddings is more akin to constellation models, self-attention layers, or multiple instance learning approaches. Layer (I) is inspired by concept bottlenecks (Section 2), but extends the definition of concepts to carry spatial semantics. To learn higher-order associations between concepts, i.e., \u201ctoken mixing\u201d and inductive reasoning, layer (II) is inspired by both sliding window attention and the emergent -gram circuits seen in transformer induction heads. We foster the pattern-recognition capability via associative memory units built with an encoder\u2019s learned representations and graphical models. The result is that while prospectors are inspired by LLM reasoning, their implementation uses efficient statistical techniques, modeling primitives, and data structures.",
        "For transparency, we also outline our choice to rule out certain baselines for our experiments. A top priority for baseline selection was modality generalizability.",
        "we rule out LIME as a baseline for any of our tested data modalities. This is primarily due to the fact that LIME requires ground truth labels to explain each input. Since the inputs to our partial context encoders (and, in turn, LIME) are prespecified tokens (e.g., sentences for the WikiSection task), LIME requires token-level labels to explain the importance of sub-tokens (e.g., words). This requirement of token-level labels in our setting is fundamentally at odds with prospectors\u2019 goal to predict token labels, i.e., learn class-specific tokens de novo.",
        "we do not compare prospectors to modern methods like FastSHAP, which requires training additional models. FastSHAP specifically requires training two comparable models to the original encoder (i.e., with a classifier head) with respect to parameter count: a \u201csurrogate\u201d model that typically mimics the encoder in architecture but trained with a masked-input training regime and an \u201cexplainer\u201d model that learns to identify class-specific tokens. Such approaches are out of scope for this work, which aims to perform feature attribution with large models like FMs. Training surrogates for FMs is often practically infeasible.",
        "All unstructured data can be represented as map graphs of tokens interacting in physical space. We introduce mathematical definitions to describe these representations. Map graphs are also depicted in Figure S1.",
        "A map graph  is a collection of vertices  and edges  connecting neighboring vertices in Euclidean space. Each vertex  has features  and each edge  connects vertices  to .",
        "A map graph \u2019s connectivity  is its maximal node degree.",
        "Given a map graph , an encoder  is considered partial-context if it produces an embedding .",
        "Given a map graph , an encoder  is considered full-context if it produces embeddings , where .",
        "We choose to visualize any dictionaries created by prospectors (e.g., kernel  and during rollup Appendix C.3.1) in two main styles throughout this work. Firstly, visualization can take the form of (1) semantic networks, which easily allow us to visualize either frequencies (in sprites) or importance weights (in kernels) for monogram and skip-bigram associations. These plots are sometimes referred to as \u201cchord diagrams\u201d or \u201ccircos plots\u201d. This data structure is defined mathematically as a self-complete graph:",
        "A self-complete graph  is a fully connected graph with  vertices, where every pair of distinct vertices  is connected by a unique edge . It also contains all self-edges that connect any vertex  to itself with edge . Thus, self-complete graphs contain  vertices and  edges.",
        "This data structure is referenced in figures 4, 5, 8, and 9. Additionally, we can visualize all associations as (2) heatmaps, or unordered symmetric arrays, as seen in figures 5, 8, and 9.",
        "The rollup operator, named after the function of the same name in relational databases, draws similarity to a sliding bag of words featurization scheme. Internally, a dictionary  is constructed to capture all monograms and skip-bigrams in each neighborhood of . This operator is described by Algorithm 1 and depicted in Figure S2. For a full view of fitting layer (II), including both steps 1 (rollup) and 2, refer to Figure 5. We note that all sprite embeddings created in rollup were normalized using TF-IDF scaling prior to kernel fitting.",
        "Prospectors can have the following maximum number of importance weights, depending on  and : ",
        "As depicted in Figure 5, and discussed in the main body of this work, we implement two variants of prospector heads: a linear classifier variant and a fold-change variant. We provide additional details here for both variants. Details on hyperparameter selection , ,  are discussed in Appendix D.2.",
        "This variant was implemented with the sklearn python package. The elastic net classifiers  trained for a maximum of 3000 iterations using the saga solver.",
        "In order to supply an alternative to regularization for fold-change variants, we use two-way thresholding as inspired by differential expression analysis. These thresholds offer a form of \u201cmasking\u201d importance weights . As described in the main body of this work, the first threshold is , or the minimum fold-change required. The other threshold is , which is a threshold used for a statistical hypothesis test, which is tests for independent class means. This test is conducted for each weight entry  in  and significance is assessed via a Mann-Whitney U hypothesis test. Prior to weight masking, given the number of independent tests being conducted, we adjust our chosen significance threshold using the commonly used Bonferroni correction: our original  threshold is divided by the number of entries in , i.e., . Finally, to perform masking: we use  to mask out sufficiently small absolute fold changes (e.g., \u00b11, which indicates a requirement for doubling in log2-scale), and use  to mask out non-significant differences assessed by our hypothesis test.",
        "Here we conduct a comparative runtime analysis, where we analyze the worst-case time complexity required to explain a single input datum. We focus our analysis on the image modality due to compatibility with many baseline attribution methods. Suppose we have a trained encoder (e.g., an end-to-end classifier, unsupervised learner, etc.) and our datum has  (tunable) tokens to analyze. Importantly, full-context encoders process all  tokens at once while partial-context encoders process T tokens in sequence. This distinction affects runtime complexity, so we analyze complexity for both partial- and full-context settings.",
        "To analyze prospectors, we consider two main variables in computation: the number of tokens  and the number of operations for a forward pass  of the underlying encoder. Given these variables, prospectors themselves require only  computations per layer at inference time:  to quantize each token and  to traverse over all tokens during convolution. The latter operation ignores a near-constant term for the worst-case number of interactions, i.e., skip-bigrams between central token and tokens in the -neighborhood. The worst-case number of interactions is modality- and user-specific and is dependent on \u2019s topology, , and connectivity  (i.e., max node degree). Parameters  and  are both typically set as small constants, so we can consider them negligible for time complexity.",
        "Because prospectors are equipped to backbone encoders, inference in totality must account for the encoder\u2019s computational costs as well. Namely, an encoder requires  for inference with full context and  for partial context (since each prespecified token requires a forward pass). Thus, total computational complexity of an encoder-prospector pipeline operating on a single input datum is  for partial-context encoders and  for full-context encoders.",
        "To properly characterize baseline methods, we also consider variables for backward pass operations , sub-tokens (), and number of passes  if applicable. Sub-tokens are any (tunable) constituents within a token (e.g., pixels in a patch) and are required by some baselines in partial-context settings. For example, given a text document (datum) and its sentence-level tokens, SHAP may analyze the contribution of word sub-tokens. For comparative summary between prospectors and multiple baselines, please refer to Table S1. We discuss ruled-out baselines in Appendix B.",
        "Because prospectors only rely on forward passes from an equipped encoder, our approach to feature attribution is approximately 3\u20134\u00d7 more computationally efficient than gradient-based attribution methods. This analysis is based on empirical results from computing floating point operations (FLOPs) for model inference (i.e., passes with frozen weights). Since prospectors\u2019 FLOPs are significantly less than a forward pass at inference time, this efficiency boost is approximated by the forward-backward pass FLOP ratio of 1:2 to 1:3. This efficiency is especially relevant for multi-billion parameter foundation models that could be used as upstream encoders.",
        "A form of \u201cglocal\u201d attribution: prospectors simultaneously build a global, dataset-level kernel of scored concept associations while also building local, datum-specific prospect maps",
        "Interpretable: prospector kernels can be inspected and verified by users to interpret class-specific concepts",
        "Arguably generative in nature via kernel construction: kernels can be rescaled and interpreted as joint probabilities, i.e.,  for monograms and  for skip-bigrams",
        "Shift- and rotation-equivariant, thus order-free: controlling for any randomness, fitting and inference can start at any origin vertex to yield consistent attributions",
        "Scale-invariant: input data can be of any number of tokens",
        "Can be deterministic: given the above, fold-change variants (which have no trainable parameters) can create deterministic prospect maps if hypothesis testing is forgone",
        "Theoretically can learn implicit skip--grams over a datum, as discussed in Appendix C.7",
        "Prospectors have additional desirable properties: ",
        "Prospector heads are inspired by the induction heads, also referred to as -gram heads, found in trained transformers for language modeling \u2014 even inspiring our method\u2019s name. While induction heads perform a sort of \u201cpattern completion\u201d using tokens, our approach achieves a form of \u201cpattern recognition\u201d and simplifies this computation and parameter space in multiple ways: a quantization of token to a set of  concepts and the explicit learning of monograms and skip-bigrams ( and ).",
        "We claim that this strategy to learn monograms and skip-bigrams is sufficient for implicitly learning higher order -gram targets. Namely, we argue that skip-bigrams can be \u201cchained\u201d together to form implicit skip--grams during attribution, i.e., during the creation of prospect maps in layer (II). For example, iteration  of convolution may find a skip-bigram of concepts A\u2013B within the -sized receptive field (i.e. A and B may be up to  hops away) and then iteration  may find a skip-bigram of concepts B\u2013C. Together, one can argue that both skip-bigrams form an implicit skip-trigram A\u2013B\u2013C. This implicit chaining of skip--grams can also lead to implicitly capturing longer-range dependencies. In Theorem 1 below, we show that skip--grams can be implicitly chained up to  hops away in a map graph of tokens, .",
        "Given a map graph  of cardinality , prospectors with receptive field  and an ideal kernel can find all target 1-grams, skip-2-grams, ..., skip--grams spanning up to  node hops.",
        "First, we explore the  case (i.e., monograms). Here, all target 1-grams are found trivially via kernel look-ups. Next, we take a look at the  case (i.e., skip-bigrams). Given the receptive field , skip-bigrams can be found up to  hops away from the central node. Both the  and  cases can be generalized to single k2conv iterations over a large graph  (large ) or for small  where  ( fully captured within  hops). Given prospectors natively find monograms and skip-bigrams, multiple convolutional iterations are needed to find . We explore these cases next.",
        "For the  case, i.e., skip-trigrams, two skip-bigrams must be found in sequence with a shared token between them. We call this process \u201cbigram chaining.\u201d Given a skip-bigram can be learned over  hops, prospectors can thus learn a skip-trigram over  node hops. The desired property trivially generalizes over any choice of  (and ) via induction. \u25a0",
        "Through the kernel\u2019s \u201cmemorization\u201d of salient monograms and skip-bigram \u201crules,\u201d prospectors offer flexibility without exorbitant parameterization (as with attention) \u2014 i.e., the kernel does not need to see and learn a particular skip--gram in training, but at inference-time it can implicitly construct and recognize higher order skip--grams from its learned bigrams.",
        "One potential failure mode for prospection is triggered by small receptive fields , which can prevent prospectors from learning target skip-bigrams or skip--grams for any . In the previous section, we show how prospectors can potentially \u201cchain\u201d skip-bigrams to implicitly learn higher-order skip--grams (as seen with transformer induction heads). However this expressivity is hinged on a sufficient choice of  \u2014 prospectors must ensure the -size field captures the target bigrams at the minimum. We hope to study this potential failure mode with synthetic benchmarks in future works.",
        "Finally, another main motivation in prospector design is recent work on impossibility theorems, showing that (a) complete and (b) linear attribution methods can provably fail to improve on random guessing for inferring model behavior. Our approach sought to develop attribution methods outside of these traditional axioms (a) and (b). Prospector heads are not complete by nature of not constraining all token attribution scores in a datum to sum to a class prediction. The linear model variant uses its coefficients to attribute tokens, while the fold-change variant does not even output a class prediction.",
        "We run a speed benchmarking analysis between two main encoder-attribution pipelines: (1) MiniLM with a prospector head and (2) DeBERTa with a zero-shot classification head and PartitionSHAP. Given the Huggingface implementation for DeBERTa\u2019s zero-shot classification, PartitionSHAP was automatically selected by the shap Python package (over other SHAP methods like DeepSHAP). We present the speed benchmarking in Table S2.",
        "For each task, we conduct a grid-search of hyperparameter configurations to select an optimal prospector model. The prospector kernel has two main hyperparameters\u2014the number of concepts  and the skip-gram neighborhood radius . We also evaluate two prospector variants based on how the kernel is trained: hypothesis testing (with additional hyperparameters for the p-value  and fold change  cutoffs) and linear modeling (with elastic net mixing hyperparameter ). We describe all tested hyperparameters in our training grid search in Table S3.",
        "To select a top prospector configuration after the training grid-search, we first compute four token-level evaluation metrics for training set localization and apply sequential ranking over those chosen metrics. Applied in order, our chosen metrics were: precision, Matthews correlation coefficient (MCC), Dice coefficient, and AUPRC. These metrics were chosen because they enable segmentation-style evaluation, and we preferentially select on precision because it is especially important for detecting the small-scale class-specific regions in our data. For metrics that require a threshold (precision, MCC, and Dice coefficient), we select models based on the highest value attained over 11 thresholds: 0.0, 0.1, . . . , 1.0. Top prospectors per encoder, selected from the grid search and sequential ranking, are listed in Table S4.",
        "After prospect graphs are created by prospector heads, we map back the values of each token to its original coordinates (referred to in the main body as \u201cprospect maps\u201d). Prior to evaluation, we feature scale values to [0, 1]. Experimenting with other feature scaling schemes, e.g., dataset-level scaling based on minimum and maximum values, is left for future work. For reporting results on the held-out test set we focus mainly on AUPRC to provide a threshold-agnostic evaluation of each method. To compute AP, average the precision scores over a set of predetermined thresholds to binarize the prospect maps, as described in the previous section: 0.0, 0.1, . . . , 1.0.",
        "Wikisection\u2019s \u201cdisease\u201d annotated subset contains  documents total with 2513 training examples and 718 test examples. We preprocess the data into classes by searching each document for the presence of \u201cdisease.genetics\u201d section labels. If this section label is found, we assign a document-level label of class1 and class0 otherwise. Because our task is at the sentence-level, we then create tokens by breaking sections into sentences by the full-stop delimiter (\u201c.\u201d). We then label sentences by their source section labels. Raw-text sentences are then fed into our chose encoder, which handles natural language tokenization.",
        "This benchmark contains 400 gigapixel whole slide images (270 train, 130 test) of breast cancer metastases in sentinel lymph nodes. All images were partitioned into prespecified patch tokens (size 224 \u00d7 224) and filtered for foreground tissue regions (as opposed to the glass background of the slide). This process resulted in more than 200K unique patches without augmentation. For ground truth annotations, binary masks were resized with inter-area interpolation and re-binarized (value of 1 is assigned if interpolated value > 0) to match the dimensionality of data sprites.",
        "We also visualize the token embedding spaces of our encoders for the image task in Figure S3. The lack of natural clustering of class1-specific tokens (thick \u00d7 markers) from class0 tokens (\u25e6 markers) intuitively depicts the difficulty of our task. In other words, class-specific regions are made up of tokens that are conceptually similar to non-region tokens.",
        "We constructed a binary classification dataset of zinc-binding and non-binding proteins from the MetalPDB dataset. We specifically focus on proteins annotated as enzymes, since metal ions are often critical for enzymatic activity. Such enzymes are known as metallo-enzymes, and our global classification labels reflect whether a metallo-enzyme relies on zinc or a different metal ion. For the positive set, we consider only biologically-relevant zinc ions which occur within a chain (i.e., are bound to residues in the main chain of the protein, rather than ligand-binding or crystallization artifacts). We sample only one protein chain from each enzymatic class, as determined by Enzyme Commission numbers, selecting the structure with the best crystallographic resolution. This process resulted in 756 zinc-binding sites from 610 proteins, with 653 corresponding non-zinc-binding proteins sampled from unique enzymatic classes using the same procedure. For each zinc ion in the positive set, we extract all interacting residues annotated in MetalPDB to serve as our ground truth nodes for feature attribution. This dataset was split by enzyme class to ensure that no enzyme exists in both train and test sets, reserving 20% of chains for held-out evaluation. After removing four structures which produced embedding errors, this produced a training set of 1007 unique protein chains for the train set and 252 for the test set. Each protein is featurized as a graph where each node represents a residue and edges are defined between residues which share any atom within a distance of \u03f5 angstroms, where \u03f5 varies the density of the graph.",
        "Prospector trained on  vs. ",
        "Prospector trained on  vs. ",
        "Prospector trained on  vs. ",
        "Prospectors can be easily adapted to the multi-class setting by training multiple models for each class of interest. For example, if faced with three classes , , , prospectors could be applied in the following settings (class-1 and class-0, respectively): ",
        "In fact, both our protein (MetalPDB) and text (WikiSection) datasets are adapted from multi-class settings: MetalPDB contains data for many different metals, and WikiSection contains 27 different labels in the English disease document subset. In each case, we selected one class to evaluate for simplicity (zinc-binding proteins and genetics-related text, respectively), but one could easily construct an analogous dataset and train a model for any other class label.",
        "We outline specific models and how to access them.",
        "We specifically use the all-MiniLM-L6-v2 sentence Transformer model (via the sentence-transformers package), which is approximately 22M parameters in size.",
        "We specifically use the DeBERTa-v3-base-mnli-fever-anli model (via the transformers package), which is approximately 98M parameters in size. DeBERTa is able to perform off-the-shelf ZSC and NLI.",
        "Note: We forgo prospection with DeBERTa since it emits word embeddings without a simple and effective way to construct sentence embeddings. We reiterate that annotations are at the sentence-level (i.e., our prespecified tokens).",
        "We specifically use the clip-vit-base-patch32 model via the transformers package.",
        "We specifically use the plip model via the transformers package.",
        "We use the implementation and weights available at https://github.com/awfderry/COLLAPSE.",
        "We use the ESM implementation and weights available at https://github.com/facebookresearch/esm, specifically the 33-layer, 650M parameter ESM2 model (esm2_t33_650M_UR50D).",
        "We train two backbone encoders to equip with prospectors for the image task (Camelyon16):",
        "This encoder uses a ResNet-18 architecture trained for 20 epochs on a single NVIDIA T4 GPU. For training, the training set of 200K patches were formed into nearly 100K triplets with a sampling scheme similar to that of. These triplets were then used to train tile2vec with the triplet loss function.",
        "We trained a custom ViT for trained for 30 epochs on a single NVIDIA T4 GPU. It was trained to perform IID patch predictions under coarse supervision, which involved image-level label inheritance \u2014 the process of propagating image-level class labels to all constituent patches.",
        "Using our sampled training embeddings from clustering, we train a one-class SVM on class0 token embeddings () to perform novelty detection on the held-out test set. The SVM was implemented with the sklearn package and trained with an RBF kernel and hyperparameter  (where  is embedding dimension). Training ran until a stopping criterion was satisfied with 1e-3 tolerance.",
        "Using our sampled training embeddings from clustering, we train an MLP on all  token embeddings (labeled as class0 or class1) to perform fully supervised token classification held-out test set \u2014 acting as a stand-in for a segmentation-like baseline. The MLP was implemented with the sklearn package and trained with one hidden layer (dimension 100), ReLU activations, adam optimizer, L2-regularization term of 1e-4, initial learning rate 1e-3, and minibatch size of 200. Training ran for a maximum of 1000 iterations, where inputs are shuffled.",
        "The DeBERTa model can perform ZSC out of the box, giving us sentence-level ZSC probabilities (i.e., logits). We used the ZSC binary labels of [\u201cgenetics\u201d, \u201cother\u201d]. While we considered all possible labels in the WikiSection dataset\u2019s disease subset (see below), we ultimately went with binary classification due to higher performance.",
        "Unused multi-class labels: [\u201cgenetics\u201d, \u201cother\u201d, \u201cclassification\u201d, \u201ctreatment\u201d, \u201csymptom\u201d, \u201cscreening\u201d, \u201cprognosis\u201d, \u201ctomography\u201d, \u201cmechanism\u201d, \u201cpathophysiology\u201d, \u201cepidemiology\u201d, \u201cgeography\u201d, \u201cmedication\u201d, \u201cfauna\u201d, \u201csurgery\u201d, \u201cprevention\u201d, \u201cinfection\u201d, \u201cculture\u201d, \u201cresearch\u201d, \u201chistory\u201d, \u201crisk\u201d, \u201ccause\u201d, \u201ccomplication\u201d, \u201cpathology\u201d, \u201cmanagement\u201d, \u201cdiagnosis\u201d, \u201cetymology\u201d]",
        "We implemented a Shapley scoring pipeline for DeBERTa since it can perform ZSC end-to-end. The pipeline was implemented via the transformers package using the object class zeroShotClassificationPipeline. Shapley computation was performed via the shap package. The pipeline defaults to PartitionSHAP \u2014 in this setting, PartitionSHAP is applied to the partial-context DeBERTa model and considers sub-tokens (words) over all possible  tokens (sentences), ultimately pooling over sub-tokens to get a token-level score. We run a speed benchmarking analysis for this approach in Appendix D.1.",
        "Note: full-context shapley score computation was also considered. However, due to poor computational scaling (for both DeBERTa and PartitionSHAP), we ruled out this strategy.",
        "The DeBERTa model can perform NLI entailment off the shelf, yielding sentence-level NLI entailment scores. We provide the model with an NLI hypothesis (\u201cthis sentence is about genetics\u201d) and NLI premise (i.e., the input sentence). Labels extracted refer to [\u201centailment\u201d, \u201cneutral\u201d, \u201ccontradiction\u201d].",
        "The DeBERTa model can also output attention scores. Attention scores are computed by max-pooling over the attention weights for the NLI hypothesis (\u201cthis sentence is about genetics\u201d) given the NLI premise (i.e., the input sentence).",
        "For this task, baselines were chosen due to their popularity and efficiency.",
        "attention maps are created per input token and their values are averaged. This creates a single attention score per token, after which tokens are concatenated by their spatial coordinates. These values are scaled to values in [0,1].",
        "For ViT, each token\u2019s prediction probability for class1 is used to score each token, after which tokens are concatenated by their spatial coordinates. For both vision-language models, CLIP and PLIP, we prompt both FMs\u2019 text encoders with zero-shot classification labels for class0 and class1, respectively: [\u201cnormal lymph node\u201d, \u201clymph node metastasis\u201d]. These labels match the benchmark dataset\u2019s descriptions of class labels. Similarly to ViT, each token\u2019s class1 prediction probability is used to score each token, after which tokens are concatenated by their spatial coordinates",
        "Our baseline for zinc binding residue identification is a graph attention network (GAT) containing two GAT layers, each followed by batch normalization, followed by a global mean pooling and a fully-connected output layer. The input node features for each residue were given by the choice of encoder (COLLAPSE, ESM2, or AA). The GAT model was trained using weak supervision (i.e., on graph-level labels y) using a binary cross-entropy loss and Adam optimizer with default parameters and weight decay of 1 \u00d7 10\u22124. To select the best baseline model, we use a gridsearch over the edge cutoff  for the underlying protein graph (6.0 or 8.0 \u00c5), the learning rate (1 \u00d7 10\u22125, 1 \u00d7 10\u22124, 5 \u00d7 10\u22124, 1 \u00d7 10\u22123), and the GAT node feature dimension (100, 200, 500). Feature attribution for all explanation methods was performed using implementations provided by Pytorch Geometric. The best model was selected using the selection criteria in Appendix D.2. The final classification models for COLLAPSE, ESM, and AA encoders used edge cutoffs (\u03f5) of 8.0 \u00c5, 6.0 \u00c5, and 6.0 \u00c5, learning rates of 5 \u00d7 10\u22124, 5 \u00d7 10\u22123, and 5 \u00d7 10\u22123, and feature dimensions of 100, 500, and 200, respectively.",
        "We use GNNExplainer to produce explanations for nodes (i.e., residues) only. We train the GNNExplainer module for 100 epochs with a default learning rate of 0.01.",
        "The attention baseline uses the attention scores of the trained GAT model to produce attribution scores. Attention scores across layers and heads are first max-pooled to produce aggregated attention scores for each edge. Then, we compute the attribution score for each node by averaging over the scores of all edges connected to it.",
        "We deploy SHAP using a Shapley value sampling approach adapted specifically for graph data and implemented using Captum (https://captum.ai). SHAP is computationally feasible for this task primarily due to the use of full-context classifier heads to plug into each tested encoder. This allows SHAP to explain individual tokens (amino acids) by aggregating over edge weights using the same procedure as described for our attention baseline.",
        "We report quantitative results corresponding to Figure 6 in tables S5, S6, and S7. All reported errors reflect the standard error of the mean.",
        "We also briefly study the robustness of prospector (and top baseline) test-set performance with respect to salient region characteristics: region prevalence and mean region dispersion. We display these results in figures S4, S5, and S6. The more that lines gravitate to the top of each plot, the more robust an encoder-attribution pipeline is to target region characteristics. Lines are created by convolving over the test-set examples.",
        "The metal-binding protein task is particularly challenging as the majority of its class-specific regions are below 0.1 prevalence, but prospectors were nonetheless able to achieve high performance on most test-set examples (Figure S6). Interestingly, ESM2 showed bimodal performance, with high AUPRC on one subset and a correlated, low performance on another. This suggests that a subset of data does not contain clear sequence patterns that are correlated with zinc binding, while structure-based encoders can capture local interactions between residues far apart in sequence. In addition to the prevalence of class-specific regions, mean region dispersion provides a view into their spatial organization.",
        "Figure S7 displays the results of hierarchically clustering sprite embeddings for the zinc binding task."
    ],
    "title": "Prospector Heads: Generalized Feature Attribution for Large Models & Data"
}