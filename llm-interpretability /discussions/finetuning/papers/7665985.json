{
    "content": [
        "Currently, a major limitation for natural language processing (NLP) analyses in clinical applications is that concepts are not effectively referenced in various forms across different texts. This paper introduces Multi-Ontology Refined Embeddings (MORE), a novel hybrid framework that incorporates domain knowledge from multiple ontologies into a distributional semantic model, learned from a corpus of clinical text.",
        "We use the RadCore and MIMIC-III free-text datasets for the corpus-based component of MORE. For the ontology-based part, we use the Medical Subject Headings (MeSH) ontology and three state-of-the-art ontology-based similarity measures. In our approach, we propose a new learning objective, modified from the sigmoid cross-entropy objective function.",
        "We used two established datasets of semantic similarities among biomedical concept pairs to evaluate the quality of the generated word embeddings. On the first dataset with 29 concept pairs, with similarity scores established by physicians and medical coders, MORE\u2019s similarity scores have the highest combined correlation (0.633), which is 5.0% higher than that of the baseline model, and 12.4% higher than that of the best ontology-based similarity measure. On the second dataset with 449 concept pairs, MORE\u2019s similarity scores have a correlation of 0.481, based on the average of four medical residents\u2019 similarity ratings, and that outperforms the skip-gram model by 8.1%, and the best ontology measure by 6.9%. Furthermore, MORE outperforms three pre-trained transformer-based word embedding models (i.e., BERT, ClinicalBERT, and BioBERT) on both datasets.",
        "MORE incorporates knowledge from several biomedical ontologies into an existing corpus-based distributional semantics model, improving both the accuracy of the learned word embeddings and the extensibility of the model to a broader range of biomedical concepts. MORE allows for more accurate clustering of concepts across a wide range of applications, such as analyzing patient health records to identify subjects with similar pathologies, or integrating heterogeneous clinical data to improve interoperability between hospitals.",
        "With the increasing availability of health-related textual data, such as Electronic Health Records (EHR), novel applications of Natural Language Processing (NLP) in the field of medical informatics are a growing topic of interest. Currently, a major limitation of NLP analysis techniques for clinical text is related to the free-text format of these records and notes\u2014the same concept can be referenced in various forms across different texts (e.g., \u201ckidney failure\u201d and \u201crenal failure\u201d). In particular, different physicians and institutions may use unique terminologies for reporting the same concepts in EHRs. To address this issue, researchers use semantic similarity measures to identify similar biomedical concepts in free-text records and notes. A semantic similarity measure takes two concepts as input and returns a numeric score that quantifies how alike they are in meaning. A hybrid biomedical semantic similarity measure can improve the identification and clustering of biomedical concepts across a wide range of applications, improving patient care and clinical outcomes. For example, patient health records can be analyzed to identify subjects with similar conditions or pathologies. With this information, data-mining techniques can be used to extract useful information about previous care processes, the evolution of certain diseases, and social trends.",
        "Semantic similarity measures can also assist in identifying patients for clinical studies and clustering symptoms in clinical text for post-marketing medication safety surveillance. Furthermore, they can be used to integrate heterogeneous clinical data, which can improve interoperability between medical entities and allow hospitals to share patient health information more effectively to improve clinical outcomes. Finally, in the fields of medical information retrieval and literature mining, users\u2019 queries can be extended to conceptually equivalent formulations to improve the effectiveness of keyword-based search engines. Ultimately, semantic similarity measures can improve the statistical power of NLP analyses, making it easier to identify associations between medical conditions and clinical outcomes in health records, and improve information retrieval from scientific journals and clinical reports to track relevant advancements in this field.",
        "A variety of semantic similarity measures have been developed to describe the strength of the relationships between concepts in biomedicine. These existing semantic similarity measures mostly fall into two common categories: ontology-based or corpus-based semantic similarities. Ontology-based semantic similarities typically rely on different graph-based features, such as the shortest path length between concepts and the position of their lowest common ancestors, to capture semantic similarity. These ontology-based approaches depend on the completeness and quality of the underlying ontologies; however, curating and maintaining domain ontologies is a labor-intensive and complicated task.",
        "As an alternative to ontology-based semantic similarity, corpus-based semantic similarities are based on distributional semantics and co-occurrences of terms in free text. These corpus-based models rely on the linguistic principle that the meaning of a word (i.e., semantics) can be inferred based on its surrounding words (i.e., context). With recent advances in deep learning, and the widespread use of distributional semantics to construct word embeddings for word representation in deep-neural networks, these corpus-based models have gained vast popularity. The word2vec distributional semantics model is one of the most common methods for generating such word embeddings. Intuitively, the word2vec model is a neural network that maps words with similar context to nearby points in a vector space. The cosine similarity between the resulting word representations is commonly considered to be a corpus-based semantic similarity in various settings. Although corpus-based semantic similarities are generated by unsupervised models, making them more extensible to a broader range of concepts, human curation could further improve their accuracy in biomedical applications.",
        "Previous efforts to combine ontology-based and corpus-based similarities to better capture semantic similarities between biomedical concepts are hybrid approaches that mostly rely on the frequency of the appearance of those concepts in a corpus to compute information content, rather than considering the free-text context for measuring similarities. Additionally, existing hybrid measures for semantic similarity in biomedical domains do not incorporate ontological knowledge into the process of generating word embeddings. In non-biomedical domains, a few approaches have been developed to use constraints among words, such as word categories, links, or typed relations, as regularization terms in model training to construct better word embeddings. In this paper, we propose Multi-Ontology Refined Embeddings (MORE) as a semantic similarity model that effectively integrates ontological knowledge and corpus-based context into a novel semantic similarity measure. MORE uses existing ontology-based semantic similarity measures from the Unified Medical Language System (UMLS) to modify the objective function of the word2vec skip-gram model, a popular distributional semantic model. In our approach, we propose a mathematical framework for vector representation refinement that relies on a collection of the most established and reliable ontology-based measures, rather than a single ontology-based similarity, to maximize the utility of our measure in a broad domain. In other words, MORE uses multiple ontology-based semantic similarities as the overall indicator of ontological similarity to refine the representations in the distributional semantic model. Of note, our implementation is based on the official TensorFlow implementation of word2vec, and we have made it available for public use.1",
        "Our model is benchmarked against existing state-of-the-art semantic similarities using an established evaluation dataset to measure the semantic similarity between biomedical concepts. We find that MORE outperforms multiple corpus-based semantic similarity model, as well as the individual ontology-based semantic similarities, in terms of correlation with physician and medical coder similarity scores in the evaluation dataset. The main contributions of this paper are two-fold: 1) we present a generalizable and extensible framework for incorporating domain-specific knowledge into a distributional semantic model, and 2) we show that this hybrid framework outperforms the baseline word2vec model, multiple pre-trained transformer-based models, and ontology similarity measures on two established benchmarks. In the remainder of this paper, we provide context for corpus-based, ontology-based, and hybrid semantic similarity measures in the biomedical domain. We also discuss the following components: the corpora used to train the corpus-based component of the model (i.e., RadCore and MIMIC-III), the ontology-based similarity measures used to modify the objective function, the mathematical framework used for modifying the cross-entropy objective function, and the benchmark dataset against which the proposed method was evaluated. We also discuss the results from evaluating the proposed measure against state-of-the-art benchmarks, present a conclusion, and propose directions for future research.",
        "In the biomedical domain, there are a growing number of ontologies or hierarchical knowledge bases that represent semantic relationships between concepts. One of the best examples of this is UMLS, which is maintained by the National Library of Medicine (NLM) and includes two of the largest and most extensive ontology knowledge bases: Systematized Nomenclature of Medicine-Clinical Terms (SNOMED-CT) and Medical Subject Headings (MeSH). Ontology-based semantic similarity measures are based on \u201cis-a\u201d relations found in the underlying taxonomy or ontology in which the concepts reside. For example, the terms \u201ccommon cold\u201d and \u201cillness\u201d are similar because \u201ccommon cold is a kind of illness. Likewise, common cold and influenza are similar in that they are both kinds of illness\u201d. As these ontology-based approaches are sensitive to the completeness and quality of the underlying ontologies, curating and maintaining domain ontologies is critical to guarantee the accuracy and robustness of ontology-based semantic similarities. Although there have been major efforts, such as the ongoing support by NLM, to curate and maintain biomedical ontologies as valuable sources of domain knowledge, it is a labor-intensive and elaborate task. Furthermore, due to the heterogeneity of biomedical domains and their corresponding concepts, there is no single top-performing ontology-based similarity measure across all domains and applications.",
        "In 2013, Mikolov et al. introduced the word2vec distributional semantics model, a neural network that maps words with similar context to nearby points in a vector space. As previously noted, the semantic similarity between two words is calculated as the cosine similarity of the generated vectors representing those words, ranging from \u22121 to 1. The paper of Mikolov et al introduced two model architectures: Continuous Bag-of-Words (CBOW) and skip-gram. While these models are algorithmically similar, CBOW predicts target words from context words, whereas skip-gram predicts context words from the target words. Statistically, CBOW smooths over a lot of the distributional information because it treats the entire context as one observation, an approach that works better for larger datasets. However, skip-gram treats each context-target pair as a new observation, which tends to perform better with smaller corpora. Given the size of our corpora, we opt to use skip-gram as our baseline distributional semantic model in the present study.",
        "In the biomedical domain, Pakhomov\u2019s work indicates the word2vec representations trained on a clinical corpus of text are able to capture the relationship between biomedical terms. However, this study only utilized default hyperparameters, such as embedding dimension, for training the word2vec representations. The work of Chiu et al modifies the hyperparameters of word2vec and finds that the performance can be significantly improved in the biomedical domain by hyperparameter tuning. In both Pakhomov\u2019s and Chiu\u2019s studies, the word representations are still extracted from the vanilla word2vec models, as Pakhomov and Chiu only changed the training corpus or the hyperparameters. While corpus-based measures have proven to be more flexible and extensible than ontology-based measures, the lack of both human curation and access to large representative corpora might limit their accuracy.",
        "Recently, transformers with a multi-head self-attention mechanism, such as BERT, have been proven to be instrumental in various NLP tasks, including generating contextual word embeddings. Such transformer-based models are usually pre-trained through a self-supervised procedure, which is focused on learning a masked language model on a large corpus. The pre-trained model can be further fine-tuned on a supervised downstream task. Notably, transformer-based models, like BERT, capture word context in generating word embeddings. Variations of BERT, such as ClinicalBERT and BioBERT, have been trained on large biomedical corpora for clinical and biomedical applications. These contextual word embeddings have proven to be beneficial in various NLP tasks and achieved state-of-the-art performance.",
        "There have been previous efforts to combine ontology-based and corpus-based similarities to better capture semantic similarities; however, no framework currently exists in the biomedical domain for incorporating ontological knowledge into the process of generating word embeddings for semantic similarity. Yu and Dredze introduced a general model for learning word embeddings by incorporating prior information. This group proposed relation constrained loss, which is the average log probability of all relations for each term. By adding the relation constrained loss to the original word2vec loss function, this method could include a word\u2019s synonyms from WordNet in the word embeddings. The generated word embeddings, produced from the joint objective function and trained on a general corpus, outperformed the baseline word embeddings in three tasks: language modeling, measuring word similarity, and predicting human judgment on word pairs. In addition to word2vec, Alsuhaibani et al. extend the objective function of GloVe in a way similar to Yu and Dredze by adding relation constrained loss to incorporate prior knowledge. Compared to these studies, our method uses a different approach to modify the loss function, as we re-weight the objective function of the word2vec model according to ontology-based semantic similarities for each pair of terms. Particularly, in contrast to these methods, our approach does not introduce additional hyperparameters. As a result, our proposed method does not require careful determination of the value of additional hyperparameters, making the training process more straightforward. Xu et al. introduce RC-NET, a combination of two models, R-NET and C-NET, which use different objective functions to capture relational knowledge and categorical knowledge, respectively. They show that RC-NET, trained on a general corpus, outperforms R-NET, C-NET, and the baseline skip-gram model in the word similarity and topic prediction tasks. Faruqui et al. propose a method for augmenting vector space representations of words using relational information from semantic lexicons. The main contribution of their proposed method, retrofitting, is that it is applied as a post-processing step, which allows it to be used on any pre-trained word vectors. They show that using retrofitting as a post-processing step improves performance on a variety of tasks, including word similarity, syntactic relations, synonym-selection, and sentiment analysis. Finally, Pivovarov and Elhadad present a hybrid score that uses a weighted average of ontology measures and corpus measures to calculate semantic similarity. However, their method doesn\u2019t incorporate ontological knowledge into the process for generating the word embeddings; instead, it combines the outputted scores to produce a more accurate final semantic similarity score.",
        "Of note, while there are hybrid methods that combine elements of corpus-based and ontology-based methods, in this paper, we present a general framework for incorporating ontological knowledge into the process for generating word embeddings for semantic similarity in the biomedical domain. We created a new objective function by modifying the sigmoid cross-entropy objective function of the skip-gram model with ontological knowledge from the MeSH- ontology similarity measures to broaden the domain of our model beyond the scope of the corpus. As a result, our proposed model is able to learn word embeddings that encode both contextual information and domain knowledge, thus making it more accurate and extensible than previous methods.",
        "In this work, we use the RadCore and MIMIC-III corpora to train the corpus-based component of our proposed model. Assembled at Stanford in 2007, RadCore is a large multi-institutional radiology report corpus for NLP. The reports in the RadCore corpus range from 1995 to 2006 and were de-identified by their source organizations before submission to RadCore. In its entirety, RadCore contains 1,899,482 reports from three major healthcare organizations: Mayo Clinic (812 reports), MD Anderson Cancer Center (5,000 reports), and Medical College of Wisconsin (1,893,670 reports). Additionally, all of the radiology reports are in free text format and do not contain any metadata about the type and nature of the imaging exams. Medical Information Mart for Intensive Care (MIMIC-III) is a database containing information gathered from patients that were admitted to critical care units at a large hospital. In this study, we use MIMIC-III\u2019s gold standard corpus of 2,434 ICU nursing notes that were \u201cgathered simultaneously with the signals, trends, laboratory reports, discharge summaries and other data in the MIMIC-III databases\u201d. The corpus was thoroughly de-identified; all detected instances of Protected Health Information (PHI) were replaced by realistic surrogate data. The final training corpus, which is a combination of the RadCore and MIMIC-III corpora, contains 195,101,383 total words, 145,274 unique words, and 43,232 unique frequent words with at least five occurrences in the corpora.",
        "One of the challenges with having numerous medical domain ontologies is that they are typically developed independently of each other and rely on different standards, programming languages, and interfaces. Of note, the UMLS framework, developed by NLM, includes over 100 controlled medical ontologies. Among those, Medical Subject Headings (MeSH) is a controlled hierarchical vocabulary developed by NLM and is widely used for cataloging and searching biomedical and health-related information. In addition, MeSH vocabulary size is more manageable in comparison to larger controlled terminologies, such as SNOMED-CT. Of note, our proposed hybrid semantic similarity measure can be trained with various ontologies and controlled terminologies and is not dependent on a specific ontology. The choice of using MeSH in this study is due to engineering considerations and trade-offs that reduce the required computational time and resources. We use a simple approach to identify and extract MeSH concepts in text. To do that, we tokenized and normalized corpus text using a standard Natural Language Toolkit (NLTK) module. Then the normalized text was matched against MeSH concepts provided by the UMLS Perl interface using a regular express functionality. Overall, 6,035 MeSH concepts were extracted from our corpus in this study and were used in training the MORE embeddings. This number shows a considerable amount of MeSH concepts present in clinical notes.",
        "In this study, we use three state-of-the-art semantic similarity measures on concepts in the MeSH ontology: Wu & Palmer (wup), Leacock & Chodorow (lch), and Al-Mubaid & Nguyen (nam). These three semantic similarity measures are defined below.",
        "Before using the ontology-based similarities to modify the objective function of the skip-gram model, we first identified the set of words that appear at the intersection of the set of words in the corpus vocabulary and the set of words that exist in the MeSH hierarchical ontology. Using this intersection set, we generate a similarity matrix containing all pairwise similarities of the intersection terms, normalizing each measure to be in the range of 0 to 1. We utilized an established and widely used Similarity Perl package to compute these similarity measures. For each word pair, if more than one set of ontology-based similarity scores is produced, we calculate the median of the similarity scores for the final similarity score. And, if a similarity score doesn\u2019t exist as defined by any of the ontology-based similarity measures, we use a placeholder value of \u22121 to denote that we do not modify the objective function for that particular word pair in the training process. It is important to note that not every pair of words has an ontology-based similarity score because a path may not exist between them in an ontology. Thus, in our study, the final similarity matrix contains 4,878 unique words and 11,945,574 pair-wise similarity scores.",
        "Multi-Ontology Refined Embeddings (MORE) is a hybrid semantic similarity measure that effectively integrates ontological knowledge and corpus-based contexts in a novel semantic similarity measure. The ontology-based similarity measures are used to modify the objective function of the word2vec skip-gram model. MORE uses a mathematical framework for vector representation refinement that is extensible in that any number of established and reliable ontology-based measures can be incorporated into the existing framework collection, allowing the model to maximize our measure\u2019s utility in a broad domain (see Figure 1).",
        "Traditionally, the objective function of the skip-gram model is a full softmax function. However, the specific implementation of the skip-gram model used for this project relies on a simplified variant of Noise Contrastive Estimation (NCE) that trains faster and results in better vector representations for frequent words, compared to the full softmax function. The loss function, L\u03b8, is the average sigmoid cross-entropy loss, which incorporates both the loss computed from the context words, LPOS, and the loss calculated from the negatively sampled words, LNEG, over the batch size:    where S is the sigmoid function, wI is the input word, wC is a context word, wNEG is a negatively sampled word, and logit(wi/wI) is the log odds of the conditional probability of the label word (wC or wNEG) given the input word, as predicted by the model.",
        "In computing the loss for the context words and negatively sampled words, we modify the binary labels used in the traditional cross-entropy loss function to incorporate the ontology similarities. For the context words, rather than multiplying the negative log of the sigmoid of the model output by one, we multiply it by the average of 1 and the ontology similarity score.",
        "Similarly, for the negatively sampled words, rather than multiplying the negative log of the sigmoid of the model output by one, we multiply it by one minus the average of zero and the ontology similarity score.",
        "By averaging the binary labels (i.e., 1 and 0) with the similarity scores outputted by the model, the loss function is adjusted to incorporate relational knowledge from the ontologies. For instance, in the case of computing the loss for context words (LPOS\u2217), if the word pair has a high ontology similarity score, the loss will be higher. In order to minimize loss, the network will adjust the weights in the direction suggested by the ontological knowledge, encouraging the model to output higher probabilities for word pairs with high ontology similarity scores and lower probabilities for word pairs with low ontology similarity scores.",
        "In 2007, Pedersen et al. introduced a test set of word pairs for the evaluation of measures of semantic similarity and relatedness in the biomedical domain. They collected 120 concept pairs in the biomedical domain and asked physicians and medical coders to score their similarities. By only selecting the pairs whose inter-rater agreement was high, they curated a reliable test set with 30 concept pairs. Since the introduction of this dataset, it has become the \u201cde facto evaluation standard\u201d and benchmark in the biomedical domain and has been used in multiple studies in this domain for evaluating various semantic similarity measures.",
        "These 30 concept pairs of medical terms (see Table 1) were scored by multiple physicians and medical coders on a 4-point scale, according to their relatedness: \u201cpractically synonymous (4.0), related (3.0), marginally related (2.0), and unrelated (1.0)\u201d. The average correlation between physicians was 0.68, the average correlation between medical coders was 0.78, and the correlation across groups was 0.85. In this study, term pair 5, \u201cDelusion \u2014 Schizophrenia\u201d, has been excluded from the final evaluation dataset because one of the terms did not appear a minimum of five times in our combined corpora. Accordingly, the resulting test set consists of 29 of the 30 original pairs. To evaluate the different methods, we calculated the correlation between the similarity scores outputted by the methods and the physician/medical coder similarity scores. Since this dataset is relatively small, we also evaluated our model in a larger dataset, which includes 449 concept pairs. The similarity score of each pair of concepts in this dataset was calculated by taking the average of four medical residents\u2019 similarity ratings. According to the dataset curation team, 117 of the original 566 concept pairs were discarded from the final dataset because the concepts could not be found in PubMed Central (PMC), a corpus of clinical notes from the Fairview Health System between 2010 and 2014, Wikipedia corpus.",
        "In our dataset, there are many biomedical multi-word terms, such as \u2018Congestive heart failure.\u2019 There are two conventional approaches to determine the representations for multi-word terms. The first is constructing a new concept vector directly, and the other is based on the summation/average of component word vectors. Previous research indicates that there is no statistically significant difference between the two approaches. Considering the comparable performance of these two methods, constructing new concept vectors will increase the vocabulary size, and, therefore, add to the computation time and complexity. Thus, in this study, we adopted the second approach and calculated the average of the representation vectors of all words in a multi-word term. We use the average vector as the representation for that multi-word term, and that does not expand the vocabulary size. For each measure of similarity, we calculated the correlation coefficients of the results with human expert ratings and tested the statistical significance of the correlations using a t-test. The null hypothesis in this statistical test is that the correlation coefficient is zero. Of note, for the first dataset, in addition to comparing our model to the method used separately by physicians and medical coders, we mixed the scores of physicians and medical coders and used the merged 58 concept pairs to calculate the combined correlation coefficient.",
        "We also applied three widely used transformer-based embedding models to our test sets and compared them to the MORE model in evaluation. These three pre-trained transformer-based models include BERT (i.e., BERT-base-uncased), ClinicalBERT, and BioBERT. BERT has been used for generating contextual word embeddings in a wide variety of applications, while ClinicalBERT and BioBERT are focused on biomedical applications. In this study, we generated these transformer-based embeddings for each concept using the corresponding pre-trained models and calculated their correlation coefficients with the humans\u2019 scores for evaluation. Of note, we applied the standard word piece-based tokenizer that BERT and other pre-trained contextual word embedding models use to tokenize the input medical concepts. These tokenized medical concepts, which could contain multiple word piece tokens, were then fed as inputs to a BERT-based model to obtain contextual word embeddings, which were represented by the CLS vectors.",
        "In this section, we compare the proposed model against three established ontology similarity measures, the baseline skip-gram model, and three pre-trained transformer-based word embeddings. The correlation values for the ontology-based measures are extracted from McInnes et al.. The goal of these experiments is to demonstrate the value of using the MORE framework to learn semantic embeddings with information from ontology similarity measures. In each experiment, we compare the baseline embeddings trained with skip-gram against the embeddings trained using the MORE framework. We quantify the evaluation task of measuring semantic similarity utilizing the correlation between the similarity scores generated by the embeddings and the similarity scores produced by expert human raters.",
        "In training the baseline skip-gram model and the proposed model, we used the following default parameters of the TensorFlow implementation of the skip-gram model: embedding size of 300, window size of 10, minimum word count of 5, and a subsampling threshold of 0.001. To expedite the training process, we used a learning rate of 0.3 and a batch size of 1,024. We trained each model for 10 epochs at a time, warm starting each model with the previous model as a checkpoint, for a total of 150 epochs.",
        "Table 2 shows a comparison of the results achieved by all of the models and ontology measures for the 29 concept pairs. Table 3 indicates the comparison of different measures for the large dataset.",
        "We found that, under identical training conditions, MORE consistently outperforms the baseline skip-gram model in terms of correlation with expert-generated similarity scores. For our first dataset with 29 concept pairs, Table 2 illustrates that MORE had a 5.8% higher correlation with the physician similarity scores and a 3.8% higher correlation with the medical coder similarity scores than the baseline skip-gram model. Additionally, MORE had a 27.2% higher correlation with the physician similarity scores than the best ontology similarity measure. Of note, MORE outperformed all except one of the ontology-based measures (nam) in terms of correlation with medical coder similarity scores. The higher correlation between ontology-based measures and medical coders is likely because medical coders were trained to use hierarchical classifications and ontologies to assign similarity scores. Therefore, their performance is more aligned with the structured knowledge in ontologies. The combined correlations in the fourth column of Table 2 show that MORE has the highest combined correlation, which is 5.0% higher than the correlation of the baseline skip-gram model and 12.4% higher than the best ontology measure. Also, MORE outperformed all three pre-trained transformer-based word embeddings with more than 6-fold improvement in correlation with expert-generated similarity scores on this dataset.",
        "For the large evaluation dataset, which includes 449 concept pairs, the MORE model still had the best performance. As shown in Table 3, our MORE model outperformed the baseline skip-gram model by 8.1%, the best ontology-based measure by 6.9%, and the best pre-trained transformer-based model by 6.7-fold on this dataset. Of note, as shown in Tables 2 and 3, the MORE model achieved the smallest p-value (i.e., the most significant p-value) on both datasets in comparison to the baseline skip-gram model, pre-trained transformer-based models, and all ontology-based semantic similarity measures. Notably, MORE\u2019s custom loss function did not have a substantial impact on training time. This is because the concept-pairs\u2019 similarity scores from the MeSH ontology were calculated and stored in a table in memory (i.e., hashed) prior to training in our proposed pipeline.",
        "As mentioned in the Introduction section, due to the heterogeneity of biomedical concepts, there is no single top-performing corpus-based or ontology-based semantic similarity measure across all applications and domains. However, by modifying the objective function of the skip-gram model with knowledge from the MeSH ontology and multiple ontology-based similarity measures, we can generate embeddings from the RadCore and MIMIC-III corpora that incorporate knowledge beyond the scope of the corpora and maximize the measure\u2019s utility in a broad domain. MORE outperforms the baseline skip-gram model and pre-trained transformer-based models in every case, as well as the ontology similarity measures in most cases. As a result, we have demonstrated that the embeddings generated using the MORE framework are more effective at capturing semantic similarity for biomedical concepts, in a broader domain, than any of MORE\u2019s individual components. Our results show that transformer-based embeddings from three pre-trained transformer-based models, BERT, ClinicalBERT, and BioBERT, are not effective in the setup presented in our study. Although pre-training transformer-based models on biomedical text in BioBERT and ClinicalBERT marginally improved the results in our study in comparison to BERT, we expect fine-tuning a transformer-based model on a downstream task could further enhance its utility for representing biomedical concepts. However, this fine-tuning was not possible in our framework because we did not have access to training labels for our corpora in this study. Of note, transformer-based embedding for a concept depends on the word contexts. In this study, our transformer-based embeddings were generated for concepts that usually consist of only a few words, without an extensive context, which might also have contributed to the low performance of these models in our task.",
        "Notably, MORE incorporates word relationships in word embeddings that are carefully captured in ontologies and structured knowledgebases available in the biomedical domain. These relationships and semantics may not be captured by BERT and other corpus-based word embedding methods that rely on word co-occurrences and proximities. Additionally, training a transformer-based model is computationally expensive and requires a large training corpus. On the other hand, MORE has fewer parameters, requires a smaller training set and computational resources, and is easier to train in comparison to transformer-based models, such as BERT.",
        "Despite MORE\u2019s promising performance in our evaluation, we recognize that this study has several limitations. First, aside from the increased learning rate and batch size used to expedite training, we used the default training parameters, as suggested by the TensorFlow implementation of the skip-gram model, to train both the baseline skip-gram model and the proposed model. While these parameters have been optimized for training the baseline model, we did not experiment with tuning hyperparameters to optimize the training of the proposed model. However, this suggests that, under equal but potentially sub-optimal training conditions, MORE outperforms the baseline skip-gram model. Second, we have only incorporated three ontology similarity measures (lch, wup, and nam) from one ontology (MeSH) into our novel framework. Also, in our pipeline, ontology concepts in text were identified using a simple regular expression-based method. With more comprehensive ontologies, such as SNOMED-CT, a broader range of similarity measures, and more elaborate concept extraction methods from text, it is possible that MORE could generate embeddings that are more generalizable and accurate than those produced by the present work. Finally, in this study, we only evaluate the quality of the generated word embeddings with a semantic similarity task on two relatively small datasets. Although these test sets have been widely used as benchmarks for various similarity measures in previous work, our evaluation might have been affected by relatively low inter-rater agreement among human expert annotators and a subjective definition of the similarity in these test sets.",
        "To address these limitations, in future work, we plan to experiment by tuning different training parameters (e.g., learning rate, number of training epochs, and batch size) and fine-tuning the transformer-based models on a downstream task. Furthermore, we plan to extend the model by incorporating more and larger ontologies, such as SNOMED-CT, and other ontology-based similarity measures. In addition, we plan to leverage existing NLP tools for clinical text analysis, such as clinical Text Analysis and Knowledge Extraction System (cTAKES) and the Clinical Language Annotation Modeling and Processing (CLAMP) Toolkit, to improve the sensitivity and specificity of concept extraction from text in our pipeline. The goal of this proof of concept study is to demonstrate the advantages of MORE as a hybrid biomedical semantic similarity measure. We expect implementation of these enhancements in our methodology to further improve MORE\u2019s performance and demonstrate its advantages over the baseline models. Finally, we expect that the proposed framework has further implications beyond semantic similarity. Accordingly, in future work, we plan to evaluate the quality of the MORE embeddings on other downstream extrinsic semantic tasks, such as analogical reasoning, text classification, synonym-selection, and topic modeling.",
        "Learning high-quality word embeddings for semantic similarity in the biomedical domain is valuable for improving the statistical power of NLP analyses, thus making it easier to identify associations between conditions and clinical outcomes in health records and improve information retrieval from scientific journals and clinical reports. To address existing limitations of biomedical semantic similarity measures, we propose a new modified objective function that incorporates domain knowledge into the process for generating word embeddings. In this paper, we presented a novel framework for integrating knowledge from biomedical ontologies into an existing distributional semantic model to improve both the flexibility and accuracy of the learned word embeddings. Our implementation is based on the official TensorFlow implementation of word2vec, and we have made it available for public use. We demonstrate that MORE generally outperforms the baseline skip-gram model, three pre-trained transformer-based models, as well as the individual ontology-based similarity measures, in computing semantic similarity scores for biomedical word pairs using two benchmark evaluation datasets."
    ],
    "title": "Multi-Ontology Refined Embeddings (MORE): A Hybrid Multi-Ontology and Corpus-based Semantic Representation Model for Biomedical Concepts"
}