{
    "content": [
        "Specialised pre-trained language models are becoming more frequent in Natural language Processing (NLP) since they can potentially outperform models trained on generic texts. BioBERT (Sanh et al., Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv: 1910.01108, 2019) and BioClinicalBERT (Alsentzer et al., Publicly available clinical bert embeddings. In Proceedings of the 2nd Clinical Natural Language Processing Workshop, pp. 72\u201378, 2019) are two examples of such models that have shown promise in medical NLP tasks. Many of these models are overparametrised and resource-intensive, but thanks to techniques like knowledge distillation, it is possible to create smaller versions that perform almost as well as their larger counterparts. In this work, we specifically focus on development of compact language models for processing clinical texts (i.e. progress notes, discharge summaries, etc). We developed a number of efficient lightweight clinical transformers using knowledge distillation and continual learning, with the number of parameters ranging from  million to  million. These models performed comparably to larger models such as BioBERT and ClinicalBioBERT and significantly outperformed other compact models trained on general or biomedical data. Our extensive evaluation was done across several standard datasets and covered a wide range of clinical text-mining tasks, including natural language inference, relation extraction, named entity recognition and sequence classification. To our knowledge, this is the first comprehensive study specifically focused on creating efficient and compact transformers for clinical NLP tasks. The models and code used in this study can be found on our Huggingface profile at  and Github page at , respectively, promoting reproducibility of our results.",
        "Large language models pre-trained on generic texts serve as the foundation upon which most state-of-the-art Natural language Processing (NLP) models are built. There is ample evidence that, for certain domains and downstream tasks, models that are pre-trained on specialised data outperform baselines that have only relied on generic texts (Sanh et al. ; Alsentzer et al. ; Beltagy, Lo, and Cohan; Nguyen, Vu, and Nguyen; Chalkidis et al. ).",
        "These models, however, are heavy in size and number of parameters, making them unsuitable for devices with limited memory and processing capacity. Furthermore, the rate at which leading technology corporations build these progressively larger and more resource-intensive models is a subject of debate in the Artificial Intelligence (AI) community (Bender et al. ), and there is interest in developing methods that would make these tools more accessible by creating smaller and faster versions of them that would run reasonably well on smaller devices (Li et al. ; Schick and Sch\u00fctze). This would allow independent researchers, particularly those from low-income nations, to contribute to the advancement of AI.",
        "From the point of view of energy consumption and environmental impact, developing smaller transformer-based language models can be thought of as a step towards green AI (Schwartz et al. ), an approach to developing AI that prioritises sustainable use of computational resources and development of models with minimal carbon footprint (Strubell, Ganesh, and McCallum).",
        "We pre-trained five different compact clinical models using either distillation or continual learning on the MIMIC-III notes dataset.",
        "We used three different distillation techniques for training models in varying sizes and architectures.",
        "We evaluated our models on named entity recognition (NER), relation extraction (RE) and sequence classification (CLS) on four widely used clinical datasets plus an internal cancer identification dataset.",
        "We are the first to focus exclusively on developing compact clinical language models, and we make all of our models publicly available on Huggingface.a ",
        "Numerous works exist in the NLP literature with the aim to develop fast, efficient and lightweight versions of larger transformer-based models (Sanh et al. ; Jiao et al. ; Sun et al. ; Dehghani et al. ). However, there are comparatively fewer compact models developed for special domains like law, biology and medicine (Ozyurt; Bambroo and Awasthi; Rohanian et al. ). The present work focuses on development of efficient lightweight language models specifically developed for clinical NLP tasks. These models can be used to process a range of different clinical texts including patient history, discharge summaries and progress notes. The contributions of this work are as follows: ",
        "Clinical notes are written documents generated by medical practitioners in order to communicate information about a patient treated at a health facility (Rethans, Martin, and Metsemakers). These documents are regarded as \u2018unstructured data\u2019. This means that, unlike tabular data that are categorised and quantifiable, clinical notes are irregular, disorganised and not coded using predefined terms that domain experts would all understand (Boulton and Hammersley; Rosenbloom et al. ). Clinical notes contain a wide range of information about a patient, ranging from medical history and response to medication to discharge summaries and even billing. Fast Healthcare Interoperability Resources (FHIR)b identifies eight different types of clinical notes, including consultations notes, imaging narratives, laboratory reports and procedure notes (Bender and Sartipi).",
        "Unstructured data constitutes % of all electronic health records (EHR) data (Kong; Mahbub et al. ) and can potentially contain information that is otherwise not present elsewhere in the patient\u2019s EHR (Zhang et al. ). They can, therefore, be exploited by computational models to infer more information about a patient or develop predictive models for patient monitoring. There are numerous examples in the literature where clinical notes have been used, sometimes in conjunction with structured data, to develop diagnostic or predictive models. Some examples include adverse drug effects (Dandala, Joopudi, and Devarakonda), (Mahendran and McInnes), self-harm and drug abuse prediction (Obeid et al. ; Ridgway et al. ), hospitalisation and readmission risk (Huang, Altosaar, and Ranganath; Song et al. ), mortality prediction (Si and Roberts; Ye et al. ) and automatic phenotype annotation (Zhang et al. ).",
        "Unless preprocessed and redacted, clinical notes might contain private information about patients, and models trained on this data are known to be prone to adversarial attacks (Lehman et al. ). For this reason, they are yet to be widely used for research and different de-identification methods have been developed to automatically remove personal identifiable information from text documents in order for them to be securely shared with other researchers (Melamud and Shivade; Hartman et al. ).",
        "There is a distinction in the literature between clinical and biomedical texts and they are understood to be different in terms of their linguistic qualities (Alsentzer et al. ). Clinical notes are collected by healthcare professionals when the patient is seen or being treated. They are free-text, without a fixed structure, can contain spelling errors, abbreviations, non-standard grammar, differences in personal style and words and phrases from different languages. These characteristics contribute to the fact that they are still underutilised as a resource (Sanyal, Rubin, and Banerjee).",
        "What is referred to as biomedical texts, on the other hand, are often compilations of scientific texts in the biomedical and life sciences from resources such as PubMed. They are written in a more polished standard style,c and while they do overlap with clinical texts, they are larger in size and easier to process using standard NLP methods.",
        "Due to the differences between biomedical and clinical texts (Section 2.1), transformer-based language models that have been only trained on generic and biomedical texts are not always sufficient to capture all the complexities of clinical notes. For this reason, it is common to use pre-trained models as a starting point and either use fine-tuning to adapt them to clinical notes (van Aken et al. ; Agnikula Kshatriya et al. ) or use continual learning and further pre-train a model like BERT or BioBERT (Lee et al. ) on clinical texts (Si et al. ; Alsentzer et al. ; Qiu et al. ).",
        "Continual learning is a powerful alternative to the standard transfer learning approach which involves pre-training and fine-tuning on a target task. In this paradigm, models can adapt to new domains during the pre-training stage. This linearity resembles biological learning and also alleviates the need for excessive model re-training (Mehta et al. ).",
        "The idea here is to adapt the model to new streams of data while retaining knowledge of the previous domains (Parisi et al. ). Using this strategy, we may pre-train models that have previously been trained on biomedical texts and expose them to clinical notes and the vocabulary associated with them. While this method is not specifically related to model compression, we can develop lightweight clinical models by using already compressed biomedical models, such as BioMobileBERT, and through continual learning adapt them to the clinical domain. We explore this approach in this work in Section 3.2.",
        "As discussed in Section 1, issues like overparameterisation, computational overhead and the negative environmental impact of large pre-trained language models have led researchers to develop strategies for compressing these models into smaller, faster, but almost equally performant versions (Sun et al. ).",
        "Knowledge distillation (KD) (Hinton et al. ) is a well studied and powerful technique that is designed to create such models in a \u2018teacher\u2013student\u2019 setup, where the smaller student model learns to mimic its teacher, either task-specifically by using the teacher\u2019s outputs as soft labels or task-agnostically by looking at the outputs of a pre-training objective such as Masked Language Modelling (Devlin et al. ). The latter option allows for greater flexibility because the student may then independently be fine-tuned on the target task (Wang et al. ).",
        "DistilBERT (Sanh et al. ) is a notable example of such an effort in NLP, inspiring a slew of alternative \u2018distilled\u2019 versions of commonly used models. DistilGPT2, DistilRoBERTa, DistilBART and DistilT5 are a few examples. More recently, other powerful approaches have also appeared in the literature, some of which will be covered in Section 3.",
        "The efficacy of KD-based compression in specialised domains like biomedical and clinical texts is still understudied. Rohanian et al.  is an example of a work that focuses on development of compact biomedical transformers. To the best of our knowledge, there is no work specifically targeting KD for models trained on clinical texts. As discussed in Section 2.1, these texts contain linguistic features and terminology that differ from generic biomedical texts, necessitating a separate treatment.",
        "In this work, we utilise KD methods (Section 2.4) to train small-sized and efficient language models specialised for processing of clinical texts. First, we use KD approaches to directly extract compact models from the BioClinicalBERT model; second, we employ continual learning to pre-train existing compact biomedical models (e.g. BioDistilBERT and BioMobileBERT) using the MIMIC-III notes dataset (Johnson et al. ).",
        "In order to distil compact models from BioClinicalBERT, three different KD methods are explored in this work: DistilClinicalBERT, TinyClinicalBERT and ClinicalMiniALBERT (Fig. 3). These methods are described in detail below.",
        "This approach follows the distillation process outlined in DistilBERT Sanh et al.  with the aim of aligning the output distributions of the student and teacher models based on the Masked Language Modelling (MLM) objective, as well as aligning their last hidden states. The loss used for this approach is defined aswhere  is the input to the model,  represents the MLM labels,  and  denote the outputs of the student and teacher models, respectively,  and  are the last hidden states of the student and teacher,  is a KL-Divergence loss for aligning the output distributions of the student and teacher,  is a cosine embedding loss for aligning the last hidden states of the student and teacher,  represents the original MLM loss and  to  are hyperparameters controlling the weighting of each component in the loss function.",
        "The student used in this approach uses six hidden layers, a hidden dimension size of  and an expansion rate of four for the MLP blocks, resulting in M parameters in total. For initialising the student model, we follow the method as described by Sanh et al.  and Lee et al. . This involves using the same embedding size as the teacher and borrowing pre-trained weights from a subset of the teacher\u2019s layers.",
        "This is a layer-to-layer distillation approach based on TinyBERT (Jiao et al. ), which is intended to align the hidden states and attention maps of each layer of the student with a specific layer of the teacher. Because the student network typically uses a smaller hidden dimension size compared to its teacher, an embedding alignment loss is also included. The combined loss in this approach is defined below:where  represents the number of layers in the student model. The embedding vectors for the student and teacher models before passing to the transformer encoder are represented by  and , respectively. For the  layer of both the student and teacher models, the attention maps and hidden states are represented by , ,  and , respectively. The mapping function  is used to determine the corresponding teacher layer index for each student layer and is the same mapping function used in TinyBERT. The mean squared error (MSE) loss  is used to align the embeddings of the student and teacher models, while the MSE losses  and  align their attention maps and hidden states, respectively. The cross-entropy loss  aligns their output distributions. Finally, hyperparameters  to  control the significance of each loss component.",
        "The student model in this approach has four hidden layers, with a hidden dimension of  and an MLP expansion rate of , totalling  million parameters. Due to the difference in hidden dimensions between the student and teacher models, the student model is initialised with random weights.",
        "This is another layer-to-layer distillation approach with the difference that the student is not a fully-parameterised transformer, but a recursive one (e.g. ALBERT (Lan et al. )). We follow the same distillation procedure introduced in MiniALBERT (Nouriborji et al. ), which is similar to Equation (2). The recursive student model in this method uses cross-layer parameter sharing and embedding factorisation in order to reduce the number of parameters and employs bottleneck adapters for layer-wise adaptation. Its architecture features a hidden dimension of , an MLP expansion rate of  and an embedding size of , which add up to a total of  million parameters. Similar to TinyClinicalBERT (Section 3.1.2), the student model is initialised randomly.",
        "We investigate an alternative method for compressing clinical models through continual learning. In this approach, already compressed biomedical models are further specialised by pre-training on a collection of clinical texts using the MLM objective. Two different models, namely, ClinicalDistilBERT and ClinicalMobileBERT, are developed in this fashion.",
        "To obtain ClinicalDistilBERT, we use the pre-trained BioDistilBERT (Rohanian et al. ) model and train it further for three epochs on MIMIC-III. This model has the same architecture as DistilClinicalBERT (as described in Section 3.1.1). ClinicalMobileBERT, on the other hand, is based on the pre-trained BioMobileBERT (Rohanian et al. ) model and is also further trained on MIMIC-III for three epochs. This model has a unique architecture that allows it to have a depth of up to  hidden layers while having only  million parameters.",
        "This section discusses the NLP tasks and datasets used in this work. We briefly explain the goals and the nature of each task, followed by information on the datasets used to evaluate the proposed models.",
        "We explored four prominent tasks in clinical NLP: NER, Relation Extraction (RE), natural language inference (NLI) and lassification (CLS). Below, we provide definitions and concrete examples from each task to illustrate their objectives and characteristics.",
        "NER is the task of automatically processing a text and identifying named entities, such as persons, organisations, locations and medical terms. For example, in the sentence \u2018The patient was diagnosed with heart disease by Dr. Johnson at JR Hospital\u2019, NER would identify \u2018patient\u2019, \u2018heart disease\u2019, \u2018Dr. Johnson\u2019 and \u2018JR Hospital\u2019 as named entities and classify them as \u2018Person\u2019, \u2018Disease\u2019, \u2018Person\u2019 and \u2018Hospital\u2019, respectively.",
        "RE is the task of recognising and extracting links between entities such as genes, proteins, diseases, treatments, tests and medical conditions. For example, in the sentence \u2018The EGFR gene has been associated with increased risk of lung cancer\u2019, the RE system may recognise the association between the EGFR gene and lung cancer as \u2018connected to increased risk\u2019.",
        "In natural language inference (NLI), the goal is to determine the connection between two texts, such as a premise and a hypothesis. The connection may be defined as entailment, contradiction or neutral. For example, if given the premise \u2018The patient is diagnosed with influenza\u2019 and the hypothesis \u2018The patient is being treated for bacterial infection\u2019, the NLI system would find that the connection is \u2018contradiction\u2019. This task helps improve understanding of the connections between biomedical concepts in language.",
        "CLS is the task of assigning class labels to word sequences in a biomedical text, such as sentences or paragraphs. The aim is to correctly predict the sequence\u2019s class label based on the contextual information provided in the text. For example, given the text \u2018Patient has high fever and severe headache\u2019, the system may predict the class label \u2018symptoms of an illness\u2019.",
        "When the class labels contain assertion modifiers (negation, uncertainty, hypothetical, conditional, etc.) and reflect degrees of certainty, the task is referred to as assertion detection (AD) (Chen), which can be regarded as a subtask of CLS. For example, in the statement \u2018Patient has heightened sensitivity to light and mild headache, which may indicate migraine\u2019, the AD system would predict the class label \u2018uncertain\u2019 or \u2018possible\u2019 based on the context.",
        "We evaluate all of our models on four publicly available datasets, namely, MedNLI, i2b2-2010, i2b2-2012, i2b2-2014, and one internal dataset named ISARIC Clinical Notes (ICN).",
        "MedNLI Romanov and Shivade is a NLI task designed for medical texts, in which two sentences are given to the model and the model should predict one of the entailments, contradiction or neutral labels as the relation of the two given sentences, as shown in Table\u00a01.",
        "Treatment improves medical problem (TrIP)",
        "Treatment worsens medical problem (TrWP)",
        "Treatment causes medical problem (TrCP)",
        "Treatment is administered for medical problem (TrAP)",
        "Treatment is not administered because of medical problem (TrNAP)",
        "Test reveals medical problem (TeRP)",
        "Test conducted to investigate medical problem (TeCP)",
        "Medical problem indicates medical problem (PIP)",
        "No relations",
        " i2b2-2010 (Uzuner et al. ) is a medical relation extraction dataset,d in which the model is required to output the relation between two concepts in a given text. The relations are between \u2018medical problems and treatments\u2019, \u2018medical problems and tests\u2019 and \u2018medical problems and other medical problems\u2019. In total, this dataset uses nine labels which are as follows: ",
        "For fine-tuning our models on this dataset, we follow the same pre-processing used in the BLUE benchmark, which models the relation extraction task as a sentence classification by replacing the concepts with certain tags, as shown in Table\u00a02 ",
        "Medical Problem (PR)",
        "Medical Treatment (TR)",
        "Medical Test (TE)",
        "Clinical Department (CD)",
        "Evidental (EV)",
        "Occurrence (OC)",
        "None (NO)",
        " i2b2-2012 (Sun, Rumshisky, and Uzuner) is a temporal relation extraction dataset that contains  discharge summaries from Partners Healthcare and the Beth Israel Deaconess Medical Center. It contains inline annotations for each discharge summary in four categories: clinical concepts, clinical departments, evidentials and occurrences. In our experiments, it is used as an NER dataset with the following entity labels: ",
        "Some samples from the training dataset are provided in Figure\u00a01.",
        " i2b2-2014 (Stubbs, Kotfila, and Uzuner) consists of two sub-tasks: De-identification and heart disease risk factors identification. In our experiments, we focus on the de-identification task in which the goal is to remove protected health information (PHI) from the clinical notes. The data in this task contain over  patient records and has inline annotations for PHIs in each note. Similar to i2b2-2012, this task is also framed as NER with  labels. Figure\u00a02 shows some examples taken from the training subset of the dataset.",
        "The ISARIC COVID-19 Clinical Databasee consists of data from patients hospitalised with COVID-19 who are enrolled in an ISARIC Partner clinical study (Garcia-Gallo et al. ). The data (which are standardised to CDISC STDM formatf) include hospital admission and discharge records, signs and symptoms, comorbidities, vital signs, treatments and outcomes. Non-prespecified terms related to one of five categories; medical history, complications, signs and symptoms, new diagnosis at follow-up or category not recorded. The non-prespecified terms may consist of one or multiple diagnoses or clinical events within the same category.",
        "In December 2021, when the initial stratified sample of non-prespecified (free text) medical terms from the ISARIC COVID-19 Clinical Database was extracted, the database comprised of data from over 708,231 participants. The sample was formed of 125,381 non-prespecified terms and all five of the aforementioned categories were represented. We have released a relevant subset of this dataset along with this work and a copy of ICN, with necessary modifications for patient confidentiality, is available at .",
        "Malignancy",
        "No Malignancy",
        "Possible Malignancy (Ambiguous)",
        "For the experiments in this work, each instance of the data consists of non-prespecified (free text) terms describing clinical and adverse events along with a classification label from a number of possible choices. An annotator with clinical training used the following three labels to annotate the free-text notes: ",
        "This annotation scheme is associated with the task of AD van Aken et al.  as explained in Section 4.1.4. We refer to this annotated portion as ISARIC Clinical Notes (ICN) cancer classification dataset. Table\u00a03 contains a few examples from ICN.",
        "We pre-train all of our models on the MIMIC-III dataset for a total of three epochs using either the MLM objective or Knowledge Distillation. We follow the same pre-processing used in Alsentzer et al.  for MIMIC and use the BERT tokeniser from the Huggingface with a max length of  tokens. The details of the hyperparameters used for pre-training and fine-tuning our models are available in Tables 7 and 8.",
        "We evaluated the proposed models and the baselines on five datasets: MedNLI, i2b2-2010, i2b2-2012, i2b2-2014 and ICN. All reported results are median of three runs with different seeds. As shown in Table\u00a04, compact clinical models significantly outperform their general and biomedical baselines and achieve competitive results against BioBERT-v1.1 and BioClinicalBERT.g ClinicalDistilBERT and ClinicalMobileBERT, which are trained using continual learning, obtain the best average results among all compact models (Table\u00a04). ClinicalMiniALBERT outperforms both DistilClincialBERT and TinyClinicalBERT in terms of average results among our distilled models.",
        "Following the work of Alsentzer et al. , we explore the effect of different initialisations for our continually learned models, as shown in Table\u00a05. We find that initialising ClinicalDistilBERT with the trained weights from a pre-existing biomedical model significantly improved the model\u2019s average performance, particularly on the MedNLI dataset. However, we discovered that initialising the ClinicalMobileBERT with a similar biomedical checkpoint did not result in a significant performance boost.",
        "Latency and Giga Multiply-Accumulate Operations per second (GMACs) are two important metrics used to evaluate the efficiency of machine learning models. Latency measures the amount of time it takes for a model to process a single input and produce an output and is typically measured in milliseconds. GMACs, on the other hand, stands for Giga Multiply-Accumulate Operations and is a measure of how many computational operations a model needs to perform its tasks. GMACs are expressed in terms of billions of operations and provides a way of quantifying the computational resources required by a machine learning model.",
        "The results of the latency and GMACs of the models, as well as the model sizes, are presented in Table\u00a06. These measurements were conducted using a V100 32G GPU. The results show that there is a trade-off between accuracy and efficiency of the models. The ClinicalBioBERT model, which is listed only as representative of a class of larger-sized models (110m parameters, as listed in Table\u00a04), has the best performance on the test sets but has the highest latency and GMACs, making it less suitable for deployment in resource-constrained environments. On the other hand, the TinyClinicalBERT model has the lowest latency, GMACs and size, but its performance may not be as good as that of ClinicalBioBERT.",
        "The DistilClinicalBERT, ClinicalDistilBERT, ClinicalMobileBERT and ClinicalMiniALBERT models offer a good balance between performance and efficiency with relatively lower latency, GMACs and smaller sizes compared to the ClinicalBioBERT model. The BioClinicalBERT and BioBERT-v1.1 models, with 110 million parameters, offer the highest performance but are also the most computationally intensive.",
        "In a real-world setting, the choice of the appropriate model should depend on the specific requirements of the application, such as the required accuracy, computational resources and memory constraints. Based on the results presented in Table\u00a04, the DistilClinicalBERT, ClinicalDistilBERT, ClinicalMobileBERT and ClinicalMiniALBERT models with  million,  million and  million parameters, respectively, provide a good balance between performance, latency, GMACs and model size.",
        "To perform error analysis, we chose three of the best-performing models, namely, BioBERT, BioClinicalBERT and our proposed ClinicalDistilBERT. In order to evaluate the models on a truly unseen dataset for this analysis, we selected the internal ICN dataset (Section 4.2.3). It consists of approximately 125,000 clinical notes, with  of them having been labelled by clinicians from the ISARIC consortium. We fine-tuned the pre-trained models on the labelled section of the ICN and then used the resulting models to predict labels for all 125,000 samples. The samples in which at least two of the models disagree on the predicted label were identified as corner cases, resulting in approximately  clinical notes.",
        "To perform the error analysis, these corner cases were annotated by a clinician and the outputs of the three fine-tuned models were analysed and compared with the expert human annotations. More information about the specifics of this adjudication is provided in the Appendix C. Figure\u00a04 provides the confusion matrices for performance of the models both on the test set and on the corner cases. Based on the information from the confusion matrices, ClinicalDistilBERT performed better than the rest of the models on these cases. BioBERT, on the other hand, fared comparatively poorly, which can be attributed to its lack of clinical domain knowledge. In the following section, we present the analysis and observations of the human expert annotator about the outputs of each model on the corner cases and investigate if there are any common mistakes or recurring patterns that have caused confusion for any of the models.",
        " Interpretation of \u2018ca\u2019 Abbreviation: In the portion of ICN on which ClinicalDistilBERT was trained, the abbreviation \u2018ca\u2019 often refers to cancer. However, these two letters could also be used in other contexts; one such example was a free text term containing \u2018low ca+\u2019. It was assumed this free text was referring to low calcium levels, therefore a \u2018No Malignancy\u2019 label was assigned by the human annotator. This was in direct contrast to the \u2018malignant neoplasm\u2019 label output of ClinicalDistilBERT. On this occasion, description of the level (i.e. low) preceding \u2018ca\u2019 and the addition sign succeeding this abbreviation indicated to human annotator that this term refers to an ion outside an expected range. It could, therefore, be reasonably assumed this term refers to a calcium measurement rather than cancer. This example shows how ClinicalDistilBERT has the potential to generate more reliable results if it is further developed to more accurately process contextual information in cases where free texts comprise abbreviations with multiple interpretations.",
        " Mislabelling of Benign Terms (Adenoma, Hemangioma, Angioma): A frequently occurring inconsistency between the labels assigned by a human annotator and ClinicalDistilBERT related to certain free text terms containing the words \u2018adenoma\u2019, \u2018hemangioma\u2019 or \u2018angioma\u2019. Some examples of false positives in Clinical DistilBERT related to the term \u2018adenoma\u2019 are as follows: \u2018bilateral adenomas\u2019, \u2018benign adrenal adenoma\u2019, \u2018benign ovarian cystadenoma\u2019 and \u2018fibroadenoma of right breast\u2019. In all of these cases, the human annotator decided the note was \u2018No Malignancy\u2019 but the model labelled it as \u2018Malignancy\u2019. It is particularly interesting that ClinicalDistilBERT incorrectly labelled these, given the fact the some samples start with the word \u2019benign\u2019. The model may have been misled due to the similarities between the terms \u2018adenocarcinoma\u2019 (a type of malignancy) and \u2018adenoma\u2019 (a benign neoplasm).",
        "We discovered a pattern of discrepancy between ClinicalDistilBERT labels and human annotator labels in samples containing words such as \u2018hemangioma\u2019 or \u2018angioma\u2019. For example, the model labelled \u2018liver hemangioma\u2019 and \u2018right frontal cavernous angioma\u2019 as \u2018Possible Malignancy (Ambiguous)\u2019 but the human annotator deemed these terms as \u2018No Malignancy\u2019. This may have occurred as the structure of these words is broadly similar to a number of conditions, such as \u2018astrocytoma\u2019 and \u2018meningioma\u2019, for which it is difficult to decipher the benign or malignant nature of the free text term; these terms were therefore previously seen by the model in its training samples associated with possible malignancy. More specifically, the aforementioned diagnoses with human and model annotation disparity end in \u2018-oma\u2019; free text terms ending in \u2018-oma\u2019 may often refer to a malignancy, but this is not universally correct. The model may have identified these samples incorrectly due to spelling similarities to malignancy terms encountered during training, but it has not yet developed the ability to differentiate between them consistently.",
        " Acronym Misinterpretation (CLL, ALL, NHL): An analysis of label discrepancies among the three models revealed a pattern of incorrect labelling of certain acronyms by BioBERT and BioClinicalBERT. The models frequently mislabelled three-letter acronyms such as \u2018CLL\u2019, \u2018ALL\u2019 and \u2018NHL\u2019 as \u2018No Malignancy\u2019, while both ClinicalDistilBERT and human annotation identified them as malignant conditions. These acronyms can commonly be understood to refer to chronic lymphocytic leukaemia, acute lymphoblastic leukaemia and non-Hodgkin lymphoma, respectively. Despite the fact that these acronyms were included in the \u2018Malignancy\u2019 training samples, BioBERT and BioClinicalBERT often labelled free text terms containing these acronyms as \u2018No Malignancy\u2019. On the other hand, these models sometimes labelled certain acronyms such \u2018gbs\u2019 as \u2018Malignancy\u2019, while human annotation and ClinicalDistilBERT did not identify them as malignant.h ",
        " Impact of Text Length on Diagnosis (Term \u2018Calculi\u2019): The word \u2018calculi\u2019 frequently led to inaccuracies in the diagnosis label of all three models. Out of the 1544 corner cases studied during the error analysis,  contained the term \u2018calculi\u2019 and were all annotated as \u2018No Malignancy\u2019 during human annotation. ClinicalDistilBERT, however, labelled  of these as malignant. These samples usually contained short free text terms, with  having  or fewer words and none having more than  words. Examples include \u2018bladder calculi\u2019, \u2018left renal calculi\u2019 and \u2018staghorn calculi\u2019. The  samples which were correctly labelled as \u2018No Malignancy\u2019 by ClinicalDistilBERT were frequently of a greater length, with  samples containing \u2013 words. Examples of these samples are: \u2018psoriasis/ depression/ renal calculi\u2019, \u2018gall bladder + common bile duct (cbd) calculi\u2019 and \u2018calculi in gallbladder and common bile duct\u2019. BioBERT and BioClinicalBERT only correctly labelled one free text term containing \u2018calculi\u2019 as \u2018No Malignancy\u2019.",
        "In this work, we have presented five compact clinical transformers that have achieved significant improvement over their baselines on a variety of clinical text-mining tasks. To train our models, we utilised both continual learning and knowledge distillation techniques. For continual learning, we employed the BioDistilBERT and BioMobileBERT models, which are compact models derived using standard techniques and trained on large biomedical data. Our experiments showed that the average performance of these models can increase by up to  after training on the MIMIC-III clinical notes dataset, and they even outperformed larger baselines on the i2b2-2010 and ICN datasets (Table\u00a04). In order to determine the best approach for knowledge distillation in clinical NLP, we explored a range of different methods including standard (Sanh et al. ), layer-to-layer (Jiao et al. ) and recursive distillation (Nouriborji et al. ).",
        "Moreover, to confirm the efficacy of our methods on an unseen private dataset, we evaluated the performance of the top models on the ICN dataset by looking at the corner cases in the test set where at least two of the models disagreed and then asked an expert annotator with clinical training to adjudicate the corner cases. In this way, we managed to further assess the models on more complicated samples and provided a more in-depth analysis of where the models tend to fail and what recurring themes exist in the more challenging cases.",
        "We also evaluated the models in terms of efficiency criteria such as latency and GMACs and compared the proposed models with BioClinicalBERT. We subsequently provided guidance on selecting the optimal model based on performance and efficiency trade-offs. We hope that by making our lightweight models public, we will make clinical text-mining methods more accessible to hospitals and academics who may not have access to GPU clusters or specialised hardware, particularly those in developing countries. The top-performing models produced in this study will be integrated into the data curation pipeline under development by ISARIC and Global.health,i thereby facilitating the rapid aggregation and analysis of global data for outbreak response.",
        "The work presented here has some limitations, however. Currently, our experiments are limited to datasets in English and it remains to be seen how the models would perform on datasets in other languages. We have also not tested our models on the task of connecting named entities to medical knowledge-bases such as SNOMED CT or ICD-10, which is an important task in clinical NLP. In future work, we aim to extend our research to more tasks and languages in order to address these limitations.",
        "Another potential avenue for future work is the integration of information from other modalities such as images or electronic health records containing tabular data, including clinical laboratory test results and radiology images. This would allow us to train and evaluate multi-modal architectures, such as CLIP (Radford et al. ), and explore their utility for clinical NLP tasks.",
        "Ethics Committee approval for the collection and analysis of ISARIC Clinical Notes was given by the Health Organisation Ethics Review Committee (RPC571 and RPC572 on 25 April 2013). National and/or institutional ethics committee approval was additionally obtained by participating sites according to local requirements.",
        "This work is a part of a global effort to accelerate and improve the collection and analysis of data in the context of infectious disease outbreaks. Rapid characterisation of novel infections is critical to an effective public health response. The model developed will be implemented in data aggregation and curation platforms for outbreak response \u2013 supporting the understanding of the variety of data collected by frontline responders. The challenges of implementing robust data collection efforts in a health emergency often result in non-standard data using a wide range of terms. This is especially the case in lower-resourced settings where data infrastructure is lacking. This work aims to improve data processing and will especially contribute to lower-resource settings to improve health equity."
    ],
    "title": "Lightweight transformers for clinical natural language processing"
}