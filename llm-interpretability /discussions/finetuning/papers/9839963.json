{
    "content": [
        "Diagnosis of primary brain tumors relies heavily on histopathology. Although various computational pathology methods have been developed for automated diagnosis of primary brain tumors, they usually require neuropathologists\u2019 annotation of region of interests or selection of image patches on whole-slide images (WSI). We developed an end-to-end Vision Transformer (ViT)\u00a0\u2013 based deep learning architecture for brain tumor WSI analysis, yielding a highly interpretable deep-learning model, ViT-WSI. Based on the principle of weakly supervised machine learning, ViT-WSI accomplishes the task of major primary brain tumor type and subtype classification. Using a systematic gradient-based attribution analysis procedure, ViT-WSI can discover diagnostic histopathological features for primary brain tumors. Furthermore, we demonstrated that ViT-WSI has high predictive power of inferring the status of three diagnostic glioma molecular markers, IDH1 mutation, p53 mutation, and MGMT methylation, directly from H&E-stained histopathological images, with patient level AUC scores of 0.960, 0.874, and 0.845, respectively.",
        "ViT-WSI, is suitable for weakly supervised learning on histopathological images",
        "ViT-WSI performs tumor typing, subtyping and molecular marker prediction",
        "ViT-WSI automatically discovers brain tumor histological features",
        "Pathology; Cancer; Machine learning",
        "Definitive diagnosis of primary brain tumor almost always requires histopathology. Histopathological diagnosis of tumors usually requires pathologists with years of experience to manually examine histological details at various levels of magnification and is inherently laborious. This is further complicated by the diversity of brain tumor histological subtypes and the subtlety to differentiate among them. In this way, pathologists are met with complicated criteria for diagnosis, and subjectivity becomes unavoidable. Therefore, significant interobserver variability has been observed retrospectively in brain tumor diagnosis, especially among gliomas and meningiomas. In recent years, because of the improved understanding of tumorigenesis and advances in molecular biology experimental techniques, molecular biology assays are having a more important role in the brain tumor diagnosis workflow. As specified in the 2016 WHO classification of tumors of the central nervous system (CNS), some molecular biomarkers, such as somatic mutations in isocitrate dehydrogenase (IDH) and 1p/19q co-deletion, have become essential in the diagnosis of certain subtypes of glioma. Compared to histopathological analysis, molecular biology assays, although much more objective and reliable, are often costly and may have less availability to economically underdeveloped regions.",
        "In recent years, significant advances in digital pathology hardware have pushed the field of histopathology into the \u2018big data\u2019 era. More and more healthcare institutes worldwide have adopted the usage of high-throughput microscopic slide scanners in their daily workflow, in which histopathology slides are digitalized into whole-slide images (WSIs) for reliable long-term data storage. Correspondingly, on the software side, new computational methods have begun to emerge for automatic and objective histopathology image analysis.",
        "With the advance of deep learning-based computer vision algorithms, much research has been conducted to automate pathologists\u2019 diagnosis from different aspects, from the early works on tumor segmentation to histopathological subtyping, grading, and prognosis. More recently, deep learning-based methods have been shown to \u2018see the unseen\u2019 from H&E-stained histopathology slides, in which they are shown to have predictive power over the presence of underlying biomarkers, such as somatic mutations, microsatellite instability, and tumor mutational burden. In this way, the computational methods can serve as a low-cost alternative or secondary verification of the costly molecular biology assays.",
        "From the algorithmic point of view, early works on computational pathology are mainly based on supervised learning algorithms. They usually require pathologists\u2019 annotation of region of interests (ROIs) or selection of image patches on WSIs. The selected ROIs or image patches are then used to train a supervised machine learning algorithm for inference tasks on histopathology images. Owing to the requirement of supervision from pathologists, such methods have limitations in real-world applications because accurate annotations from pathologists are difficult to obtain. In light of the weaknesses of such methods, a lot of the so-called \u2018weakly supervised learning algorithms\u2019 have been developed in recent years. Instead of requiring pathologists\u2019 detailed annotation at patch-level or ROI-level, such algorithms only require one annotation per slide, which can be easily obtained directly from the patients\u2019 electronic health records (EHRs). Such methods have already had successful applications in tumor detection, histopathological subtyping, and tumor origin prediction.",
        "The development of computational pathology methods on primary brain tumors has been lagging considerably behind other tumor types. This is in part because of the difficulty in obtaining large annotated pathology datasets for brain tumors. The incidence of brain tumors and other CNS tumors is generally lower than tumors of other origins. There is also a lack of publicly available high-quality brain tumor histopathology datasets, with the exception of several glioma subtypes in The Cancer Genome Atlas (TCGA). Therefore, existing works have mainly focused on the classification tasks in a few primary brain tumor subtypes, mostly only within glioma subtypes. Despite the demonstrated success in other brain tumor imaging modalities, there is a lack of systematic investigation of applying weakly supervised learning algorithms on primary brain tumor H&E histopathology.",
        "To fill the gap in brain tumor computational pathology, we here propose ViT-WSI, a highly interpretable and weakly supervised model, leveraging the state-of-the-art Vision Transformer architecture in an end-to-end manner, for brain tumor WSI analysis. ViT-WSI achieves the task of weakly supervised learning through the self-attention mechanism and a constructed patch-level graph of the WSI, which benefits ViT-WSI by modeling the relationship between WSI patches in a context-aware fashion. Importantly, we demonstrate how a weakly supervised ViT-WSI, learned from H&E WSI and labels purely extracted from electronic health records (EHRs) and without any additional pathologist supervision, accomplishes the task of major primary brain tumor type and subtype classification. We develop a systematic procedure to interpret the ViT-WSI model and show how ViT-WSI automatically discovers diagnostic histopathological features directly from WSI. Finally, the ViT-WSI model fine-tuned on additional evidence from Immunohistochemistry (IHC) and molecular biology assays, can \u2018see the unseen\u2019 from H&E histopathology and precisely predict the status of three diagnostic glioma molecular markers.",
        "To leverage the strong performance of visual recognition of Vision Transformers, we designed the ViT-WSI model for weakly-supervised histopathology image analysis (Figure\u00a01). ViT-WSI can be trained in a weakly-supervised fashion to perform various brain tumor classification tasks. ViT-WSI first segments a gigapixel WSI into  megapixel patches (Figure\u00a01). The first part of ViT-WSI, the Pretrained Patch Embedder (Figure\u00a01), extracts image features of each megapixel patch and embeds them into a 1024-dimensional vector. The Pretrained Patch Embedder is a ViT-L-16 model pretrained on the ImageNet-21k dataset. Each embedded megapixel patch is then sent to the second part of ViT-WSI, the Vision Transformer Aggregator (Figure\u00a01), which performs whole-slide aggregation for the weakly supervised learning task. Vision Transformer Aggregator is also designed as a transformer architecture, aiming to better utilize the extracted features from the Pretrained Patch Embedder. During this aggregation phase, the self-attention mechanism of the Vision Transformer Aggregator models the interaction and relationship among patches. Feature similarity and closeness of physical locations between the patches are also taken into consideration by constructing a nearest neighbor graph of the patches and are input to the Vision Transformer Aggregator via embedding and attention computation (Figure\u00a01). In this way, the self-attention mechanism in the Vision Transformer Aggregator is able to aggregate the patch-level information in a context-aware manner. Finally, the classification scores corresponding to each class can be outputted from the classification head, and the output from the penultimate layer under the classification head is used as the whole-slide representation vector (WSRV) of the WSI. Detailed model configurations, design principles, and training protocol are discussed in STAR Methods.",
        "To overcome the lack of publicly available primary brain tumor datasets, we retrieved 6,173 primary brain tumor H&E slides from The First Hospital of Harbin Medical University (Figure\u00a01). The slides were formalin-fixed paraffin-embedded (FFPE) and prepared between April 2017 and April 2020. They were scanned with a high-throughput slide scanner, SQS-1000, developed by Shenzhen ShengQiang Technology Co., Ltd (https://www.sqray.com/), and then the digitalized H&E slides were converted to public file format (the \u2018svs\u2019 format) for downstream processing (Figure\u00a01, STAR Methods). Our in-house dataset covers a total of eight brain tumor types, including glioma, meningioma, pituitary adenoma, ependymoma, craniopharyngioma, CNS lymphoma, chordoma, and germ cell tumor (Figure\u00a0S1). Table\u00a01 summarizes multiple statistics of this in-house cohort. We retrieved the corresponding EHRs of the patients which contain the diagnosis of the patient by the department of pathology of The Affiliated First Hospital of Harbin Medical University. All cases were carefully reviewed by a panel of seasoned neuropathologists at the hospital and the diagnosis was the one on which subsequent treatment was based. We kept only the slides that could be matched to their patient metadata and whose brain tumor type could be determined from the EHRs (Table\u00a01, Figure\u00a0S1). In total, we assembled a cohort of 5,216 slides from 1,211 patients spanning the eight brain tumor types (Table\u00a01, Figure\u00a0S1). The consent forms of the patients were waived before this research was carried out under the retrospective research protocol of the institution. Information on eleven glioma and meningioma subtypes and glioma molecular biomarkers were further extracted from the EHR for a subset of 597 patients (Table\u00a01, Figure\u00a0S1). The eleven subtypes span five glioma subtypes (Diffusive Astrocytoma (DA), Anaplastic Astrocytoma (AA), Oligodendroglioma (O), Anaplastic Oligodendroglioma (AO), Glioblastoma (GBM)) and six meningioma subtypes (Fibrous, Meningothelial, Transitional, Angiomatous, Atypical, Anaplastic). The hierarchy of the brain tumor types and subtypes is shown in Figure\u00a0S2. Consistent with prior knowledge, the glioma subset contains more male patients (55.21%) than female patients (44.78%), whereas the meningioma subset contains more female patients (74.30%) than male patients (25.69%) (Table\u00a01). For the training and evaluation of ViT-WSI, we subsequently refer to the classification of 8-class brain tumor types as the \u20188-class top-level type classification task\u2019, and the classification of 11-class meningioma and glioma subtypes as the \u201811-class subtyping task\u2019 (statistics shown in Table\u00a01).",
        "Previous work in lung adenocarcinoma suggests a general possibility of predicting molecular biomarkers directly from the H&E histopathology. In glioma, three molecular biomarkers are of particular interest to pathologists: Isocitrate dehydrogenase (IDH), Tumor suppressor p53 (TP53), and O6-methylguanine DNA methyltransferase (MGMT). Of the mutations in IDH, the most common one is the R132H mutation in the IDH1 allele and is a frequent somatic mutation event in multiple astrocytic and oligodendroglial subtypes that imply better prognosis.TP53 is the tumor suppressor gene that frequently mutates in high-grade gliomas that are associated with higher malignancy and poorer prognosis. Methylation at the promoter region of MGMT, a gene crucial in DNA repair pathways, silences its expression and makes tumor cells more sensitive to alkylating agents used in chemotherapy. Recent work showed that patch-level classifiers trained on glioma histopathology images could predict IDH mutation with decent accuracy. Glioma MRI images are also shown to have certain predictive power of MGMT methylation. These suggest the possibility of phenotype-to-genotype inference in glioma subtypes. To investigate the possibility of ViT-WSI to make such an inference from histopathology slides, we further extracted the status of molecular biomarkers from the glioma patients\u2019 EHR data (statistics shown in Table\u00a02). A total of 304 cases presented with their IDH1mutation status determined either by IHC (108 positive cases, 196 negative cases, with positivity indicating mutation in the IDH1 gene) or Sanger sequencing (13 cases with mutation and 13 cases without mutation). 219 cases presented with their TP53mutation status, either by IHC (104 positive cases, 115 negative cases, with positivity indicating mutation in the TP53 gene) or Sanger sequencing (showing 3 cases without TP53mutation). 71 cases presented with MGMT promoter methylation status, either by IHC (46 positive cases, 25 negative cases, with negativity suggesting MGMT promoter methylation) or Methylation Specific PCR (MSP) (11 cases with methylation and 7 cases without methylation). These cases and their associated slides are then used to train and evaluate ViT-WSI for the molecular biomarker prediction task. As there are much more patients with IHC testing than Sanger sequencing or MSP, we use the status of the molecular biomarker determined from IHC for training and performance evaluation, and additionally validate them using the status of the biomarker reported by other methods.",
        "We further retrieved 1,703 glioma FFPE slides of 879 patients from the subdirectories TCGA-GBM (for glioblastoma cases) and TCGA-LGG (for lower grade glioma cases) of The Cancer Genome Atlas (TCGA) as an independent data source (statistics shown in Table\u00a01). Excluding the ambiguous subtype \u2018mixed glioma\u2019, the slides from the other subtypes (AA, AO, DA, GBM, and O) are assembled into a 5-class classification task dataset, hereafter referred to as the \u2018TCGA glioma subtyping task\u2019. To get the patients\u2019 somatic mutation status in IDH1 and TP53, we used somatic mutations that are independently called by four methods, MuSE, MuTect2, SomaticSniper, and Varscan2 on their associated whole exome sequencing data. Only when all four methods detected (or do not detect) mutation(s) in a gene, did we regard the case as unambiguous and keep it. All other ambiguous cases were discarded. For the MGMT status, we adopted the labels as having been used in based on the Infinium methylation assay. The statistics of the three molecular biomarkers of the TCGA cases are summarized in Table\u00a02.",
        "For all three tasks, we randomly make ten independent splits with a train/test ratio of 7:3 for 10-fold cross-validation. The performance statistics are reported as the average of the ten trained models on the ten splits. We also make one more independent split, on which we will perform downstream analysis and case study of the model. The patient level and slide level statistics of the splits are provided in Table\u00a0S1.",
        "After a ViT-WSI model is trained, we systematically interpreted the trained model with the gradient-based attribution analysis algorithms (Figure\u00a01). We aim to investigate whether ViT-WSI is able to discover histological meaningful features using such procedures. Attribution analysis aims to quantitatively evaluate the contribution of a particular input to an output of a deep learning model, and it serves as a cornerstone for interpretable machine learning. A large family of attribution analysis algorithms achieves this goal by comparing what the network outputs from a given input to what it outputs from a \u2018baseline\u2019 input (which is usually chosen to be an all-zero input or a random input) and assigning attribution scores to the elements of the given input according to their contribution to the difference. These attribution algorithms have been widely used to interpret Transformer-based models. We applied one of the above-mentioned attribution analysis methods, Integrated Gradients (IG), to the ViT-WSI aggregator. Suppose that the input to the ViT-WSI aggregator (represented as function ) is  (where n is the number of megapixel patches) and the output is a scalar . IG computes the attribution of the th input element as follows:where  is the partial derivative of the network  w.r.t. the kth input element,  is the above-mentioned baseline input and  is the interpolating factor which varies from 0 to 1. This guarantees that the sum of the attribution of each input element equals the total output difference, which can be expressed as:",
        "We computed the contribution of the input patches to various network output quantities, including the classification score of a particular class (the \u2018class attribution\u2019), specific dimensions of the WSRV (the \u2018WSRV attribution\u2019), as well as on the projections of WSRV on its specific principal components PC (the \u2018PC attribution\u2019). In this way, we hope to discover subtype-specific histological features, as well as histological patterns that are managed by specific dimensions of the WSRV, or certain linear combinations of them. The result of the attribution analysis is plotted in a heatmap that highlights regions with positive/negative contributions. The histological features in those regions are then reviewed by two pathologists with ten years of experience (YC and XC). Details of the formulation of attribution analysis are discussed in STAR Methods.",
        "Table\u00a03 summarizes the performance of ViT-WSI on the 8-class top-level type classification task, the 11-class subtyping task, as well as the TCGA glioma subtyping task. We compared the performance of ViT-WSI [ViT (ViT-L-16)\u00a0+ ViT-WSI aggregator] with various other weakly supervised methods including Max Pooling and CLAM-MB. We used various pretrained patch embedders for performance evaluation, including ResNet50, Inception v3 used by Coudray et\u00a0al., and the GNN network used by Jaume et\u00a0al. As previous methods were developed without the presence of pretrained Vision Transformers, to make the comparison fair, we also evaluated the performance of other methods using both ViT-L-16 as the pretrained patch embedder. We also evaluated the patch embedder performance of the ViT-S-16 and ViT-L-16 models when they are self-supervised by DINO instead of pretrained on the ImageNet as used in Chen and Krishnan. We further compared the performance of the above weakly supervised methods with non-weakly supervised methods by training a patch-level version of the patch embedders directly for classification using the slide-level labels as ground truth.",
        "In terms of the area under curve of the one versus rest receiver operating characteristics macro-averaged across each class (Macro AUC) and the Matthews\u2019 correlation coefficient (MCC), ViT-WSI with patch graph information (ViT-WSI\u00a0+ Graph) achieves the highest performance (Macro AUC 0.9408, 0.8867 and 0.9313, MCC 0.8757, 0.5628 and 0.8344, under 10-fold cross-validation) on all three tasks compared to other aggregators that use ViT-L-16 as the pretrained patch embedder. Compared to the weakly-supervised learning methods (MIL, CLAM_MB, ViT-WSI, ViT-WSI\u00a0+ Graph), using the slide-level label to train patch-level classifiers achieves the worst performance on the three tasks. There is a larger performance gap between this non-weakly-supervised method and other weakly supervised methods in the more difficult 11-class subtyping task than the 8-class top-level type classification task and the TCGA glioma subtyping task, showing the greater advantage of developing weakly-supervised methods on more fine-grained tumor classification tasks. Adding patch-level graph information to the ViT-WSI model (ViT-WSI\u00a0+ Graph) improves the performance compared to the counterpart that is without (ViT-WSI). Using ViT-WSI-based aggregation performs better than the other methods regardless of the pretrained patch embedder that is used. Consistent with the previous observation, only on the relatively easier tasks, i.e., the 8-class top-level type classification task and TCGA-glioma subtyping task, there are further improvements by using the vision transformers ViT-S-16 or ViT-L-16 self-supervised by DINO as the patch embedders, although not statistically significant (p\u00a0= 0.15 for the 8-class top-level type classification task and p\u00a0= 0.32 for the TCGA-glioma subtyping task, by Wilcoxon signed-rank test). Because of this and to be consistent with previous weakly supervised learning methods, the rest of the analyses are still performed using the ViT-WSI model with ViT-L-16 pretrained on ImageNet as the patch embedder.",
        "We next analyzed the per-class performance of the ViT-WSI\u00a0+ Graph model on the three tasks (Figures\u00a02A\u20132G). As an additional reference, we sampled 100 slides from each of the three tasks and asked one pathologist (C.W.) to perform a brief and independent pass through them. We evaluated the performance (Macro-average FPR and TPR) of the pathologist by comparing the pathologist\u2019s classification to the original diagnosis in the EHRs which was based on the consensus of a group of pathologists and used this as the \u2018human performance\u2019 of the task (Table\u00a03, Figures\u00a02C,2D, and 2F). Because of class imbalance of the dataset, the model generally has a lower performance on classes that are under-represented (e.g., ependymoma, germ cell tumor). For the 8-class top-level type classification task, there is a very low misclassification rate among the three major primary brain tumor types (glioma, meningioma, and pituitary adenoma) and they are on par with human-level performance (Figures\u00a02A and 2B). However, there is a larger misclassification rate between glioma and ependymoma (Figure\u00a02B). This is because ependymomas, arising from ependymal cells, are also a type of glial cells, and ependymomas can often be considered gliomas in a broad sense. A higher misclassification rate indicates their similarity in histopathology. For the 11-class subtyping task, misclassification is rare between the glioma subtypes and the meningioma subtypes, with one exception (several atypical meningioma slides misclassified as several glioma subtypes) (Figure\u00a02E). This is probably because both tumor subtypes have a higher level of malignancy. In both the 11-class subtyping task (Figure\u00a02E) and the TCGA glioma subtyping task (Figure\u00a02G), misclassification is in general within glioma or within meningioma subtypes, especially between oligodendroglioma (O)\u00a0and anaplastic oligodendroglioma (AO), as well as atypical and anaplastic meningiomas, which shows the difficulty of classifying tumors of similar origin and cell type but with only slightly different levels of malignancy. There is in general more gap between the model performance and human performance on the more difficult 11-class subtyping task and TCGA glioma subtyping task than on the 8-class top-level type classification task.",
        "The whole representation vectors (WSRV) extracted from the penultimate layer of the ViT-WSI model are visualized in 2D using t-distributed stochastic neighbor embedding (t-SNE) (Figures\u00a0S3A and S3B, Supplementary Notes Section S1). For the 8-class top-level type classification task (Figure\u00a0S3A), one can observe a Clear clustering of the glioma, meningioma, pituitary adenoma, and craniopharyngioma slides in the t-SNE space. Ependymoma slides are observed to be closer to the glioma slides which agrees well with their higher misclassification rate and their histopathological similarity. For the 11-class subtyping task, there is a clear separation of all glioma subtypes and meningioma subtypes (Figure\u00a0S3B). However, a large overlap is observed for meningioma subtype pairs with higher histological similarities like fibrous and transitional, as well as atypical and anaplastic.",
        "ViT-WSI distinguishes itself from other methods by incorporating the self-attention mechanism for whole-slide aggregation. The self-attention mechanism models the relationships between different patches in a WSI. It discovers semantically similar regions (Figure\u00a0S4, Supplementary Notes Section S2), and has a greater diversity and coverage than other attention-based methods (Figure\u00a0S5), which results in its greater prediction confidence (Figure\u00a0S6).",
        "We systematically interpreted the ViT-WSI model trained on the \u201811-class subtyping task\u2019 using gradient-based attribution algorithms. As an example, we first performed attribution analysis of two meningioma slides reported by pathologists as having histological features of multiple subtypes. The first is a mixed type of fibrous and transitional meningioma (Figure\u00a03A). Attribution to the \u2018fibrous\u2019 class (based on the \u2018class attribution\u2019 method, STAR Methods) highlights the regions in the upper part of the slide with cells having typical narrow, rod-shaped histopathological features. Attribution to the \u2018transitional\u2019 class (based on the \u2018class attribution\u2019 method) highlights the typical whorl structures of transitional meningioma. In some parts of the slide where the \u2018transitional\u2019 whorl features are scattered, the attribution heatmap is even more capable of highlighting them individually (Figure\u00a03A, zoom-in view). The second is a meningothelial slide showing features of angiomatous meningioma (Figure\u00a03B). Individual attribution maps for the \u2018meningothelial\u2019 and \u2018angiomatous\u2019 classes (based on the \u2018class attribution\u2019 method) again \u2018pick out\u2019 histological structures of their corresponding subtypes.",
        "Motivated by the effectiveness of the above analysis, we further extended the attribution procedure to various other network outputs. Instead of performing attribution analysis on the network\u2019s final layer (the classification output probabilities), we moved our focus to the penultimate layer, which outputs the WSRV that has been visualized in Figure\u00a0S3. From here, we wished to find what histopathological features are being attended to by the different dimensions of the WSRV. To achieve this goal, we could exhaustively perform the analysis on each dimension of the representation vector one by one and observe their correlation with some histopathological-meaningful patterns. However, this approach is inefficient and uninformed because the dimension of the representation vector is large (1,024 dimensions), and some dimensions may have overlapping or repetitive semantics.",
        "We thus used a principal component (PC)-guided attribution procedure to interpret the trained ViT-WSI model (the \u2018PC attribution\u2019 method, STAR Methods). Without performing attribution analysis on each individual dimension, we instead performed on the WSRV\u2019s projections to their PCs. The principal components are computed using all the representation vectors of the slides in the test set of the independent split of the \u201811-class subtyping task\u2019 and ordered in their amount of explained variance (Figure\u00a0S7).",
        "The PC-projected whole slide representation vectors are plotted for each slide in the \u201811-class subtyping task\u2019 test set, separately for the first six PCs (PC1-6, Figure\u00a03C) and the next eight PCs (PC7-14, Figure\u00a03G). The slides are hierarchically clustered using the \u2018average linkage\u2019 method on their first 20\u00a0PCs. One can observe a high-level agreement between the clustering result and their true histopathological subtypes, with all the subtypes almost exclusively clustered together and the glioma slides consistently clustered on the top, and the meningioma slides at the bottom. We further dropped the anaplastic meningioma subtype in the subsequent analyses because of its limited number of slides in the test set (n\u00a0= 9) and its largely overlapping clustering semantics as the atypical meningioma subtype.",
        "For the first six components, there is a general trend that projections are highly activated for a specific subtype or a few subtypes (Figure\u00a03C, summarized in the first half of Table\u00a04). As an example, PC1 is highly activated in the glioma subtypes and specifically in the glioblastoma subtype (Figure\u00a03D). On the contrary, it is mostly negative among the meningioma subtypes and specifically in the meningothelial subtype (Figure\u00a03D). Attribution of PC1\u2019s activation to a GBM slide\u2019s input patches shows strong positive attribution to most slide locations (Figure\u00a03E), with one tissue piece possessing the highest density of malignant cells showing the most (Figure\u00a03E, black arrow pointed). Attribution of PC1\u2019s activation to a meningothelial slide\u2019s input patches shows general negative attribution to most slide locations (Figure\u00a03F).",
        "Next, PC2, with high activation in fibrous meningioma (Figures\u00a03C and S8D), also has high attribution to most regions on fibrous meningioma slides (Figures\u00a0S8B and S8C). PC2 has the highest-loaded dimension #261 (Figures\u00a0S8A and S8E) attributed to tumor cells with rod-shaped nuclei (Figures\u00a0S8G and S8I). PC3 is also highly activated in glioma populations, but unlike PC1, PC3 is mainly activated in low-grade gliomas like diffusive astrocytoma and oligodendroglioma (Figures\u00a03C and S9B). Projections on PC3 can be mostly attributed to typical astrocytoma lesions (Figure\u00a0S9C) and oligodendroglioma\u2019s \u2018fried-egg-shaped\u2019 glial cells (Figure\u00a0S9D). Some of its highest-loaded dimensions, e.g., #838, are responsible for astrocytoma components (Figures\u00a0S9E and S9G), whereas others, e.g., #335, are responsible for oligodendroglioma components (Figures\u00a0S9F and S9H). Attribution of these dimensions on the slide with the \u2018wrong\u2019 subtype is more negative (attribution of #838 on the oligodendroglioma slide, Figure\u00a0S9I) or less consistent with the full PC3 attribution (attribution of #335 on the astrocytoma slide, comparing Figures\u00a0S9H and S9C).",
        "Projections on components on the next eight PCs (PC7-14, Figure\u00a03G, summarized in the second half of Table\u00a04) are generally lower in magnitude but are distributed more diversely among subtypes and tend to be activated by histopathological features that are shared across the subtypes. PC7, for example, has a slightly higher activation in fibrous, transitional, and atypical meningiomas but has a much higher activation in the angiomatous meningioma subtype (Figure\u00a03H). Attribution analysis on this component shows that it attends not only to blood vessel structures in angiomatous meningioma (Figure\u00a03I) but also to atypical meningioma slides with angiomatous characteristics (Figure\u00a03J). PC10, also being less specific to a particular subtype (Figure\u00a0S10A), attends to blood cells in hemorrhage that are less structured than those in the blood vessels among several different glioma and meningioma subtypes (Figures\u00a0S10B\u2013S10E). PC11, still non-specific to a subtype (Figure\u00a0S11A), has its attribution map that almost exclusively covers the calcification regions, representing a common histological feature in various meningioma subtypes (Figures\u00a0S11B\u2013S11D). Similarly, PC13 has been found to attend to various necrosis regions in glioblastoma, diffusive astrocytoma, and atypical meningioma (Figures\u00a0S12B\u2013S12D). The activation of PC13, being slightly higher in glioblastoma and atypical meningioma, suggests that tumors in these two subtypes more frequently display necrosis (Figure\u00a0S12A), indicating their higher malignancy.",
        "We next investigated whether ViT-WSI, trained in a weakly-supervised fashion, can predict common diagnostic molecular markers from glioma H&E slides. We further assessed if ViT-WSI can be either used clinically as a secondary check or as a low-cost surrogate for real molecular biology experiments.",
        "We systematically explored the potential of ViT-WSI for inferring the aforementioned molecular biomarkers in a weakly supervised setting. A ViT-WSI model was fine-tuned from the trained 11-class subtyping model for each of the three binary classification tasks (somatic mutation versus wild type for IDH1 and TP53, with methylation versus without methylation for MGMT) under a 10-fold cross-validation scheme using the combined dataset from the in-house and TCGA glioma slides that are identified with the three biomarkers. Overall, at the slide level, on the combined cohort of the in-house data and TCGA, ViT-WSI achieves accuracy scores of 0.8127 (ROC-AUC\u00a0= 0.8637), 0.6609 (ROC-AUC\u00a0= 0.6763), and 0.6906 (ROC-AUC\u00a0= 0.6981), respectively for the three tasks (Figure\u00a04A). However, once the result is aggregated by averaging the prediction across multiple slides of a patient, ViT-WSI achieves patient-level accuracy scores of 0.8975 (ROC-AUC\u00a0= 0.9600), 0.8175 (ROC-AUC\u00a0= 0.8738), and 0.7916 (ROC-AUC\u00a0= 0.8451), respectively for the three tasks (Figure\u00a04B). As the most frequent subtype, the performance on glioblastoma among the three tasks is generally closer to the overall performance than the other subtypes (Figures\u00a04C\u20134E). In contrast, diffuse astrocytoma, being the most under-represented subtype for TP53 and MGMT prediction, has a much lower performance. Some subtypes, such as oligodendroglioma in MGMT prediction, are observed with a much higher accuracy score than their ROC-AUC score because of the severe class imbalance within the particular subtype.",
        "In the in-house dataset, there are 29 cases whose somatic mutation is validated by Sanger sequencing in addition to IHC staining (26 cases for IDH1, Figure\u00a04F, and 3 cases for TP53, Figure\u00a04G). There are also 18 cases whose MGMT methylation status is also validated by methylation-specific PCR (MSP)(Figure\u00a04H). Despite the higher costs, these molecular biological diagnostic methods have shown higher sensitivity and specificity than the IHC-based diagnosis. ViT-WSI\u2019s predictions agree with 23 of the 26 cases with Sanger sequencing of IDH1 (Figure\u00a04F), all 3 cases with Sanger sequencing of TP53 (Figure\u00a04G), and 16 of the 18 cases with MSP of MGMT (Figure\u00a04H). Notably, ViT-WSI\u2019s predictions agree with the 4 cases (2 cases in IDH1 and 2 cases in MGMT) whose molecular biological diagnosis disagrees with IHC assays (Figures\u00a04F and 4H).",
        "For IDH1 and TP53, attribution analysis of ViT-WSI\u2019s output probability for the \u2018somatic mutation\u2019 class prioritizes out regions having typical malignancy in two anaplastic astrocytoma examples (Figures\u00a0S13A\u2013S13H). For MGMT, attribution analysis of ViT-WSI\u2019s output probability for the \u2018without methylation\u2019 class (which implies MGMT expression) shows different attribution levels on different tissue patches of the same slide (Figures\u00a0S13I\u2013S13L). Alongside the attribution heatmap, we illustrated the IHC-stained slide of an adjacent tissue section that is sliced out from the same specimen as the H&E slide used by ViT-WSI (Figures\u00a0S13C, S13G, S13K, S13D, S13H, and S13L). Because of the adjacency of the H&E and the IHC tissue sections, the two slides show similarly shaped tissue boundaries and tissue components. A strong agreement can be observed between the attribution heatmap and the IHC-stained positive regions, indicating that the regions that strongly contributed to ViT-WSI\u2019s decision-making indeed presented the underlying molecular mechanism.",
        "We have developed a Vision Transformer-based deep learning architecture, termed ViT-WSI, for weakly supervised histopathology WSI analysis in brain tumors. We showed how ViT-WSI achieved superior performance in brain tumor type and subtype classification compared to the currently available methods. We also showed how a fully differentiable ViT-WSI aggregator is amenable to attribution-based interpretation and how it automatically discovers diagnostic histopathological features. We finally showed how ViT-WSI, in a completely weakly-supervised fashion, predicts diagnostic molecular markers with decent accuracy directly from H&E images.",
        "As a two-stage framework, the pretrained patch embedder of ViT-WSI leverages the state-of-the-art Vision Transformer for feature extraction. The ViT-WSI aggregator, also being a Vision Transformer itself, allows better utilization of the extracted features and easier optimization because it simply contains additional Transformer layers on top of the patch embedder, and optimization of deep transformer models has proven to be successful. In contrast to many previous WSI weakly supervised learning algorithms, within ViT-WSI we aggregate patches in a context-aware way using self-attention and a nearest-neighbor graph that takes into account patch feature similarity and closeness of their physical locations. In this way, the model is capable of discovering semantically similar regions in the WSIs at its information processing stage (Figure\u00a0S4). The model\u2019s attention region is generally more diverse and greater in size, and its prediction is more confident than aggregation methods that deal with patches independently (Figures\u00a0S5 and S6).",
        "Misclassification of ViT-WSI between oligodendroglioma (O)\u00a0and anaplastic oligodendroglioma (AO), as well as atypical and anaplastic meningiomas, suggested a general difficulty in classifying tumors of similar origin and cell type but with only different levels of malignancy. The latest 2021 WHO Classification of Tumors of the CNS dropped the usage of \u2018anaplastic\u2019 in gliomas completely and considered them only as a grading difference from their non-anaplastic counterparts, acknowledging the subtlety of histological difference between them. In this study, we still adhered to the 2016 WHO classification system, as the 2021 WHO classification system is still too new to be adopted by most healthcare institutions worldwide.",
        "Having a bulkier input, interpreting weakly supervised deep learning models on gigapixel WSIs is even more challenging than interpreting common computer vision models on megapixel images. We showed how ViT-WSI is amenable to attribution analysis that interprets ViT-WSI with attribution scores assigned to input patches. We performed attribution analysis on multiple types of network outputs. Attribution analysis on class probability output highlights regions on WSI belonging to a particular subtype (Figures\u00a03A and 3B) or regions presenting a particular molecular mechanism (Figures\u00a0S13C, S13G, and S13K). Attribution on principal components highlights different histopathological features enriched within a particular subtype (Figures\u00a03, S8, and S9) or shared between different subtypes (Figures\u00a0S10\u2013S12).",
        "Earlier work has shown the penultimate layer output of a vision network (VGG19), trained on brain tumor histopathology image at the patch level, highly preserves image semantics. Hierarchical clustering on the penultimate layer output, which is a high-dimensional representation vector for one histopathology image patch, automatically re-discovered a large proportion of brain tumor ontological relationships. The penultimate layer of ViT-WSI also outputs a high-dimensional representation vector, i.e., the WSRV. However, it represents a gigapixel WSI instead of just one image patch. Hierarchical clustering of the ViT-WSI\u2019s WSRV also re-discovered similar ontological relations among the meningioma and glioma subtypes that are consistent with prior knowledge (Figure\u00a03C). When interpreting the high-dimensional representation vector, Faust et\u00a0al. selected the dimensions of the feature vector with different activation distributions among subtypes and exhaustively visually examined them to find out possible correlations with known histopathological features. In ViT-WSI, we instead employed a more informative PC-guided approach for feature interpretation. In this way, each PC summarizes a particular histopathological feature, whose loadings display the contribution of each dimension to the PC, and as such, important dimensions can be readily identified according to the magnitude of their loadings.",
        "Without spatial profiling, training H&E WSI-based predictive models of molecular biomarkers is inherently a weakly supervised learning problem. Previous works with machine learning models trained at the patch level have shown predictive power of H&E images over common somatic mutations such as lung tumor, liver tumor, and microsatellite instability in gastrointestinal tumor. In this work, our proposed ViT-WSI method, weakly supervised at the slide level, also showed promising potential to \u2018see the unseen\u2019 from the histopathological images and predicted the three highly informative molecular biomarkers in glioma, and especially somatic mutation in IDH1, with decent accuracy.",
        "A limitation of ViT-WSI concerns its large memory consumption, typical for Transformer-based models. Although this problem is generally manageable during model training and evaluation, it may arise during attribution analysis that quickly depletes GPU memory. This is especially the case when being used in combination with gradient-based attribution algorithms such as Integrated Gradients, as it requires multiple iterations through the network. We, therefore, had to perform all the attribution analysis by running the attribution algorithm purely with CPU on large-memory (\u223c512\u00a0GB) computational nodes. This also limits the resolution of attribution analysis. Currently, only the ViT-WSI aggregator participated in the attribution analysis, and only one attribution score is assigned to each WSI patch. Applying attribution analysis to both the pretrained patch embedder and aggregator as a whole and attributing to each pixel of the WSI is currently not feasible with ordinary computational infrastructure. Future work that involves experimentation with more memory-efficient Transformer models and attribution methods should be encouraged and appreciated.",
        "Further information and requests for resources should be directed to and will be fulfilled by the lead contact: Xin Gao (xin.gao@kaust.edu.sa).",
        "This study did not generate new unique reagents.",
        "To overcome the lack of publicly available primary brain tumor datasets, we retrieved 6,173 primary brain tumor H&E slides from The First Hospital of Harbin Medical University (Figure\u00a01, Table\u00a01). The consent forms of the patients were waived before this research was carried out under the retrospective research protocol of the institution. Our in-house dataset covers a total of 8 brain tumor types, including glioma, meningioma, pituitary adenoma, ependymoma, craniopharyngioma, CNS lymphoma, chordoma, and germ cell tumor. We retrieved the corresponding EHRs of the patients and kept only the slides that can be matched to their patient metadata and whose brain tumor type can be determined from the EHRs. In total, we assembled a cohort of 5,216 slides from 1,211 patients spanning the eight brain tumor types (Figure\u00a0S1, Table\u00a01). Summary statistics including sex and age of the studies patients are available in Table\u00a01. Information of the subtypes and molecular biomarkers for some tumor types are further extracted from the EHR for a subset of the above-mentioned patients (Tables\u00a01 and 2).",
        "Preliminary format conversion, clean-up, and quality control are performed on the in-house brain tumor dataset. The dataset generated by the SQS-1000 scanner is in its proprietary sdpc format. They are first converted to the standard Aperio format (svs) using the proprietary tool \u2018sdpc2svs\u2019 provided by the scanner manufacturer. The converted svs format has compression type \u2018JPEG\u2019 with compression quality 80 and tile size of  The slide identifier of each slide is extracted from the \u2018slide identifier label image\u2019 associated with each WSI and is subsequently used to query the EHRs of the patients to match against their diagnostic information. We then further discarded slides whose identifier is unclear/missing or not found in the EHRs (resulting in 1,247 patients and 5,502 slides, Figure\u00a0S1) and whose scanning quality is poor after a brief manual inspection (resulting in 1,221 patients and 5,297 slides). Tumortype information was successfully extracted from their respective EHRs associated with each slide for a total of 1,211 patients and 5,216 slides, with their per-type statistics shown in Figure\u00a0S1. These slides subsequently served as the \u20188-class top-level type classification\u2019 task (statistics in Table\u00a01). Furthermore, meningioma and glioma subtype information was successfully extracted for 597 patients. The meningioma and glioma subtypes cover five glioma subtypes (anaplastic astrocytoma, anaplastic oligodendroglioma, diffuse astrocytoma, glioblastoma, and oligodendroglioma) and six meningioma subtypes (meningothelial, fibrous, transitional, angiomatous, atypical, and anaplastic). These slides subsequently served as the \u201811-class subtyping task\u2019 (statistics in Table\u00a01). The hierarchy showing the relationship between the type and subtypes is shown in Figure\u00a0S2. IDH1 immunohistochemistry (IHC) tests status for 304 glioma patients (106 positives, 198 negatives, with positivity indicating mutation in the IDH1 gene), TP53 immunohistochemistry (IHC) tests status for 219 glioma patients (104 positives, 115 negatives, with positivity indicating mutation in TP53 gene), and MGMT immunohistochemistry (IHC) tests status for 71 glioma patients (49 positives, 29 negatives, with negativity suggesting methylation), respectively (Figure\u00a01). They are subsequently used for the molecular marker prediction tasks. For the TCGA slides, there is no need for format conversion because they are already in standard WSI formats (tiff or svs). All slides can be readily read programmatically with the OpenSlide whole slide imaging library.",
        "The WSIs were then segmented for tissue regions from the empty slide background. At a  down-sampled level of each WSI, the down-sampled image was converted from the RGBA space to the HSV space. The image was then converted to a binary mask using Otsu\u2019s thresholding method on the Gaussian blurred version of the saturation channel. Small artifacts such as holes and isolated points were further removed in order to carve out a representative connected component which covers the main tissue area. Subsequently, using a sliding window-based approach, the carved-out tissue area is completely covered with a number of image patches that are  in size, which serve as units for subsequent WSI analysis (Figure\u00a01). This is because WSIs are typically gigapixel images that are too large to fit into the CPU/GPU memory when they are analyzed as a whole. Because the patches are created from the WSI directly and are approximately thousands by thousands of pixels in size, we will hereafter refer to them as \u2018WSI patches\u2019 or \u2018megapixel patches\u2019.",
        "For a given WSI (), after the proper tissue segmentation and patching steps described in the previous section, they can now be represented as a list of patches, i.e., , where  is a particular patch in the segmented WSI, and  is the total number of patches that the WSI has. In a fully-supervised setting, each of the patches is associated with a ground truth label . In a weakly-supervised setting, however, one WSI is only associated with a single slide-level label, . A model\u2019s goal is to infer the slide-level label given the patches  . There is a trivial solution to the weakly-supervised problem, which is to assign a patch-level label  for each patch to be the same as the slide-level label , as has been done in the previous works. However, this solution could over-simplify the problem and assign incorrect labels to too many patches, which will confuse and lead to biased training of the model.",
        "Transformers are attention-based deep learning architectures that are originally designed for natural language processing (NLP) and have since achieved state-of-the-art performance on many NLP tasks. More recently, Transformers have begun revolutionizing the computer vision field, where it achieves superior performance on several computer vision benchmark tasks. The Vision Transformer (ViT) achieves state-of-the-art performance by supervised pretraining on a large, labeled dataset (e.g., JFT) and then transfers the knowledge to the dataset of the task (e.g., the ImageNet 2012 dataset). ViT processes images by splitting them into a number of patches (e.g., 16x16 patches) and treats the patches as if they were tokens in an NLP task. As in the traditional Transformers, ViT produces representations of each patch (token), one layer after another, based on the attention mechanism. The prediction of the whole image is then aggregated at the last layer of the network.",
        "The high performance of ViT on image recognition tasks demonstrates that Transformers\u2019 outstanding representation learning ability is generally applicable to computer vision tasks. Inspired by the success of ViT on natural image recognition and the similarity of its image processing procedure to the WSI processing procedure, i.e., by first dividing a large image into small patches, we are particularly interested in investigating whether a Transformer-based architecture can be useful for histopathological image analysis.",
        "A multi-head self-attention layer",
        "A position-wise feed-forward layer",
        "Layer Normalization layers following the above two layers",
        "Specifically, we designed a Transformer-based model, termed ViT-WSI, to address the above weakly-supervised learning task. Suppose that we have an input WSI,  The ViT-WSI architecture is composed of the \u2018Encoder Layers,\u2019which are based on the original Transformer encoder architecture used in BERT. For a specific layer , it takes in  representations from the one layer below and outputs  representations as the current layer\u2019s output:where each of the  input representations () and n output representations ( corresponds to a specific patch in the input.  contains the learnable layer parameters for the  layer. Each Encoder Layer is made up of:",
        "In each of the above equations, X serves as a shorthand notation for the layer\u2019s input, which is formed by row-wise-stacking up the internal intermediate representations of each patch from its preceding layer, and  as the notation for its output. , the learnable layer parameters of the layer , is a collection of the parameters above , whiled is the embedding dimension used throughout the two parts of the model.",
        "The pipeline of ViT-WSI can be divided into two stages: The first stage consists of a pretrained patch embedder (Figure\u00a01) that is responsible for transforming the WSI patches into token embeddings that will be used in the second stage. In the second stage, a Vision Transformer aggregator (Figure\u00a01) aggregates the information across the WSI patches and summarizes them into a single, slide-level prediction. Both the pretrained patch embedder and the aggregator consist of the Encoder Layers as described above. For the pretrained patch embedder, the parameters of its Encoder Layers are extracted from the ViT-L-16 model that is pretrained on the full Image21k dataset. It is, therefore, able to produce high-level embedding for a specific WSI patch. As for the aggregator, it contains much fewer Encoder Layers compared to the patch embedder, with randomly initialized parameters.",
        "A WSI is first segmented and split into megapixel patches as described in the previous section. Then, each patch is sent out to the patch embedder for its embedding computation. Inside the patch embedder, each patch is further split into even smaller patches, which are referred to as \u2018kilopixel patches\u2019 (Figure\u00a01), as they are usually tens of pixels by tens of pixels in size. Inside the pretrained patch embedder, each kilopixel patch is a token. After being embedded with a linear embedder layer which is also pretrained, they are sent to the Encoder Layers of the pretrained patch embedder for feature extraction. The output of the patch embedder is a patch embedding that holds the extracted features for one megapixel patch, which serves as one token in the aggregator. When the embeddings of all the megapixel patches of the WSI are computed and generated, they are input together to the aggregator for aggregation and prediction. The aggregator\u2019s Encoder Layers then process the patch embeddings layer by layer and finally summarize the representation of all megapixel patches of the WSI with a global average pooling (GAP) operation. The resulting vector serves as a whole slide representation vector (WSRV) and is attached to an MLP classification head for generating the final classification outcome.",
        "The advantages of developing ViT-WSI to solve the weakly-supervised WSI classification tasks are three-fold. Firstly, the aggregator serves as a learnable aggregation function of the WSI megapixel patches. This is in contrast to the multiple instance learning (MIL)\u2013based solution of the weakly-supervised learning problem. In the latter, a fixed operation, typically a MAX operation, selects the prediction score from a single patch in a WSI image as the whole slide-level prediction. This can be inefficient for model training, as back-propagation is performed using only one patch and is also ineffective for making the prediction, as only one patch is actually used for the whole-slide prediction. Most importantly, a fixed aggregation operation rules out the opportunity of the aggregator to learn a complex aggregation function over the WSI patches in a data-driven approach. On the contrary, our aggregator is not only a learned network but also makes full use of all patches within a WSI. The aggregator function is also fully differentiable, thereby making it more amenable to gradient-based interpretability analysis. Secondly, the ViT encoder-based aggregator fits nicely into the aggregation procedure of the WSI weakly-supervised learning problem, as (i)\u00a0it naturally allows each input example to have a variable number (up to a certain memory limit) of tokens (i.e., WSI patches) and (ii) its inference procedure respects the pairwise relationship between patches by modeling them using self-attention. This context-aware approach is in contrast to the majority of previous works on WSI weakly-supervised learning, which aggregate patches by weighting them with attention scores that are calculated independently. Based on the self-attention between patches, the ViT-WSI model\u2019s attention generally covers a greater area and has greater diversity than the aggregation methods that deal with patches independently (Figures\u00a0S5 and S6); Thirdly, the ViT-based pretrained patch embedder is more powerful in performance than previous vision networks (e.g., VGG, Inception, and ResNet), which are mostly based on convolutional neural networks (CNN). Moreover, designing the aggregator to be in a similar architecture as the pretrained patch embedder offers the advantage of introducing more compatibility between the parts, as demonstrated by a lot of network design cases with repeated blocks.",
        "The first layer\u2019s input of the aggregator is added with a node degree centrality embedding, , i.e.",
        "The computation of self-attention is added with a bias term that depends on the shortest distance between the two nodes calculated by the Floyd-Warshall algorithm 74. Concretely, the attention between patch  and  can be calculated as:",
        "To further improve the utilization of the topological structure of the WSI patches in the aggregation step, we built a WSI patch graph based on the patches\u2019 embedding similarity and inter-patch distances within the original WSI image. Then, the computation in each of the ViT encoder layers is modified to incorporate this graph information. Specifically, a nearest-neighbor graph  is constructed with a WSI, .  contains the WSI patches. And for each vertex, the k-nearest neighbors (kNN) in terms of location (coordinates) in the original WSI and the cosine similarity of the patch embeddings are connected with edges. Inspired by the recent Graphormer work, the topological information in the graph  can be added to the aggregator as follows:where  is a learned embedding, which is shared across the nodes that have the same degree centrality. Its dimension is the same as the embedding dimension of the patch embedder.where  and  are the query vector and key vector used for the attention computation, respectively, while  is the learned scalar bias term that is shared by the node pairs with the same shortest path distance.",
        "For each of the prediction tasks, whether the 8-class top-level type classification task, the 11-class subtyping task, the TCGA glioma subtype classification task or the molecular biomarker prediction tasks, the datasets are randomly partitioned into ten folds for cross-validation and fine-tuning of hyperparameters. The performance is reported as the averaged performance over the ten folds. Another independent train-test split with a ratio of 7:3 is done on each task dataset. The model trained on this separate split is used for WSRV generation and downstream attribution analysis. Slides are carefully managed in the splitting process to ensure that all folds or splits have slides from distinct sets of patients. In all cases, the network is trained with the adaptive moment estimation (Adam) optimizer. Each sample is weighted in the cross-entropy loss to overcome the class imbalance issue. We construct the network using the PyTorch deep learning framework and utilize NVIDIA A100 Tensor Core GPU as the hardware platform.",
        "The whole representation vector (WSRV) is the network value extracted from the penultimate layer (before the classification head) of ViT-WSI. The WSRVs of the slides from the test set the type and subtype classification tasks are visualized in 2D using t-distributed stochastic neighbor embedding (t-SNE) (Figures\u00a0S3A and S3B). For the 8-class top-level type classification task (Figure\u00a0S3A), one can observe a clear clustering of the glioma, meningioma, pituitary adenoma, and craniopharyngioma slides in the t-SNE space. Ependymoma slides are observed to be closer to the glioma slides, which agrees well with their higher misclassification rate and their histopathological similarity. For the 11-class subtyping task, there is a clear separation of all glioma subtypes and meningioma subtypes.",
        "Closeness in the t-SNE space between oligodendroglioma (O)\u00a0and anaplastic oligodendroglioma (AO), as well as atypical and anaplastic meningiomas, suggested a general difficulty in classifying tumors of similar origin and cell type but with only different levels of malignancy. The latest 2021 WHO Classification of Tumors of the Central Nervous System dropped the usage of \u2018anaplastic\u2019 in gliomas completely and considered them only as a grading difference from their non-anaplastic counterparts, acknowledging the subtlety of histological difference between them. In this study, we still adhered to the 2016 WHO classification system, as the 2021 WHO classification system is still too new to be adopted by most healthcare institutions worldwide.",
        "One of the distinctive features of ViT-WSI lies in its Vision Transformer-based aggregator. It aggregates patch-level embeddings using the multi-head self-attention. The multi-head attention aggregates patches through several independent heads, creating an \u2018ensemble effect\u2019 that is beneficial to the model\u2019s performance and generalizability. The self-attention mechanism models the interdependence between patches, in contrast to most previous weakly-supervised WSI learning algorithms that process each WSI patch independently.",
        "The multi-head self-attention of ViT-WSI is illustrated with three examples, including one glioblastoma (GBM) example (Figure\u00a0S4A), one transitional meningioma example (Figure\u00a0S4B), and one craniopharyngioma example (Figure\u00a0S4C). For each slide, we observed high agreement between the aggregated attention map across different heads (Figure\u00a0S4, the third column, \u2018Aggregated Attention Map\u2019) with at least one of the pathologist-annotated lesions (Figure\u00a0S4, the first column). Different regions of the slide are attended to differently by different heads (Figure\u00a0S4, the second column, \u2018Per-head Self-Attention Map\u2019). When attention scores from multiple heads are aggregated, some parts cancel out, while others are reinforced.",
        "Figure\u00a0S3 also illustrates how the ViT-WSI self-attention mechanism allows the model to discover semantically similar regions in an unsupervised fashion. By visualizing self-attention between a specific query patch and all other patches in the WSI, semantically similar regions are specifically highlighted (Figure\u00a0S4, the second column of the lower panel, \u2018Per-patch Self-Attention Map\u2019). Concretely, \u2018neuropil structures\u2019 and \u2018blood cell scattered between malignant cells\u2019 due to hemorrhage are recognized in glioblastoma; \u2018whorl structures\u2019 and \u2018arachnoidal hyperplasia\u2019 are recognized in transitional meningioma; \u2018calcification\u2019 and \u2018fibrocyte/fibroblast region\u2019 are recognized in craniopharyngioma. The patches which are highly attended in self-attention are shown to have similar semantics to the query patch in attention computation (Figure\u00a0S4, the third column of the lower panel).",
        "Attribution analysis has been adopted as an important strategy for neural network model explanation and interpretation. It contains a family of algorithms that achieve this goal by calculating real-valued importance scores that \u2018attribute\u2019 the result of a network output to a network input. The importance scores, which serve as the attribution, form an array that has exactly the same shape as the network input, with each being the attribution for each input element. There are a variety of attribution algorithms, including Integrated Gradients, DeepLIFT, and SHAP. All of the above three algorithms can calculate the attribution by comparing what the network outputs from a given input to what it outputs from a \u2018baseline\u2019 input (which is usually chosen to be an all-zero input or a random input) and assign attribution scores to the elements of the given input according to their contribution to the difference. A positive score indicates a positive contribution while a negative indicates a negative contribution.",
        "In the attribution analysis of ViT-WSI, we use Integrated Gradients (hereafter abbreviated as \u2018IG\u2019) to perform attribution analysis. We will illustrate how ViT-WSI can be amenable to this gradient-based attribution analysis under various configurations. Suppose that the input to the ViT-WSI aggregator (represented as function ) is  (where n is the number of megapixel patches) and the output is a scalar . IG computes the attribution of the th input element as follows:",
        "where  is the partial derivative of the network  w.r.t. the kth input element,  is the above-mentioned baseline input and  is the interpolating factor which varies from 0 to 1. This guarantees that the sum of the attribution of each input element equals the total output difference, which can be expressed\u00a0as:",
        "Class attribution. The classification score of a particular class is used as the network output value in the attribution analysis. This score quantifies the contribution of the input to the likelihood of a particular class. Examples are shown in Figures\u00a03A and 3B.",
        "WSRV attribution. A particular dimension of WSRV is used as the network output value. This assesses the contribution of the input to a particular dimension in the WSI representation. Examples are illustrated in Figures\u00a0S8G, S8I, S8H, and S8J.",
        "PC attribution. To systematically interpret the ViT-WSI model, we performed the Principal Component Analysis (PCA) on ViT-WSI computed WSI representations of the test set. The percentage of variance explained by the top PCs is plotted in Figure\u00a0S7. To distinguish this from another PCA performed on the input to the ViT-WSI aggregator (described later), we termed this as \u2018output PCA projection.\u2019 Suppose that the PCA-learned components matrix is  and mean vector . A WSI representation  can be projected onto the PCA principal components as follows:",
        "Overall, the following three types of outputs are used in the ViT-WSI attribution analysis.where the ith dimension of , , is the column \u2018PC i\u2019 shown in Figures\u00a03C and 3G. Its attribution to the input is computed in the same way as described above. Examples are shown in Figures\u00a03E, 3F, 3I, and 3J.",
        "Due to the large dimension (1024) of the feature vector produced by the ViT-WSI feature extractor, if they are used as the input as a whole, the IG attribution method will fail to converge under typical memory constraints. For all the three above-mentioned attribution analyses, we first precomputed another PCA projection on the input feature vector to get the \u2018projected version\u2019 of each input patch feature as follows:",
        "During network forward propagation, a reconstructed feature vector can be computed from the projection:",
        "Attribution is then performed from the three above-mentioned output values to . In this study, the attribution to a particular WSI megapixel patch is defined as the averaged attribution on the first 30\u00a0PCs. The whole attribution analysis is implemented in PyTorch with the utilization of the Captum interpretability library. Finally, the attribution heatmap is plotted separately for the positively (red) and negatively (green) attributed patches with a quantile-normalized color map.",
        "Student\u2019s t test, Mann-Whitney U Test, Wilcoxon signed-rank test, and Kruskal-Wallis H Test were performed using the statistical functions from the Scipy package (scipy.stats). Statistical significance (p values) was reported in respective figures with \u2217 (p\u00a0<\u00a00.05), \u2217\u2217 (p\u00a0<\u00a00.01), \u2217\u2217\u2217 (p\u00a0<\u00a00.001), and \u2217\u2217\u2217\u2217 (p\u00a0<\u00a00.0001)."
    ],
    "title": "Vision transformer-based weakly supervised histopathological image analysis of primary brain tumors"
}