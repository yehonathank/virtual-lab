{
    "content": [
        "Multilingual large language models (MLLMs) leverage advanced large language models to process and respond to queries across multiple languages, achieving significant success in polyglot tasks. Despite these breakthroughs, a comprehensive survey summarizing existing approaches and recent developments remains absent. To this end, this paper presents a unified and thorough review of the field, highlighting recent progress and emerging trends in MLLM research. The contributions of this paper are as follows. (1) Extensive survey: to our knowledge, this is the pioneering thorough review of multilingual alignment in MLLMs. (2) Unified taxonomy: we provide a unified framework to summarize the current progress in MLLMs. (3) Emerging frontiers: key emerging frontiers are identified, alongside a discussion of associated challenges. (4) Abundant resources: we collect abundant open-source resources, including relevant papers, data corpora, and leaderboards. We hope our work can provide the community quick access and spur breakthrough research in MLLMs.",
        "The rapid advancement of large language models (LLMs) has significantly transformed natural language processing (NLP), enabling machines to understand and generate human-like text. However, most LLMs are predominantly English centric, limiting their applicability in our linguistically diverse world. With over 7,000 languages spoken globally, there is a pressing need for models that can comprehend and generate text across multiple languages. Multilingual large language models (MLLMs) address this gap by processing and producing content in various languages, thereby enhancing global communication and accessibility. This survey provides a comprehensive overview of MLLMs, introducing a systematic taxonomy based on alignment strategies to deepen understanding in this field. By highlighting emerging trends and challenges, this survey aims to guide future research and development, fostering the creation of more inclusive and effective language models that cater to the diverse linguistic landscape of our world.",
        "This survey explores the growing field of multilingual large language models (MLLMs), addressing the challenges of linguistic diversity in natural language processing. By introducing a systematic taxonomy based on alignment strategies, the authors provide a structured understanding of MLLMs' capabilities. The paper reviews current advancements, identifies emerging trends, and outlines challenges in building inclusive language models. This comprehensive overview offers critical insights to guide future research and development, promoting more equitable and effective multilingual communication in an increasingly interconnected world.",
        "In recent years, remarkable progress has been witnessed in large language models (LLMs), which have achieved excellent performance in various natural language processing tasks. In addition, LLMs raise surprising emergent capabilities, including in-context learning, chain-of-thought reasoning, and even planning. Nevertheless, the majority of LLMs are English centric, primarily focusing on English tasks, which renders them relatively weak in multilingual settings, especially in low-resource scenarios.",
        "Actually, there are over 7,000 languages in the world. With the acceleration of globalization, the success of LLMs should be leveraged to serve diverse countries and languages. To this end, as shown in Figure\u00a01, multilingual large language models (MLLMs) possess the advantage of comprehensively handling multiple languages, gaining increasing attention. Specifically, existing MLLMs can be broadly divided into two groups based on different stages. The first series of works leverages multilingual data to tune parameters and boost the overall multilingual performance. The second series of works also adapts advanced prompting strategies to unlock the deeper multilingual potential of MLLMs during the parameter-frozen inference stage.",
        "While remarkable success has been achieved in MLLMs, there remains a lack of comprehensive review and analysis of recent efforts in the literature, which hinders the development of MLLMs. To bridge this gap, we attempt to conduct an extensive and detailed analysis of MLLMs. Specifically, we first introduce the widely used data resources and evaluation techniques. Furthermore, due to the key challenge of alignment across languages, we introduce a novel taxonomy according to alignment strategies, aiming to provide a unified perspective in the literature. This taxonomy primarily includes parameter-tuning alignment (PTA) and parameter-frozen alignment (PFA; as shown in Figure\u00a02). In particular, PTA involves fine-tuning model parameters to enhance alignment between English and target languages during pretraining, supervised fine-tuning (SFT), reinforcement learning from human feedback (RLHF), and downstream fine-tuning. On the other hand, PFA refers to alignment achieved by prompting across languages, without requiring adjustments to model parameters. Finally, we highlight some potential frontier areas and the corresponding challenges, especially MLLMs for low-resource languages, hoping to inspire follow-up research.",
        "The main contributions of this work can be summarized as follows. (1) Extensive survey: to the best of our knowledge, we present a comprehensive survey in the MLLM literature based on multilingual alignment. (2) Unified taxonomy: we introduce a unified taxonomy categorizing MLLMs into two alignment types, parameter frozen and parameter tuning, providing a systematic perspective to understand the MLLM literature. (3) Emerging frontiers: we discuss emerging frontiers and highlight their challenges as well as their opportunities, aiming to pave the way for future research developments. (4) Abundant resources: we attempt to organize MLLM resources, including open-source software, diverse corpora, and a curated list of relevant publications.",
        "We hope this work can serve as a valuable resource for researchers and inspire further breakthroughs in future research.",
        "Multilingual and cross-lingual natural language processing (NLP) has emerged as a vibrant research area. Recent surveys have shed light on multilingual models and cross-lingual transfer, examining language technologies in diverse linguistic and cultural contexts. Previously, Doddapaneni et\u00a0al. demonstrated that pretrained language models (PLMs) enhance performance in both familiar and unfamiliar languages across various tasks. Similarly, Philippy et\u00a0al. analyzed the factors affecting zero-shot cross-lingual transfer, offering an in-depth discussion. Additionally, Do\u011fru\u00f6z et\u00a0al. and Winata et\u00a0al. explored the linguistic and social dynamics of code switching, highlighting its significance in multilingual NLP. The rising demand for global multilingual systems has spurred numerous downstream tasks. For instance, Dabre et\u00a0al. examined PLMs in machine translation, while Deng et\u00a0al. focused on their role in information extraction. Panchendrarajan and Zubiaga reviewed methods for identifying fact claims in multilingual and cross-lingual settings. As PLMs are deployed in real-world applications, concerns regarding safety, fairness, and bias have grown. Hershcovich et\u00a0al. emphasized the cultural sensitivities crucial for effective cross-lingual NLP, while Jiang and Zubiaga discussed offensive language management and dataset challenges. Navigli et\u00a0al. and Ramesh et\u00a0al. highlighted the risks of bias, particularly in non-English languages, stressing the need for fairness in multilingual models. Lastly, Yadav and Sitaram expanded their reviews of multilingual PLMs to include multi-modal scenarios. In summary, existing surveys provide comprehensive insights into the technical advancements and challenges of multilingual PLMs, calling for a deeper understanding of these models across diverse cultural and linguistic environments.",
        "With the emergence of LLMs such as GPT-3 and GPT-4, various surveys have examined the architecture, capabilities, and limitations of these models, with a particular emphasis on multilingual performance and their alignment with human-like understanding. Nonetheless, there is a notable lack of comprehensive surveys specifically focused on MLLMs. To address this gap, we undertake a systematic analysis of MLLMs within the contemporary landscape of LLMs.",
        "In this section, we formally describe the definitions of monolingual LLMs and MLLMs.",
        "A monolingual LLM can process only one language at a time. For example, as illustrated in Figure\u00a03A, an English LLM and a Chinese LLM can handle English and Chinese languages separately. Formally, considering a set of languages , given an input utterance  in languages , the process of a monolingual LLM () generating the output  can be defined aswhere Unexpect indicates that the LLM generates an unexpected output in an unintended language; mono denotes the single language that the LLM can correctly process.",
        "As shown in Figure\u00a03B, unlike a monolingual LLM, an MLLM is capable of handling and producing content in various languages simultaneously, such as English and Chinese. Formally, for an MLLM , where  and , the multilingual response of the MLLM is given bywhere  and  belong to multiple languages in multi.",
        "In this section, we describe the widely used data resources in pretraining, SFT, and RLHF stages in MLLMs.",
        "As shown in Table\u00a01, the widely used multilingual corpora for pretraining in MLLMs can be divided into three categories: (1) manual creation: high-quality pretraining corpora obtained through manual creation and proofreading, including the Bible Corpus and MultiUN. (2) Web crawling: this involves crawling extensive multilingual data from the internet, which include OSCAR, CC-100, mC4, and Redpajama-v.2. Relatively speaking, the data quality obtained through extensive crawling is often poor; however, the sheer volume of data compensates for this by providing substantial world knowledge and long-tail knowledge. Another category of data is extracted from Wikipedia to enhance the knowledge embedded in MLLMs. Common datasets include Wikipedia, WikiMatrix, and WikiExpl. Since Wiki data are authored by humans, they feature high quality and sufficient knowledge density, making them a crucial resource for injecting knowledge into MLLMs. (3) Benchmark adaptation: this refers to re-cleaning or integrating existing benchmarks to enhance data quality, which includes datasets such as OPUS-100, Culturax, OPUS, WMT, and ROOTS. The pretraining data produced by this method are of higher quality than web-crawled data. However, this also results in such data being relatively scarce and lacking diversity.",
        "Similarly, as shown in Table\u00a02, we categorize the existing multilingual SFT data into four classes: (1) manual creation: this involves acquiring SFT corpora through manual creation and proofreading, which includes Sup-NatInst, OpenAssist, and COIG-PClite. This method ensures the highest quality, but it is costlier and produces a smaller volume of labeled data. (2) Machine translation: this method translates existing monolingual datasets into multilingual instruction datasets, which comprise xP3-MT, MGSM8KInstruct, CrossAlpaca, MultilingualSIFT, and Bactrain-X. This approach generates extremely large quantities of data with moderate quality. Its advantage lies in rapidly producing a substantial amount of non-English SFT data. However, it may fail to account for the cultural background of specific languages, leading to implicit biases. (3) Benchmark adaptation: this method transforms from existing benchmarks into an instruction format.\u00a0Widely used datasets include xP3, PolyglotPrompt, and BUFFET. The data quality of this approach is high, but the diversity of tasks and instructions is limited. (4) MLLM-aided generation: such a strategy means that the data are automatically synthesized by MLLMs, containing Vicuna, OverMiss, ShareGPT, BELLE, MultiAlpaca, Guanaco, and Alpaca-4. Data generated from advanced MLLMs may surpass translation quality in high-resource languages. However, it is concerning that data quality may degrade in low-resource languages.",
        "Furthermore, some works leverage multilingual RLHF data to improve alignment. Specifically, Lai et\u00a0al. leverage multilingual ranking data to train a reward model using RLHF. Similarly, Zeng et\u00a0al. introduce the TIM dataset to train a more effective reward model in multilingual contexts. This type of data is often labeled with preferences for translation tasks and is of high quality, though there remains room for improvement in task diversity.",
        "To facilitate the comparison of LLMs, extensive efforts have been dedicated to exploring enhanced evaluation methods for multilingual scenarios. This discussion elaborates on MLLM evaluation, covering both (1) evaluation metrics and (2) evaluation benchmarks.",
        "Using traditional automatic metrics means that we assess the predicted output using decoding probabilities or PLM logits. In general, researchers utilize BLEU, BLEURT, chrF++, and COMET for translation evaluation and ROUGE for summary evaluation. Further, Guerreiro et\u00a0al. propose xCOMET for improved translation evaluation through fine-grained error detection. To assess the general quality of the generated text, the commonly employed approach is to utilize multilingual BERTScore as an evaluation metric. Qin et\u00a0al. extend Roscoe to multi-language for quality assessment of multilingual CoT. Furthermore, Hlavnova and Ruder develop a comprehensive and robust multilingual checklist system to thoroughly assess the MLLMs\u2019 performance.",
        "This approach employs robust MLLMs to score or compare generated outputs for evaluation purposes. Specifically, Zheng et\u00a0al. introduce LLM-as-a-judge, where GPT-4 is prompted to assess the performance of other LLMs by comparing its output to the predicted one. However, this method remains unreliable in multilingual settings. Moreover, caution should be exercised particularly in languages where the MLLM is known to perform poorly. Furthermore, Kim et\u00a0al. and Muller et\u00a0al. conduct an attribution evaluation to thoroughly assess the robustness of the model.",
        "Human evaluation involves manually assessing MLLMs through detailed evaluations. Specifically, Lyu et\u00a0al. initially explore the multilingual challenges of ChatGPT through manually annotated cases. Furthermore, Hu et\u00a0al. introduce a new multilingual platform to facilitate more convenient manual assessments.",
        "Current MLLMs tend to focus more on the alignment performance of non-English languages. Based on different perspectives of alignment, we categorize this into two areas: (1) natural language understanding and (2) natural language generation.",
        "Linguistics analysis. For multilingual models, the fundamental requirement is understanding the linguistic differences across languages. The most common multilingual linguistic assessments include part-of-speech (POS) tagging, grammar analysis, and morphology. Additionally, Zhang et\u00a0al. and Song et\u00a0al. provide a comprehensive evaluation of the linguistic acceptability of MLLMs across languages.",
        "Semantic understanding. Compared with linguistics analysis, researchers are more focused on the ability to analyze and understand the specific semantics of multiple languages. The most fundamental aspect of multilingual NLP is performing local semantic understanding, with the most typical task being information extraction, including datasets such as masakhaNER, MASSIVE, MultiCoNER, WikiAnn, and SMiLER. The second level involves a semantic understanding of complete sentences, which includes tasks like XNLI, Paws-X, MixATIS++, MTOP, MultiNLU, and PRESTO. Finally, semantic understanding can be further extended to paragraphs, exemplified by question-answering tasks with\u00a0context, such as MLQA, XQuAD, TyDiQA, X-PARADE, X-CLAIM, Readme++, XKaggle-DBQA, and de Varda and Marelli. Due to the emergence of a large number of multilingual benchmarks in recent years, a series of\u00a0works has begun to combine various existing semantic understanding tasks into unified evaluations, including XTREME, XTREME-R, XGLUE, MEGA, MEGAVerse, AGIEval, and Superlim. Further, Thapliyal et\u00a0al., Changpinyo et\u00a0al., Fujinuma et\u00a0al., and Kudugunta et\u00a0al. extend semantic understanding to multi-modal contexts. Given that MLLMs exhibit certain biases or vulnerabilities, a growing body of work is dedicated to developing benchmarks specifically designed to rigorously evaluate the performance and reliability of MLLMs in addressing these issues.",
        "Cultural understanding. Limited by cultural differences, understanding between different languages is not completely parallel. Consequently, researchers have begun exploring ways to evaluate multi-cultural scenarios, with the most typical being multi-cultural sentiment analysis. Furthermore, Zhang et\u00a0al. expands the multi-cultural scope to the sociopragmatic understanding level. Specifically, Kabra et\u00a0al., Wang et\u00a0al., Jiang and Joshi, Fung et\u00a0al., Li et\u00a0al., Son et\u00a0al., and Zhou and Zhang propose new benchmarks that require models to fully comprehend diverse cultures. Furthermore, with the emergence of reasoning capabilities, Qin et\u00a0al., Liu et\u00a0al., and Wang et\u00a0al. start to evaluate the reasoning abilities of MLLMs across different cultural backgrounds.",
        "Knowledge understanding. A large amount of work has been done to test the degree of knowledge transfer of MLLMs between different languages through examination questions. Specifically, Hardalov et\u00a0al., Xuan-Quy et\u00a0al., Zhang et\u00a0al., Nie et\u00a0al., and Ni et\u00a0al. propose comprehensive knowledge tests in multilingual scenarios. Zhang et\u00a0al. design a complex translation strategy to translate existing benchmarks for multilingual evaluation. On this basis, M3Exam and EXAMS-V further expand comprehensive knowledge testing to multilingual and multi-modal scenarios. Furthermore, Gekhman et\u00a0al. test the factual consistency of MLLMs, and Jin et\u00a0al., Joseph et\u00a0al., Zhao et\u00a0al., Wei et\u00a0al., Goenaga et\u00a0al., Datta et\u00a0al., and Thulke et\u00a0al. propose benchmarks to evaluate the multilingual scientific and professional domain knowledge of current MLLMs.",
        "Translation. In the process of multilingual alignment, in addition to testing whether multiple languages are aligned in terms of understanding capabilities, researchers often need to consider whether they can also be aligned in terms of output capabilities. The most typical task is machine translation. Currently, commonly used datasets include FLORES-101, FLORES-200, WMT, and DiaBLa. Furthermore, Lou et\u00a0al. propose CCEval for Chinese-centric translation to enable comprehensive evaluation on MLLMs. Due to the significant gap between languages, Kuparinen et\u00a0al., Wassie, Liu et\u00a0al., Rakhimova et\u00a0al. focus more on low-resource-language translation. Additionally, Yang et\u00a0al., Gueuwou et\u00a0al., Bellagente et\u00a0al., Zhong et\u00a0al., and Tuo et\u00a0al. further extend translation and restatement tasks into multi-modal settings for practical scenarios.",
        "Reasoning. Currently, the most commonly used reasoning ability assessments for MLLMs focus on commonsense and mathematical reasoning. Specifically, commonsense reasoning includes XCOPA, MARC, XWinograd, GEOMLAMA, X-CSQA, XStoryCloze, ASPEN, and Masakhanews. Mathematical reasoning includes MGSM and WizardMath. Additionally, due to the high cost of annotations for multilingual reasoning, Zhang et\u00a0al. propose a complex translation and filtering process to construct a multilingual reasoning benchmark.",
        "Coding generation. The generation of code by MLLMs necessitates the capability to produce structured and executable programs based on multilingual natural language instructions. Commonly utilized benchmarks for evaluating this capability include XSPIDER, XSEMPLR, ODEX, Mconala, and HumanEval-XL.",
        "Summarization. To test the summarization ability of MLLMs, summarizing key information from multilingual long texts is required. The simplest example is that from Ryan et\u00a0al., who propose a multilingual text reduction benchmark for the evaluation of MLLMs. Secondly, much work focuses on cross-lingual summarization, with typical datasets including XSUM and CrossSum. On this basis, Wang et\u00a0al. introduce multilingual conversation summarization, and Zhang and Eickhoff propose incorporating code switching into evaluations, making them more practical. Urlana et\u00a0al. further propose headline summarization for Indian languages. SEAHORSE extends this work to multifaceted multilingual summarization. Additionally, Nguyen et\u00a0al. and Verma et\u00a0al. develop summarization benchmarks for multi-modal scenarios.",
        "Dialogue. The communication between models and humans is\u00a0often interactive; hence, a series of works pay attention to the\u00a0dialogue ability of MLLMs. Current evaluation sets include\u00a0xDial-Eval, Multi3WOZ, DIALIGHT, HPD, and X-RiSAWOZ. Since multi-turn dialogues are inherently uncontrollable, traditional metrics are insufficient. To address this, Mendonca et\u00a0al. utilize PLMs for multi-turn dialogue evaluation. Furthermore, Mendon\u00e7a et\u00a0al. propose a new benchmark that enables more robust evaluation by leveraging PLMs. Finally, Ferron et\u00a0al. introduce the MEEP benchmark to further assess the dialogue participation of MLLMs.",
        "While the performances of most prevalent MLLMs are exceptional for English, their effectiveness in other languages is notably lower, primarily due to the limited availability of linguistic resources. Consequently, alignment emerges as an effective strategy for improving this performance. As demonstrated in Table\u00a03, efficient alignment can even surpass the model\u2019s scaling laws, yielding superior results.",
        "Inspired by this, as shown in Figure\u00a04, we introduce a unified taxonomy focusing on multilingual alignment, which includes PTA and PFA, aiming to provide a systematic framework for researchers to better understand the MLLM literature. Specifically, PTA comprises a series of progressively advanced training and alignment strategies, including pretraining alignment, SFT alignment, RLHF alignment, and downstream fine-tuning alignment. These PTA stages collectively aim to refine model parameters to comprehensively improve multilingual performance. Conversely, PFA focuses on four prompting strategies based on the MLLMs trained with PTA: direct prompting, code-switching prompting, translation alignment prompting, and retrieval-augmented alignment. These PFA methods retain the original parameters to achieve the desired outcomes.",
        "PTA refers to the process of tuning the parameters of MLLMs to achieve better cross-lingual alignment. As shown in Figure\u00a05, we discuss four categories of PTA, including PTA during the pretraining stage, the SFT stage, the RLHF stage, and the fine-tuning stage.",
        "From-scratch pretraining alignment. A range of approaches have achieved alignment across languages by tuning the initially random parameters of MLLMs during pretraining (see Figure\u00a05A). Specifically, Blevins and Zettlemoyer, Briakou et\u00a0al., and Holmstr\u00f6m et\u00a0al. observe that adding even a small amount of multilingual data during from-scratch pretraining alignment, whether intentional or not, can significantly enhance multilingual performance. Inspired by this, Zeng et\u00a0al. and Su et\u00a0al. proactively incorporate bilingual data into their from-scratch pretraining for alignment. Furthermore, a range of studies, such as mT5, Ernie3.0, ByT5, BLOOM, LLaMA, PaLM, Mistral, Mixtral, PolyLM, and Nemotron-15B, incorporate multilingual data in the pretraining stage for better alignment. Blevins et\u00a0al. utilize mixture of experts (MoE) to independently train language models on subsets of multilingual corpora to alleviate the problem of multilingual parameter competition. Furthermore, to enhance the performance of low-resource languages, umT5 and XGLM adopt equitable multilingual data sampling methods during from-scratch pretraining for more effective alignment. Muraoka et\u00a0al. introduce VCT to leverage vision for indirect cross-lingual alignment in from-scratch pretraining.",
        "Continual pretraining alignment. To address the high computational cost of from-scratch pretraining, continual pretraining alignment is proposed to build the continual training process upon pretrained MLLMs (as shown in Figure\u00a05A). Specifically, CPM-2, Sabia, FinGPT, X-Gen, AFP, Cabrita, LLaMAntino, CroissantLLM, MedMT5, and Tang et\u00a0al. focus on adding more target language data during continual pretraining to enhance general performance. Further, Cui et\u00a0al., Yamaguchi et\u00a0al., and Lin et\u00a0al. emphasize extending the MLLMs\u2019 vocabularies to adapt to new languages and enable more effective decoding. Singh et\u00a0al. and Fujii et\u00a0al. demonstrate that continual pretraining on a specific language significantly enhances model performance across related languages. Blevins et\u00a0al. extend continual pretraining to the MoE paradigm for improved parameter efficiency. To achieve deeper model alignment, Xu et\u00a0al. and Guo et\u00a0al. introduce a novel continuous pretraining paradigm, which is structured in two stages. Initially, the model undergoes pretraining on a substantial corpus of monolingual data. Subsequently, it engages in continual pretraining utilizing multilingual parallel data.",
        "As illustrated in Figure\u00a05B, PTA in the SFT stage involves leveraging multiple multilingual task datasets with instruction formats for tuning parameters. In particular, MLLMs like Flan-PaLM, mT0, BLOOMz, PolyLM, Tk-Instruct, Chinese-Alpaca, Bayling, Phoenix, and Bode directly incorporate multilingual data in the SFT stage to achieve implicit multilingual alignment across languages. Besides, to address the scarcity of multilingual SFT task data, PaLM2, Zhu et\u00a0al., Cahyawijaya et\u00a0al., Li et\u00a0al., Gao et\u00a0al., and Aryabumi et\u00a0al. introduce translation tasks during the SFT alignment stage to improve alignment. Furthermore, Upadhayay and Behzadan, Chai et\u00a0al., and Zhu et\u00a0al. have begun exploring more effective SFT alignment strategies to optimize the reasoning process.",
        "As shown in Figure\u00a05C, to achieve alignment in the RLHF stage, Okapi, LLaMA2-Chat, ChatGLM, Baichuan, Huozi, Chinese-Tiny-LLM, Qwen, InternLM, ParroT, TigerBot, MOSS, YAYI-2, Yang et\u00a0al., Moura Ramos et\u00a0al., Nemotron-340B, and Orion directly integrate multilingual RLHF data for training multilingual reward models. Additionally, Zeng et\u00a0al., Dong et\u00a0al., and She et\u00a0al. introduce a multilingual reward model to compare translation outputs at different levels of granularity. Sun et\u00a0al. propose the Salmon framework to enhance multilingual RLHF by self-generating rewards for better alignment. Furthermore, Xu et\u00a0al. introduce contrastive preference optimization (CPO) for translation tasks to address memory or speed inefficiencies in direct preference optimization (DPO).",
        "Full-parameter fine-tuning alignment. Full-parameter fine-tuning in MLLMs involves tuning all parameters for downstream tasks (see Figure\u00a05D). Specifically, GShard, Linguist, Fan et\u00a0al., Bapna et\u00a0al., Tseng and Lin, Iyer et\u00a0al., NLLB, AlexTM, and BigTrans focus on directly fine-tuning all parameters across various downstream tasks (e.g., information extraction, machine translation). Xu et\u00a0al., Huot et\u00a0al., Yuan et\u00a0al., and Li et\u00a0al. propose multi-step or fine-grained alignment strategies during full-parameter tuning. To enhance efficiency, Awasthi et\u00a0al., De Raedt et\u00a0al., Thakur et\u00a0al., Whitehouse et\u00a0al., Bansal and Sharma, Xu et\u00a0al., and Reinauer et\u00a0al. focus on fine-tuning alignment by knowledge distillation from larger to smaller MLLMs. Furthermore, Zhang et\u00a0al. identify a scaling law for fine-tuning in translation tasks, significantly advancing the understanding of performance improvements through multilingual fine-tuning alignment.",
        "PEFT. A series of studies employ parameter-efficient fine-tuning (PEFT) alignment approaches to reduce the costs of full-parameter fine-tuning, which is shown in Figure\u00a05D. Agrawal et\u00a0al., Tu et\u00a0al., Park et\u00a0al., and Dai et\u00a0al. utilize minimal soft-prompt prefixes for improved fine-tuning alignment. Furthermore, Whitehouse et\u00a0al., Xiao et\u00a0al., Aggarwal et\u00a0al., Le et\u00a0al., and Singh et\u00a0al. introduce methods based on low-rank adaptation (LoRA) to achieve PEFT alignment. Further, Yoon et\u00a0al. propose a LangBridge model to bridge a multilingual encoder to a single-lingual LLM, effectively achieving promising performance. In addition, Zhao et\u00a0al. introduce two distinct types of adapters: one tailored for language processing and the other for task-specific applications. These adapters can be effectively combined and integrated, facilitating rapid adaptation to new tasks or datasets.",
        "To further evaluate the effectiveness of various PTA strategies, as shown in Table\u00a03, we compare different MLLMs trained at various stages using the XNLI benchmark, a widely recognized assessment for multilingual understanding.",
        "Performance in English vs. other languages. All MLLMs exhibit consistent and strong performance in English, attributable to the extensive availability of English training data and the emphasis on English during model development. For example, GPT-4 achieves an accuracy of 84.9% in English, compared to 68.1% in Urdu. This trend extends to models like GPT-3.5-turbo and Aya-13B, which also perform better in English than non-English languages. These disparities highlight the challenges that MLLMs encounter when addressing a diverse range of languages characterized by varying linguistic structures, data quality, and resource availability.",
        "Impact of training stages on performance. The training stage of MLLMs plays a crucial role in determining their multilingual capabilities. Models in the pretraining stage generally develop fundamental cross-lingual abilities, especially in non-English languages. For instance, BLOOMZ and AFP-BLOOM, which heavily rely on the linguistic performance of the pretrained model BLOOM, exhibit limited average improvements of 0.8% and 2.1%, respectively.",
        "Importance of alignment over model size. Recent findings suggest that the degree of alignment, achieved through pretraining, SFT, and RLHF, is more critical to MLLM performance than simply increasing the model size. For instance, despite having fewer parameters than LLaMA-2-70B-chat, Aya-13B demonstrates superior performance across multiple languages, including those with fewer resources. This observation underscores that effective alignment strategies enable models to generalize more effectively across languages, thereby narrowing the performance gap between high- and low-resource languages.",
        "Takeaways. (1) PTA during the pretraining stage brings essential multilingual capabilities to MLLMs. (2) The effectiveness of alignment in MLLMs is greatly influenced by prior alignment stages (e.g., pretraining significantly impacts SFT).",
        "In contrast to traditional parameter-tuning approaches, PFA methods aim to perform alignment without any parameter tuning. The most popular approaches employ prompting strategies to elicit the alignment potential of MLLMs. As shown in Figure\u00a06, this section discusses four prompting strategies for alignment without parameter tuning, consisting of (1) direct prompting, (2) code-switching prompting, (3) translation alignment prompting, and (4) retrieval-augmented alignment.",
        "As shown in Figure\u00a06A, direct prompting refers to directly outputting the request without any additional instructions for implicit alignment through the MLLM itself. Even in specific scenarios, MLLMs may actively select the language they excel in or find most suitable for expression, thereby achieving effective language alignment.",
        "As shown in Figure\u00a06B, this approach integrates multilingual words into a single-language utterance, a typical linguistic phenomenon that facilitates effective language alignment. Specifically, Yong et\u00a0al. and Amin et\u00a0al. demonstrate the effectiveness of MLLMs in cross-lingual alignment through model-generated code-switching texts. Moreover, Zhang et\u00a0al. highlight the need for fairer and more detailed code-switching optimization in future research.",
        "Translation alignment prompting approaches involve translating the query into other languages to achieve better alignment (see Figure\u00a06C). These approaches can be categorized as follows: (1) key information translation: this approach focuses on extracting key information and executing translation for word-level cross-lingual alignment. (2) Direct translation: the model directly translates the whole input, enhancing alignment performance, which even exhibits superior results compared to the Google Translation API. (3) Step-by-step translation: instead of direct translation, this method prompts MLLMs to translate the whole input step by step. (4) Restatement: beyond preserving the original semantics, some studies focus on prompting MLLMs to restate multilingual inputs to enhance cross-lingual effectiveness. Further, considering the differences in multiple languages, Qin et\u00a0al., Ranaldi et\u00a0al., and Zhang et\u00a0al. integrated knowledge and translation strategies across different languages by cross-lingual prompting.",
        "Retrieval-augmented alignment incorporates external retrieval during prompting to inject additional knowledge into MLLMs (see Figure\u00a06D). Specifically, He et\u00a0al., Zhang et\u00a0al., Conia et\u00a0al., Xu et\u00a0al., and Ahmad focus on retrieving cultural or professional knowledge to enrich the prompts. Another series of work focuses on retrieval for high-quality alignment demonstrations, yielding significant improvements. To address the challenge of limited knowledge in low-resource languages, Huang et\u00a0al. propose a novel paradigm that integrates a language-specific detector designed to enhance low-resource knowledge. This approach compels MLLMs to select a pertinent language, followed by the execution of answer replacement and integration processes.",
        "To further evaluate the effectiveness of different PFA strategies, as shown in Table\u00a04, we compare different prompting strategies on an MGSM benchmark (a widely used multilingual reasoning benchmark). Notably, due to limitations in code-switching alignment, no relevant optimal prompting strategy currently exists for MLLMs.",
        "MLLM performance in translation alignment prompting. As shown in Table\u00a04, translation alignment prompting consistently improves MLLM performance across various languages. Notably, Cross-ToT and AutoCAP achieve high average scores of 80.7% and 78.6%, respectively, surpassing even few-shot results. These methods excel in widely supported languages like Spanish, Russian, and Chinese while also boosting performance in low-resource languages such as Swahili and Telugu. This underscores the importance of high-quality translation alignments in enhancing MLLM generalization and multilingual capabilities.",
        "Further improvement via retrieval-augmented alignment prompting. Retrieval-augmented prompting further enhances multilingual performance by incorporating external knowledge during alignment. Approaches like En-CoT\u00a0+ 5-shot and XLT\u00a0+ 5-shot yield notable gains, especially in low-resource languages, with XLT\u00a0+ 5-shot achieving an average score of 71.3%. By leveraging external information, these methods address knowledge gaps, resulting in more accurate and context-aware responses. This underscores the critical role of retrieval mechanisms in enhancing knowledge alignment and performance in multilingual tasks.",
        "Takeaways. (1) Translation alignment prompting is more effective for cross-lingual alignment. (2) Retrieval-augmented alignment further mitigates knowledge gaps in MLLMs.",
        "In this section, as illustrated in Figure\u00a07, we highlight some emerging frontiers in the field of MLLMs, aiming to spur more breakthroughs in the future.",
        "Despite significant advancements in MLLMs, hallucination remains a critical concern that undermines their reliability. For instance, as shown in Figure\u00a07, an MLLM generated false information about a historical event, leading to misinformation in an educational content, which highlights the need for robust mechanisms to mitigate such occurrences. Specifically, Guerreiro et\u00a0al., Aharoni et\u00a0al., Dale et\u00a0al., and Qiu et\u00a0al. have previously identified the hallucination phenomenon in current MLLMs, particularly in multilingual summarization and translation tasks. Furthermore, several studies propose corresponding solutions at different stages of the model life cycle. For instance, during the pretraining stage, Pfeiffer et\u00a0al. introduce modular multilingual pretraining to address this issue. Chen et\u00a0al. propose segment-weighted instruction embedding (SWIE) at the SFT stage to enhance the model\u2019s instruction understanding and introduce the instruction-following dataset OVERMISS, which compares over-translation and mistranslation results with correct translations. During inference, a series of works explore calibration through faithful decoding.",
        "The key challenges in this direction include the following: (1) multilingual hallucination detection: effectively detecting the hallucination phenomenon of MLLMs across different languages is the primary problem to be addressed in this field. (2) Multilingual hallucination alleviation: current strategies for hallucination alleviation still focus on incorporating extensive factual data or utilizing external systems, which pose significant challenges for multiple languages, especially low-resource languages.",
        "The challenge of maintaining accuracy while updating current knowledge is a persistent issue for MLLMs, particularly when addressing multilingual datasets. A relevant case, as shown in Figure\u00a07, occurs when an MLLM edits medical guideline knowledge about diabetes internally in English but does not effectively translate it into multiple languages, resulting in confusion and misinterpretation by healthcare professionals in non-English-speaking countries. This emphasizes the importance of real-time multilingual knowledge editing. To solve this issue, Wu et\u00a0al., Wang et\u00a0al., and Beniwal et\u00a0al. introduce a multilingual knowledge editing approach and propose a new benchmark for knowledge editing in MLLMs. In addition, Qi et\u00a0al. introduce the cross-lingual consistency metric to ensure factual consistency across languages. Additionally, Wang et\u00a0al. incorporate a multilingual knowledge base into MLLMs through retrieval methods to facilitate knowledge editing.",
        "The key challenges of this research include the following: (1) continuous knowledge editing: how to continuously integrate new language knowledge while preserving the accuracy of existing knowledge is a core challenge to explore. (2) Balancing universal and language-specific knowledge: current work often neglects language-specific details, such as culture and slang, which impacts the user experience and can lead to cultural conflicts. Balancing universal knowledge while preserving language-specific nuances presents a fascinating question.",
        "With the development and application of MLLMs, researchers have found that MLLMs often suffer from serious moral and privacy risks, hindering their progress. For example, as shown in Figure\u00a07, an MLLM inadvertently generated toxic content during user interactions, especially in multilingual scenarios. This has prompted wide discussion from the community and raised concerns about the ethical implications of AI systems in public applications. Therefore, improving the safety of MLLMs is a promising research question.",
        "The main challenges for ensuring safe MLLMs are as follows: (1) lack of safety benchmarks: the lack of safe data in the current literature hampers relevant research. Consequently, acquiring a large-scale safety dataset to facilitate future studies has become a significant focus. (2) Removal of unsafe data: the multilingual data generated by MLLMs poses potential safety risks during training. Therefore, identifying and filtering out unsafe multilingual content is a critical issue.",
        "Due to the limited number of languages supported by current work, integrating new languages into existing MLLMs is a promising direction to explore. For example, consider a multilingual customer service chatbot powered by an MLLM. As shown in Figure\u00a07, if the chatbot performs well in English but refuses to provide service in other languages, such as Chinese or Russian, then this may lead to frustration and a sense of exclusion. Therefore, with the global expansion of businesses, it is definitely essential to continuously add new languages. To this end, Cui et\u00a0al. and Yang et\u00a0al. suggest adding languages through two-stage pretraining. Yong et\u00a0al. observe that adapter-based methods are more effective than continuous pretraining.",
        "This challenge encompasses two main aspects: (1) multiple-language extension: how to dynamically and effectively extend the language capabilities of MLLMs remains an interesting research question. (2) Original-language preservation: expanding the model to support additional languages often degrades its performance in previously supported languages. Therefore, ensuring that the addition of new language does not lead to the unintentional forgetting of previously learned ones is a major challenge.",
        "Since the improvement in the usability of MLLMs, a large amount of work has begun to further extend MLLMs into the visual modality, speech modality, video modality, and even other modalities. An illustrative case, shown in Figure\u00a07, is a recent MLLM that successfully combined image and text inputs for enhanced contextual understanding and reasoning, demonstrating the potential for richer interactions in applications such as educational tools and content creation.",
        "This field faces two main challenges: (1) complex reasoning exploration: current multi-modal MLLMs are limited to simple cross-modal and cross-lingual tasks, necessitating further exploration into complex reasoning. (2) Comprehensive benchmarks: the existing literature lacks comprehensive benchmarks, which hinders progress and proper evaluation in this evolving field.",
        "As shown in Figure\u00a07, while understanding the mechanics of multilingual alignment is crucial to ensure that these strategies are explainable and transparent, a significant issue remains: there is no theoretical foundation for the effectiveness of multilingual alignment. To address this, Tang et\u00a0al. identify language-specific neurons that significantly impact the performance of aligned languages through a white-box analysis of neural mechanisms. Furthermore, Wang et\u00a0al. propose a neural-mechanism-based method for estimating and predicting alignment performance during MLLM training.",
        "Research in this area faces two primary challenges: (1) multilingual interaction mechanism: current analyses predominantly focus on two-language interaction studies and lack a comprehensive model that explains the interplay and alignment among multiple languages. (2) Language-specific and language-independent capability interaction mechanism: enhancing language-specific features often compromises language-independent capabilities. Understanding this dynamic and fostering the mutual enhancement of these aspects is a vital direction for increasing interpretability.",
        "In this section, we discuss the deployment of MLLMs in real-world settings, with attention to computational costs and model updates. Although models like GPT-4o perform exceptionally, adapting them for real-world use, especially in under-resourced or on-device settings, presents several challenges.",
        "This field faces two primary challenges: (1) resource efficiency in deployment: while MLLMs support multiple languages, they require substantial computational resources, largely due to the size of word embedding layers, which are nearly as large as the model itself. Deploying these models on hardware-limited devices, such as mobile phones or edge devices, leads to inefficient memory use and slower inference times. Additionally, under-resourced languages encounter performance barriers due to limited datasets and computing infrastructure. (2) Update trade-offs between multilingual and monolingual models: regular\u00a0updates are essential to integrate new languages, data,\u00a0or continual optimizations. However, maintaining performance consistency across languages during updates is challenging. Fine-tuning and retraining, especially for low-resource languages, exacerbate this issue due to data scarcity. In some cases, hardware constraints further hinder large-scale updates.",
        "Low-resource languages are critical to global linguistic diversity, embodying the cultural and intellectual heritage of millions of speakers. Despite this, they are often neglected in LLM development due to limited data and computational resources. Addressing these challenges is essential not only to promote inclusivity in AI technologies but also to preserve linguistic diversity in an increasingly digital world. These languages face unique obstacles in MLLMs, extending beyond data scarcity to include unequal access to computational resources.",
        "The lack of data for low-resource languages causes significant performance disparities in MLLMs. Languages like Zulu, Swahili, and Tupi often perform poorly compared to high-resource languages such as English and French, affecting both accuracy and text fluency. Even in advanced MLLMs like GPT-4, high-resource languages generate excellent text, while low-resource languages exhibit grammar issues, logical issues, and even unsafe content due to both limited data quantity and quality.",
        "To address these gaps, data augmentation through synthetic data, along with few-shot or zero-shot learning, can significantly enhance the performance of low-resource languages. Techniques such as knowledge distillation, pretraining, SFT, RLHF, and even in-context learning enable models to generalize effectively from limited data, thereby enhancing text fluency and accuracy despite the challenges posed by data scarcity. Furthermore, Yoon et\u00a0al. and Xu et\u00a0al. introduce methods that incorporate a projection layer to bridge the multilingual encoder and English language models, thus improving generalization for low-resource languages. Nevertheless, despite the proliferation of various alignment methods, a substantial performance gap persists between high-resource and low-resource languages, necessitating further exploration in this area.",
        "Tokenization, which transforms text into processable tokens, incurs varying costs across languages. Morphologically complex languages, such as Arabic and Finnish, necessitate a greater number of tokens to convey the same meaning as English, resulting in inefficiencies, particularly in low-resource languages. For example, Arabic, owing to its morphological characteristics, such as prefixes and suffixes, frequently divides a single word into multiple tokens, whereas its English counterpart may require only one token. This disparity increases computational costs and diminishes fluency in Arabic text generation. Furthermore, in OpenAI\u2019s GPT-4, Arabic requires over three times as many tokens as English, leading to slower inference times and a reduction in output quality, which also illustrates the importance and urgency of fair tokenization.",
        "To address these inequities, researchers have developed novel tokenization methods aimed at reducing inefficiencies for low-resource languages. One such approach, semantic-based tokenization, considers contextual information to prevent unnecessary splits. Xue et\u00a0al. and Nicosia and Piccinno propose encoding all tokens using byte representations to achieve fairer tokenization. However, this method introduces increased computational demands when processing long contexts. Therefore, implementing tokenization methods that balance performance, efficiency, and fairness remains a critical issue for future research.",
        "Many low-resource languages in the Southern Hemisphere belong to communities that are geographically isolated and culturally distinct, leading to limited digitization and linguistic documentation.",
        "The infrastructure and resources necessary to collect, process, and annotate data for low-resource languages in the Southern Hemisphere are often lacking due to socioeconomic disparities. For instance, many regions in Africa, South America, and the Pacific Islands lack the technical infrastructure or funding required for large-scale linguistic projects. As a result, the creation of NLP datasets for languages like Sesotho, Xhosa, or M\u0101ori is slow and often relies on external funding or non-local research initiatives, which may not fully understand the linguistic or cultural nuances of these languages. Efforts to develop NLP resources for African languages such as Sesotho and Zulu have faced significant delays due to the high cost and logistical difficulties of data collection. Despite recent attempts to include more African languages in models like Masakhane, there remains a stark imbalance in the quality and availability of training data compared to languages in more developed regions.",
        "In this work, we present a comprehensive survey of advancements in MLLMs. Specifically, we propose a systematic taxonomy for MLLMs from an alignment perspective, offering a unified view for researchers to understand the progress of MLLMs. Additionally, we highlight emerging trends and frontiers along with their corresponding challenges in MLLMs. We hope this work facilitates research and inspires further breakthroughs in the MLLM literature.",
        "MLLM resources, including open-source software, diverse corpora, and a curated list of relevant publications, are accessible at https://multilingual-llm.net. All the papers about MLLMs can be found at https://github.com/LightChen233/Awesome-Multilingual-LLM."
    ],
    "title": "A survey of multilingual large language models"
}