{
    "content": [
        "In the rapidly evolving landscape of healthcare and drug development, the ability to efficiently collect, process, and analyze large volumes of real\u2010world data (RWD) is critical for advancing drug development. This article provides a blueprint for establishing an end\u2010to\u2010end data and analytics pipeline in a cloud\u2010based environment. The pipeline presented here includes four major components, including data ingestion, transformation, visualization, and analytics, each supported by a suite of Amazon Web Services (AWS) tools. The pipeline is exemplified through the CURE ID platform, a collaborative tool designed to capture and analyze real\u2010world, off\u2010label treatment administrations. By using services such as AWS Lambda, Amazon Relational Database Service (RDS), Amazon QuickSight, and Amazon SageMaker, the pipeline facilitates the ingestion of diverse data sources, the transformation of raw data into structured formats, the creation of interactive dashboards for data visualization, and the application of advanced machine learning models for data analytics. The described architecture not only supports the needs of the CURE ID platform, but also offers a scalable and adaptable framework that can be applied across various domains to enhance data\u2010driven decision making beyond drug repurposing.",
        "In today's data\u2010driven world, the need for efficient and scalable end\u2010to\u2010end data pipelines has become increasingly critical across various domains. In healthcare, the growing interest in efficiently collecting and analyzing large amounts of real\u2010world data (RWD) puts emphasis on the importance of such pipelines. This is particularly crucial in the context of drug development, where the integration of RWD into the drug development lifecycle can significantly enhance the discovery and development of new therapeutic agents. Unlike data from randomized controlled trials, RWD is inherently multi\u2010modal, drawing from diverse sources such as medical claims, patient registries, doctors' notes, and wearable devices. This diversity offers the unique advantage of capturing longer durations of the disease state and earlier stages of disease progression. Efficient data pipelines can reduce the time and effort required to process RWD, making it ready for advanced analyses, including artificial intelligence and machine learning applications, ultimately accelerating the generation of real\u2010world evidence (RWE) and enhancing the understanding of medical products in drug development across diverse patient populations. ",
        "Handling RWD poses certain challenges due to issues with data quality and integration from diverse sources, scalability with growing data volumes, and the need for real\u2010time processing. Without the use of robust resources, such as those that are cloud\u2010based, these challenges are further exacerbated. For organizations that do not utilize such resources, the alternative often involves a tedious process of managing data on\u2010premises, which can lead to several significant drawbacks, including increasing complexity in data management, higher costs associated with maintaining physical infrastructure, and the potential for inconsistent data collection practices across disparate data sources. The lack of a unified and scalable platform can also hinder real\u2010time data processing, which is essential for generating timely insights from RWD. Moreover, the process of sharing data between collaborators can be cumbersome, leading to a lack of participation in data\u2010sharing initiatives. An end\u2010to\u2010end data pipeline to handle RWD can address these challenges by ensuring data integrity, enabling seamless data aggregation, optimizing resource allocation for high\u2010volume data handling, and supporting real\u2010time analytics to deliver actionable insights. This not only streamlines the process but also encourages broader participation in data\u2010sharing initiatives, leading to more comprehensive and reliable real\u2010world evidence that can inform drug development.",
        "One notable use case demonstrating the power of such RWD pipelines is CURE ID (cure.ncats.io), a collaborative platform designed to capture novel uses of existing drugs. CURE ID is a web\u2010based tool (accessible via computers, smartphones, or mobile devices) that allows healthcare professionals and patients to document and share their real\u2010world treatment experiences through a simple online case report form. The primary goal of CURE ID is to accelerate drug repurposing research, especially in the context of challenging infectious diseases where there are limited treatment options. The platform has data spanning over 300 disease areas.",
        "Cloud\u2010based computing platforms can support a comprehensive suite of services that facilitate the creation of end\u2010to\u2010end data and analytics pipelines, tailored to meet diverse use case requirements. In this tutorial paper, we will delve into the intricacies of building an efficient and scalable data and analytics pipeline using AWS, specifically focusing on the CURE ID database as a use case. By leveraging AWS services, we demonstrate how to design a robust pipeline capable of ingesting, processing, and storing large volumes of data, ultimately enabling powerful data analytics and insights that can drive impactful medical research and decision making.",
        "In the following sections, we will describe four core components of an end\u2010to\u2010end data and analytics pipeline, including (i) data ingestion, (ii) data transformation, (iii) data dashboards and reports, and (iv) data analytics. This will include information on potential options to consider in each component, along with a walk\u2010through of a proposed architecture of an end\u2010to\u2010end pipeline with a use case for CURE ID (Figure\u00a01). Within the case reports used in the CURE ID application, data fields include both structured to unstructured types (Table\u00a01).",
        "Cloud computing platforms like AWS offer a range of services for data ingestion. For example, an AWS Lambda can be set up to regularly poll external applications, fetching data that can then be ingested into other AWS services, such as Amazon Simple Storage Service (S3) or Amazon Relational Database Service (RDS). To enhance this process, services like Amazon CloudWatch can be integrated to monitor and manage the performance of the polling Lambda, tracking metrics such as the number of records ingested, the size of the data, and the response time from the data source. When combined with RESTful Application Programming Interfaces (APIs), CloudWatch can further automate monitoring, allowing HTTP requests to efficiently access and ingest data from external sources. This setup enables external systems and users to interact seamlessly with the AWS environment. Additionally, CloudWatch can help monitor and manage Lambda costs, ensuring that the API polling process remains efficient and cost\u2010effective. AWS provides robust data encryption options and Identity and Access Management (IAM) roles to ensure that data is securely transmitted, stored, and accessed only by authorized users, maintaining the integrity and confidentiality of the data throughout the ingestion process. Similarly, data de\u2010identification methods, along with the removal of protected health information (PHI) and personally identifiable information (PII), can be applied to the data within the pipeline to ensure that any sensitive data is appropriately anonymized before storage or processing, if necessary, further enhancing data security.",
        "After data ingestion, choosing the appropriate storage solution is critical and depends on your application's needs. AWS offers multiple options, including the Amazon S3 and RDS solutions, along with other services like Amazon DynamoDB and Amazon Redshift. Amazon S3, a cost\u2010effective object storage service, is the go\u2010to for larger datasets, while Amazon RDS is preferred when structured data require complex transactions and queries. Amazon RDS can handle both structured and semi\u2010structured (i.e., JavaScript Object Notation (JSON)) data effectively. DynamoDB and Redshift can be used in specific cases where high performance and analytical capabilities are needed. Another consideration is the infrastructure, expertise, and existing tools within your organization. For instance, if your organization already relies heavily on relational databases and has expertise with them, Amazon RDS may be the best solution. Another vital component of data ingestion is ensuring data validation, which is essential for maintaining data quality and reliability. This involves first verifying that the data is in the expected format and schema, preventing downstream errors caused by incomplete data. Additional validation checks, such as detecting duplicates, verifying data ranges, and confirming data consistency across sources are crucial steps. These checks not only ensure that the data can be properly utilized by the target systems but also improve the overall accuracy and trustworthiness of the data pipeline. This component can be helped by the utilization of detailed case report forms (CRFs) as a template for data submission, creating consistency and limiting errors at the front end of the process.",
        "In our example use case with CURE ID, the data ingestion section of the pipeline in AWS begins with an API Polling Lambda, which is employed with a RESTful API to periodically retrieve data from the CURE ID database. Data input into the CURE ID application are entered in a detailed CRF, ensuring data consistency and limiting errors, and upon ingestion, the data are securely stored in raw JSON format in an AWS RDS PostgreSQL database for further processing. The API Polling Lambda ensures that the data are updated regularly and consistently, facilitating a robust and reliable data ingestion mechanism that supports the dynamic needs of the analytics pipeline.",
        "Once the data are ingested, AWS Step Functions are utilized to handle the processing of various fields within the case report forms. These Step Functions operate in a parallel manner, enabling the efficient and simultaneous processing of different data fields, which significantly reduces the overall processing time. This process results in data structured in a relational format that is stored and written back into the RDS PostgreSQL database. This parallel processing approach not only enhances performance but also ensures that the data are ready for complex queries and analytics tasks, providing a seamless integration into the broader data analytics workflow. Example tables that are generated through the utilization of the AWS Step Functions are shown in Figures\u00a02 and 3.",
        "When the data of interest includes unstructured text thought to contain relevant information, various natural language processing (NLP) tools can be employed to extract meaningful insights. In the context of CURE ID, identifying and narrowing down pertinent information from unstructured clinical notes and literature abstracts can be challenging. Topic modeling, an unsupervised NLP technique, addresses this by discovering latent topics across diverse documents, enabling the development of a preliminary data filtering mechanism that helps make sense of the unstructured text data along with highlighting relevant information for further analysis. This approach can be applied to the six different unstructured text fields within the CURE ID application. Methods such as Latent Dirichlet Allocation (LDA) and BERTopic are particularly effective in this regard, as we've shown previously. An example of output from topic modeling is shown in Table\u00a02.",
        "The relevant information discovered through topic modeling, including medical conditions and diagnoses, medications, and treatment procedures, can then be structured using a combination of Named Entity Recognition (NER) and Relation Extraction (RE). Within AWS, it is possible to implement these components using various open\u2010source libraries, such as the spaCy NLP pipeline. By fine\u2010tuning NER and RE components with labeled data (i.e., data annotated as a part of the supervised learning process), we can accurately identify entities and their relationships within text. This structured information is then combined with the structured data elements in the AWS RDS database, creating a comprehensive and integrated dataset. Leveraging large language models (LLMs) enhances this process by providing advanced capabilities in understanding and processing natural language. LLMs can be used to refine both the topic modeling and the NER and RE processes, ensuring that the extracted information is precise and contextually relevant. For example, an LLM can be used to summarize the abstract topic keywords obtained from a topic modeling algorithm (Table\u00a02). This combination of topic modeling and sophisticated NLP techniques facilitates the seamless integration of unstructured and structured data, enabling advanced data analytics and generating actionable insights for drug repurposing research. An example of the output of structuring data through the utilization of an LLM is shown in Table\u00a03.",
        "To ensure the validity of this data and analytics pipeline, there are several key validation methods that can be applied at various stages. First, if topic modeling is implemented, it can be validated by assessing metrics like topic coherence, which is a measure of the topics' interpretability and quality. Similarly, for the NER and RE components mentioned here, labeled validation datasets can be used to measure various forms of performance, including precision, recall, and F1 scores, verifying that entities and their relationships are accurately identified within the text. In general, when considering the validation of NLP methods, assessing the performance of the various NLP tasks is vitally important.",
        "The third component in our pipeline is data visualization. AWS offers a suite of services tailored to various data visualization needs, each integrating seamlessly with the broader AWS ecosystem. These include, but are not limited to, Amazon QuickSight, Amazon Managed Grafana, and Amazon OpenSearch. AWS QuickSight is a powerful, user\u2010friendly interface with a flexible pricing model that allows users to create and share interactive dashboards and visualizations. When paired with the in\u2010memory store, known as QuickSight, Super\u2010face Parallel In\u2010memory Calculation Engine (SPICE), the power of the QuickSight service is increased significantly and the performance of data visualization is enhanced by storing data in memory and leveraging parallel processing. On the other hand, Amazon Managed Grafana and Amazon OpenSearch are tailored for operational monitoring and real\u2010time data visualization, particularly for time\u2010series data, offering deep customizability and extensive integration with AWS services like CloudWatch. For more advanced users requiring flexibility and open\u2010source customizability, Amazon Managed Service for Apache Superset provides a robust platform for creating detailed and tailored visualizations, while AWS Glue DataBrew focuses on data preparation with basic visualization capabilities to help users clean and explore their data before deeper analysis.",
        "For the specific use case of CURE ID, which involves ingesting and analyzing clinical data to track and visualize trends in real time, we decided to use Amazon QuickSight. QuickSight was selected for its ease of use, cost\u2010effectiveness, and seamless integration with the broader AWS services we are leveraging for data ingestion, storage, and processing. Specifically, QuickSight SPICE was selected, to improve the performance of the dashboards. Data from the AWS RDS PostgreSQL can be visualized through intuitive and customizable dashboards that can display key metrics and trends identified from the CURE ID data, such as the prevalence of certain drug uses, emerging treatment patterns, and outcomes across various diseases. The ability to drill down into the data, filter by specific parameters, and view real\u2010time updates ensures that healthcare professionals and researchers have access to the most relevant and up\u2010to\u2010date information, which is why we selected this service for the end\u2010to\u2010end data and analytics pipeline. An example of various dashboard reports is shown in Figure\u00a04.",
        "For the final component of an end\u2010to\u2010end data and analytics pipeline, AWS provides a range of services for machine learning and analytics. Amazon SageMaker, for instance, offers an integrated development environment that supports the entire machine learning lifecycle, including model training, tuning, and deployment. It also comes with pre\u2010constructed algorithms and the ability to deploy models in a scalable and secure environment. Alongside SageMaker, AWS provides other specialized services, including but not limited to (i) Amazon Polly, which converts text into lifelike speech, making it useful for applications in accessibility and automated customer service; (ii) Amazon Transcribe, which automatically converts speech to text, allowing for the processing of audio data; (iii) Amazon Textract, which facilitates the extraction of text and data from scanned documents, an essential tasks for digitizing and analyzing paper or PDF\u2010based records; (iv) Amazon Kendra, which provides intelligent search capabilities, enabling users to query enterprise data efficiently and find relevant information across datasets; (v) Amazon Forecast, which is a time\u2010series forecasting service that uses machine learning in areas like demand forecasting, financial planning, and supply chain management; and finally, (vi) Amazon Elastic MapReduce (EMR), which is a managed service that makes it easy to process a vast amount of data using frameworks like Apache Spark, which is particularly useful for big data analytics and machine learning at scale.",
        "In the described use case of an end\u2010to\u2010end data and analytics pipeline for the CURE ID database, we chose to use Amazon SageMaker due to its comprehensive abilities and seamless integration with the other AWS services, reducing the complexity associated with managing multiple tools and environments. For instance, in the context of performing clustering analysis on data from the CURE ID app, we can leverage a Kmodes algorithm, a clustering method designed to handle categorical data, in SageMaker to group similar data points based on features like patient demographics, treatment outcomes, and clinical observations. Here, the transformed data in the RDS PostgreSQL database is fed into SageMaker, where the Kmodes algorithm clusters the data into meaningful groups (Figure\u00a05). These clusters can then be analyzed to identify patterns, such as common treatment responses or patient subgroups, which can provide valuable insights for healthcare professionals.",
        "By leveraging various resources within the AWS ecosystem, including advanced analytics and machine learning models, it is possible to transform raw data into valuable insights. This can drive drug repurposing research and improve patient outcomes. Beyond drug repurposing, such a pipeline could be beneficial in areas such as digital health technologies (DHTs), along with clinical trial data. An end\u2010to\u2010end data and analytics pipeline similar to what is presented here can enable a deeper understanding of complex datasets, leading to the development, deployment, and monitoring of evidence generated from big data in a real\u2010world setting through data ingestion, transformation, visualization, and analytics components."
    ],
    "title": "Real\u2010world evidence in the cloud: Tutorial on developing an end\u2010to\u2010end data and analytics pipeline using Amazon Web Services resources"
}