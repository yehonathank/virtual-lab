{
    "content": [
        "The biomedical literature is growing rapidly, and it is increasingly important to extract meaningful information from the vast amount of literature. Biomedical named entity recognition (BioNER) is one of the key and fundamental tasks in biomedical text mining. It also acts as a primitive step for many downstream applications such as relation extraction and knowledge base completion. Therefore, the accurate identification of entities in biomedical literature has certain research value. However, this task is challenging due to the insufficiency of sequence labeling and the lack of large-scale labeled training data and domain knowledge.",
        "In this paper, we use a novel word-pair classification method, design a simple attention mechanism and propose a novel architecture to solve the research difficulties of BioNER more efficiently without leveraging any external knowledge. Specifically, we break down the limitations of sequence labeling-based approaches by predicting the relationship between word pairs. Based on this, we enhance the pre-trained model BioBERT, through the proposed prefix and attention map dscrimination fusion guided attention and propose the E-BioBERT. Our proposed attention differentiates the distribution of different heads in different layers in the BioBERT, which enriches the diversity of self-attention. Our model is superior to state-of-the-art compared models on five available datasets: BC4CHEMD, BC2GM, BC5CDR-Disease, BC5CDR-Chem, and NCBI-Disease, achieving F1-score of 92.55%, 85.45%, 87.53%, 94.16% and 90.55%, respectively.",
        "Compared with many previous various models, our method does not require additional training datasets, external knowledge, and complex training process. The experimental results on five BioNER benchmark datasets demonstrate that our model is better at mining semantic information, alleviating the problem of label inconsistency, and has higher entity recognition ability. More importantly, we analyze and demonstrate the effectiveness of our proposed attention.",
        "As the number of biomedical articles and resources increases, searching and extracting valuable information becomes challenging. Researchers consider a variety of information sources to transform unstructured textual data into refined knowledge to improve research efficiency. Manual annotation and feature generation by biomedical experts are inefficient because they involve complex processes. Therefore, deep learning (DL) and natural language processing (NLP) are particularly important for biomedical text mining and computational data analysis. Valuable information such as relationships between objects requires us to identify meaningful terms from the text. A meaningful term or phrase in a domain that can be distinguished from similar objects is called a named entity (NE). Named entity recognition (NER) has become a mature technology in mining medical text terms because it is one fundamental task for natural language processing, which aims to recognize named entities (NEs), such as person, location, disease from raw text and classify them into pre-defined categories. Over the past few decades, NER has attracted a great deal of attention owing to its importance in downstream tasks such as entity linking, question answering, and relationship extraction. In the biomedical field, biomedical named entity recognition (BioNER) also acts as a fundamental task in biomedical text mining that aims to automatically recognize and classify biomedical entities (e.g. genes, proteins, chemicals and diseases) from biomedical text. Although medical NER is a fundamental upstream task, many difficulties remain. That\u2019s because most of the medical literature is disorganized. Medical texts contain some special features, such as the publication of a large volume of medically relevant disease terminology (such as \u201cadenomatous polyposis coli \u201d), the publication of some chemicals in letters and numbers (such as \u201cCD-832 \u201d), a number of medical professional abbreviations (such as \u201cSYN\u201d), and BioNEs constantly increase with new discoveries (e.g. COVID-19 is new.). The specificity of medical texts increases the difficulty of treating NER as a sequence labeling problem. Besides, unlike those public domain named entity recognition tasks, BioNER is more challenging due to the naming complexity, lack of large-scale labeled training data, domain knowledge, data privacy, and some ethical concerns. These various factors bring limitations and challenges to solving BioNER. With the development of machine learning, some researchers have traditionally used a variety of natural language processing tools and domain knowledge to solve BioNER problems through well-designed feature engineering. Since feature engineering relies on models and domain-specific knowledge, there has been a lot of research on BioNER over the past few decades, ranging from traditional feature-based approaches to recent deep learning-based neural approaches.",
        "We first use a word-pair relation classification to solve the difficulty of sequence labeling for BioNER tasks.",
        "We design an attention mechanism guided by fused prefix and attention map discrimination to enhance the BioBERT. Our proposed attention can be easily integrated to Transformer-based PLMs, which allows initialization from PLMs without introducing any new parameters, and only affects fine-tuning of standard model parameters.",
        "We evaluate the proposed model on five BioNER datasets to demonstrate its generality.",
        "In recent years, BioNER methods based on DL and NLP have attracted more and more attention due to their excellent performance because deep learning-based approaches typically do not require manually labeling features. It automatically learns useful features from the sentences. Furthermore, advances in deep learning techniques used in NLP have enabled advances in biomedical text mining models. In the NLP field, a deep learning-based approach transforms text into embeddings and then extracts useful features from these embeddings for biomedical entity recognition. So choosing a suitable feature encoder has always been the most important step in NLP. From 2017 to 2022, the research on BioNER is roughly divided into the following several categories, methods based on various neural networks, pre-trained models, external knowledge, and multi-task learning. For example, some studies use neural network models to generate high-quality features have become prevalent in solving BioNER tasks. The feature extractors of neural networks are usually convolutional neural network (CNN), long short term memory networks (LSTM), bi-directional LSTM (BiLSTM), or a combination of various neural networks. Machine learning-based conditional random field (CRF) is often used as a classifier in conjunction with these feature extractors. Considering the correlation between neighboring labels, CRF can obtain the global optimal label chain for a given sequence. For instance, BiLSTM-CRF is the most common architecture for BioNER using the deep learning method. Since 2018, large-scale pre-trained language models (PLMs) are proved to be effective in many NLP tasks. Integrating or fine-tuning pre-trained language model embeddings for BioNER is becoming a new paradigm. Pre-trained models like BERT show the effectiveness of the paradigm of first pre-trained an language model on the unlabeled text then fine-tuning the model on the down-stream NLP tasks. Therefore, Lee et al. proposed a variant of BERT, namely, BioBERT, for the biomedical domain, which is pre-trained on large raw biomedical corpora and achieves state-of-the-art performance in BioNER. It was hard to beat the performance of BioBERT until recently when someone tries to use external knowledge and multi-task learning to improve the performance of BioNER. The recent SoTA model on BioNER in some datasets using multi-task methods are proposed by Tong et al. and Chai et al.. Tong et al. try to combine BioBERT and multi-task by designing three auxiliary classification tasks and one main BioNER task to explore multi-granularity information in the dataset. The multiple loss functions of multiple tasks are jointly trained by assigning different fixed weight coefficients. Their multi-task model is hard parameter sharing. Different from them, Chai et al. select 14 datasets containing 4 types of entities for training and evaluate the model on specific task, which realizes the multi-level information fusion between the underlying entity features and the upper data features. Different from the above models, Tian et al. utilize additional syntactic knowledge to enhance BioNER for the first time. However, these methods have major disadvantages. For example, although multi-task learning is an effective approach to guide the language model to learn task-specific knowledge, the relationships between different BioNER tasks are often difficult to consider comprehensively due to differences among different datasets. Besides, multi-task learning is not conducive to model training because loss between different tasks may conflict, resulting in mutual consumption cancellation, or even negative transfer phenomenon, which makes us hard to balance the joint training process of all tasks. As for the methods leveraging additional knowledge, the disadvantages are also obvious: (1) acquiring external knowledge is labor-intensive (e.g., knowledge base) or computationally costly (e.g., dependency); (2) Integrating external knowledge adversely affects end-to-end learning and compromises the generality of DL-based systems. Although some external syntactic information is easier to obtain through off-the-shelf NLP toolkits like spaCy or Stanford CoreNLP Toolkits, the text structure of BioNER is usually complex, and it is difficult to integrate general syntactic structure information into multiple BioNER datasets. Finally, the above methods are all based on sequence labeling which means the label of each word is predicted independently based on a context-dependent representation, regardless of its neighbors. We believe that ignoring the neighbors around entity words in sequence labeling will weaken the recognition ability of special medical words. And the complexity of biomedical terminology brings challenges and difficulties to sequence labeling. Different from all the above methods, we use a new prediction mode, namely word-pair relation classification, instead of the sequence tagging-based mode. We will introduce the differences and advantages between our novel approach and sequence tagging in Related work and Method sections. And we enhance the pre-trained BioBERT model with the proposed attention in a way that does not require additional knowledge to improve the performance of recognizing complex medical terms. To summarize, this paper makes the following contributions:",
        "Sequence labeling (also known as sequence tagging) is to input a string and output the sequence corresponding to each character in the string. Complete word segmentation through sequence tagging, that is, mark a character, whether it is the beginning, end, or middle part of a word. Sequence labeling has long been used to model and solve NLP tasks, including the BioNER tasks. Sequence labeling is a relatively simple NLP task, but it can also be the most basic task since it covers a wide range of characters, which can solve a series of problems in character classification, such as word segmentation, part-of-speech tagging, named entity recognition, relation extraction and so on. In this method, we need training a sequence labeling model by assigning and designing a label with some tagging schemes for each token in a given sequence. However, sequence labeling methods have many disadvantages in solving the BioNER tasks. First, designing a general labeling scheme for all BioNER subtasks is difficult and labor-intensive. Data annotation plays a crucial role in establishing benchmarks and ensuring that the correct information is used to learn BioNER models. Getting accurate labels requires not only time but also expertise. However, labelling errors are almost unavoidable, and wrong labels can lead to label inconsistencies between subsets of the labeled data (e.g., training and test sets, or multiple training subsets). Different tagging schemes such as BIO (i.e., B-begin, I-inside, O-outside) or BIOES (i.e., B-begin, I-inside, O-outside, E-end, S-single) usually have a great impact on the performance of the model. Second, for sequence labeling methods, it is difficult to capture the relationship between entity tokens because labels are usually independent and unrelated. Therefore, modeling between entity words has always been the bottleneck of BioNER. For example, \u201cCD-832\u201d is an important and indivisible terminology of chemical entity. However, in the model based on sequence tagging, even if BioBERT model is used, it may only recognize several entity words in these three words, and even recognize three words in the form of \u201cBIB\u201d (In other words, the label consistency problem still remains.). As a result, the model finally mistakenly divides this complete chemical entity word into two biomedical concepts, namely \u201cCD-\u201d and \u201c832\u201d. This is incorrect and unfriendly to biomedical literature mining. At last, entity sparsity problem often exists in the BioNER because sentences in BioNER datasets are too long but contain few entity words. Sequence tagging-based models such as BioBERT+Softmax often cause the classifier to classify entities into too many pre-defined categories, which will lower the probability accuracy of the model. And traditional sequence labeling methods require a specific-label output layer based on PLMs, which is not conducive to generalization. Sequence labeling methods can not better solve BioNER tasks, which provides research interest and value for our efforts to use a more efficient prediction pattern to solve BioNER related tasks.",
        "Besides, in the last 3 years, Transformer-based pre-trained models such as BERT is designed to pre-train language representations on large-scale unlabeled datasets, which has achieved remarkable success on many biomedical text mining tasks. Unlike traditional word embeddings such as Word2Vec and GloVe, pre-trained models can capture the meaning of words in different contexts. As we all know, a pre-trained language model like BERT is normally a large-scale and powerful neural network trained with huge amounts of data samples and computing resources. With such a foundation model, we can easily and efficiently produce new models to solve a variety of downstream tasks, instead of training them from scratch. PLMs\u2019 key feature is the self-attention mechanism. Relying on this attention mechanism, Transfomer can contextualize the input and provides an alternative to conventionally used recurrent neural networks (RNN). However, although Transformer has performed remarkably well, standing on the multi-headed dot-product attention which fully takes into account the global contextualized information, there are still many problems with this attention mechanism. For example, people find that these language models exhibit simple attention patterns. For example, Kovaleva et al. find that 40% of heads in a pre-trained BERT model simply pay attention to the delimiters, such as \u2018[CLS]\u2019 and/or \u2018[SEP]\u2019. This means that different heads of different self-attention layers of BERT always exhibit limited and redundant attention patterns. Michel et al. find that a large percentage of attention heads can be removed at test time without significantly impacting performance. Experiments of Raganato et al. conclude that many patterns learned from the encoder\u2019s attention head just reflect contextual location information. Some studies have also found that skimming irrelevant parts or tokens in the input sequence and keeping the hidden state unchanged can speed up the inference of the Transformer will remove the partial redundancy of the Transformer. To deal with these problems, many studies are devoted to changing the distribution of attention heads by the following three research strategies. The first research strategy focuses on the interpretation of networks, namely analyzing attention mechanisms and interpretability of weights and connections or try to to guide the attention mechanism by external information. The second research strategy is that some researchers have explored how to improve the inference efficiency of Transformer by pruning attention heads during inference due to excessive redundancy of the information learned among multiple heads of the Transformer, resulting in excessive network parameters. The last research strategy is some researchers make the Transformer focus on the local area by modifying the attention formula and explore the opportunity on the dynamic reduction of input sequence length to reduce time complexity of Transformer. Our attention in this paper belongs to the first research line, which is an guided attention but we don\u2019t need additional information. Although the Transformer\u2019s self-attention head learns partially redundant information and unimportant patterns, this will vary from dataset to dataset. It is difficult for us to judge which information learned by the head is useful for different datasets. We want to guide self-attention learning in the global attention map without modifying the self-attention computation and introducing external information. Meanwhile, notably, although the paradigm of pre-training and fine-tuning has achieved remarkable results on many NLP tasks, this paradigm can not better stimulate the knowledgeability of the pre-trained models. Some recent efforts on probing knowledge of PLMs show that, by writing some natural language prompts (also known as templates), we can induce PLMs to complete factual knowledge. For example, GPT-3 further utilizes the information provided by prompts to conduct few-shot learning and achieves awesome results. Then, prompt learning has been introduced. The operation of the prompt is different from the previous fine-tuning based on the PLMs paradigm. In prompt learning, especially for text classification, downstream tasks are formalized as equivalent cloze-style tasks, and PLMs are asked to handle these cloze-style tasks instead of original downstream tasks. For example, compared with conventional fine-tuning methods, prompt learning needs to reconstruct the input data through the template, so that the predicted content is also embedded in the input data, and the mask language model-like (MLM) method can be used to learn the label information. Prompt learning has two types of prompts, namely discrete prompt and continuous prompt (also known as prefix). Prompt learning has been proven to have good results on some simple NLP tasks, including text classification, natural language inference, and so on. But unfortunately, prompt learning may perform poorly compared to fine-tuning on several hard sequence tasks such as information extraction and sequence tagging. Because the template-based prompt method needs to iterate over all spans, the complexity is very high. Later, Liu and Li et al. proposed prompt tuning, an idea of tuning only the continuous prompts. They try to apply continuous prefix for every layer of the pre-trained model. In other words, prefix tuning prepends a sequence of continuous task-specific vectors to the input. This is also a great inspiration for our work.",
        "In this work, instead of treating the BioNER task as a sequence labeling problem, we formulate it as a word pairs relation classification problem. To the best of our knowledge, there is currently no specific research for BioNER by using this research mode. Furthermore, we are the first to explore enhancing PLMs based on this new research mode in BioNER. And we believe that generating continuous prompts can provide certain guided semantic information for word-pair representation on the BioNER datasets because word-pair relationship classification can be seen as a dimensionality reduction operation for sequence labeling. We are committed to designing a more diverse attention mechanism based on prompt tuning, which can make the representation of the same head as similar as possible while the distribution of different heads is as diverse as possible. In this way, the probability of entity words being noticed will increase. This kind of attention can enrich the diversity of multi-head attention at different layers of PLMs without introducing external knowledge, syntax trees and modifying the self-attention. We design this attention as a unified auxiliary task, which can be applied to any efficient model (This is our future work.). Therefore, we propose the prefix and attention map discrimination fusion guided attention (PAMDFGA). As far as we know, in the BioNER\u00a0filed, no researchers have done similar prompt and attention guiding research. Our work is the first to guide the distribution of pre-trained models\u00a0by using the prompt to solve the BioNER problem. The following section will introduce how PAMDFGA guides our model in detail.",
        "Formally, for a sequence labeling task, given a sequence of tokens   , ,\u00a0...\u00a0,  , PLMs are to output a list of tuples  , , t . Here,   [1, N] and   [1, N] are the start and the end indexes of a named entity mention. t is the entity type from a pre-defined category set. However, in our model, we do not use this prediction mode. This method cannot better mine the entity information of biomedical text, so we explore a model that can strengthen the attention of biomedical entities. Inspired by Li et al, our task is to predict the relationship between biomedical word pairs. Specifically, we design two pointer-like word-pair representations for BioNER datasets, namely Next-Neighboring-Word (NNW) and Tail-Head-Word (THW) for BioNER. The NNW relation addresses entity word identification, indicating if two argument words are adjacent in an entity, while the THW relation accounts for entity boundary and type detection, revealing if two argument words are the tail and head boundaries respectively. We give an example as demonstrated in Fig.\u00a01 for a better understanding. Our task aims to extract the relations , between each word pairs (), where  is pre-defined, including None, NNW, and THW- (\u201c\u201d represents the type of the entity.). As shown in Fig.\u00a01, \u201cCD-832\u201d is a complete entity of chemical. This whole entity includes two relations NNW (CD-, and -832 ) and THW-C (832CD). If there is no relationship between word pairs, we set it to None. Therefore, a 2-dimensional grid for word pairs is constructed in Fig.\u00a01. If an entity such as \u201ccalcium\u201d has only one word we set it to THW-C. To avoid the sparsity of relation instances, NNW and THW-C relations are tagged in the upper and lower triangular regions. Our model needs to predict the relation between all word pairs and finally decode it. Through this method, we can better capture the semantic relationship between adjacent entities. With this constructed grid, we don\u2019t have to design a label for each word.",
        "In this section, we will present the overall model architecture proposed in our method. The architecture of our framework is illustrated in Fig.\u00a02. It mainly consists of three components. First, the enhanced BioBERT (E-BioBERT), and widely-used bi-directional LSTM are used as the encoder to yield contextualized word representations from input sentences. Then a simple convolution layer is used to build and refine the representation of the word-pair grid for later word-word relation classification. Afterward, a multi-layer perceptron is leveraged for reasoning the relations between all word pairs.",
        "Answer engineering has a strong impact on the performance of prompt learning. As for entity class prediction in BioNER, adding additional label-specific parameters representing different entity types hinders the applicability of prompt learning. As shown in Fig.\u00a03, we use the prefix tuning to tune the attention weights of BioBERT. This approach eliminates the need for a verbalizer and becomes a fully generative model that outputs a token-level class at each token position. Prompts in different layers are added as prefix tokens in the input sequence and are independent from other layers (rather than computed by previous transformer layers). Inspired by Chen et al. and Li et al.\u00a0, we add a set of trainable embedding matrices  to each layer of BioBERT, where l is the layer number of BioBERT and  (P is the length of the prompt and d represents the dimension of the hidden layer of the encoder). The prefix of each layer participates in the calculation of self-attention. That is, unlike methods that place templates in the original input sequence, we incorporate continuous prompts into the self-attention layer and utilize these prefixes to guide attention allocation, which is sufficiently flexible and lightweight. Specifically, we inherit the structure of the Transformer, as a specific component, we introduce the prefix-guided attention layer over the original layer queries, keys and values (, , and ) to achieve more guided attention effect. As we all know, Transformer use stacked self-attentions to encode contextual information for input tokens. The calculation of self-attention depends on the following components of Q, K and V, which are projected from the hidden vectors of the previous layer. Then the attention output A of one head is computed as follows:where d is the dimension of keys. Within the standard self-attention layer, global attention mechanism is employed that each token provides information to other tokens in the input sentence. A key feature of the Transformer architecture is the so-called multi-head attention mechanism, which allows the model to focus simultaneously on different parts of the input. Furthermore, Transformer rely on multi-head self-attention to capture dependencies between tokens. Given a hidden state H (input of initialized BioBERT), multi-head self-attention first projects it linearly into queries , keys , and values  using parameter matrices ,  and  respectively. The formulation is as follows:Then, we introduce the attention mechanism after the prefix to redefine the self-attention mechanism of  as follows:where the self-attention distribution (attention weight)  is computed via scaled dot-product of  and  by Eq.\u00a03. These weights are assigned to the corresponding value vectors  to obtain output states :. Finally, the output states  of all heads are concatenated to produce the final states. To allow the different attention heads to interact with each other, Transformer applies a non-linear feed-forward network over the multi-head attention\u2019s output at each Transformer layer. However, even with prefix-guided attention, for the BioNER task, we still find that our attention mechanism has redundant attention patterns and insufficient attention to entities. In view of the shortage of Transformer, we propose the PAMDFGA. Inspired by the instance discrimination learning proposed by Wu et al., take BioBERT\u2019s twelve layers and twelve heads as an example, we treat each head in BioBERT as an instance and match different heads at different layers to maximize the difference between different heads. This will make our model get more information from the input text from different aspects and perspectives. We want to learn a good feature representation for each instance (head), which requires the semantic information\u00a0learned between different heads is as different as possible. Instance discrimination learning can implicitly group similar instances together in the representation space without any explicit learning force directs to do so. Our designed attention discrimination model is shown in Fig.\u00a04. The specific construction process of our designed attention PAMDFGA is as follows: we first obtain attention weights from different heads and layers of prefix-guided BioBERT. Our proposed attention mechanism is based on the whole attention map. This is expressed as follows:",
        "where  is BioBERT\u2019s multi-head attention map. l and h denote the layer number and head number in each layer, respectively. Each attention map   , where L is the maximum sentence length in each batch and P is the length of random initial prefix.  represent the input token and  is the trainable parameters of the BioBERT model which is fine-tuned during model training. Then we stack twelve layers of attention map and perform an average pooling operation on the  by summing up the attention values that all original (o) input length tokens received and all original input tokens with prefix () received. Then, the corresponding formula of transforming the attention map  to the attention vector  and  via the following:where i represents the i-th attention map and j is the column index of  of the attention map.    and   . Then, we rebuild\u00a0the entire attention map as follows:where  denotes the concatenate operation.    and    represents the attention matrix. Finally, we push the diversity of attention maps via the idea of instance discrimination. We treat each attention head as a distinct class of its own and train final category results of each head are different, so that the characteristics of each head are different, which means that the information of each head is different. The probability of one attention map  and  being assigned into the i-th class can be computed as follows:where  measures how well o matches the i-th class because  is regarded as the weight of j-th class.  is a temperature parameter that controls the concentration of the distribution, which is necessary for tuning the concentration of  on our unit sphere and we enforce  and  to 1 via a L2-normalization layer. The objective of the auxiliary task is to maximize the joint probability  and  or equivalently to minimize the negative log-likelihood over the training set\u00a0, asAs such, the training objective of our PAMDFGA is revised as:where  fuse the information of the prefix and the original attention weight. We use  as the auxiliary loss of main task .",
        "Combined with our attention guidance mechanism, at the beginning of Fig.\u00a03, using BioBERT, we add a special symbol token, i.e. \u2019[CLS]\u2019, in front of each input sample. By concatenating with both position embeddings and segmentation embeddings, the token embeddings were fed into the E-BioBERT model to get the output representation   . Formally, given the input tokens, the label-specific encoder calculates:where  is the trainable parameters of the E-BioBERT model, which is fine-tuned during training.  is the special token \u2019[CLS]\u2019 and  = 768 is the dimensionality of the local representation. Besides, we use the bi-directional LSTM to yield contextual word representation from input embedding. The contextualized sentence-level representation  are used as the input embeddings of bi-directional LSTM layer, denoted as,where  is the corresponding trainable parameters of the BiLSTM model.   , where  denotes the dimension of a word representation.  are the hidden layer state sequence of BiLSTM.",
        "The second part of the model is the convolutional layer. Since CNNs are naturally suitable for 2-dimensional convolution on the grid, and also show the very prominence on handling relation determination jobs. We use a convolution module to capture grid information. Our convolution layer includes three modules, including a condition layer with normalization (CLN) for generating the representation of the word-pair grid, a hybrid sentence grid representation build-up to enrich the representation of the word-pair grid, and a single-layer dilated convolution for capturing the interactions between close and distant words. Specifically, we follow prior work and use a conditional layer with normalization for generating the representation of the word-pair grid. Then, we combine the enhanced word pair representations from CLN with randomly initialized distance and region embeddings to augment sentence representations. In the third part of the convolutional layer, we just use a simple and single-layer dilated convolutional neural network to capture the interaction information between different word pairs. The specific module information of the convolutional layer is as follows.",
        " Conditional layer normalization ",
        "The idea of conditional layer normalization comes from the idea of popular conditional generative adversarial networks (GAN) in image field - conditional batch normalization (CBN). That means a conditional vector is introduced as external contextual information to generate the gain parameter and bias of the well known layer normalization (LN) mechanism. In our BioNER framework, since we need to to predict the final relations between word pairs by generating grid representations of the word-pair grid, which can be regarded as a 3-dimensional matrix,   , where  denotes the representation of the word pair  and N is the number of tokens in each batch. Because both NNW and THW relations are directional, the representation  of the word pair  can be considered as a combination of the representation  of  and  of , where the combination should imply that  is conditioned on . We adopt the CLN to calculate :where  is the condition to generate the gain parameter  and bias  of layer normalization.  and  are the mean and standard deviation across the elements of , denoted as:where  denotes the k-th dimension of .",
        " Hybrid sentence representation ",
        "Building a grid representation of word pairs is a key step in our word pair classification. To further enhance sentence representations from E-BioBERT and conditional layer normalization, the distance embeddings (D) and region embeddings (R) are leveraged to better represent the positional information of word pairs in the grid. After we get the 3-dimensional vector    encoded by the BiLSTM encoder and E-BioBERT, we concat the word embedding (), distance embedding (), and region embedding () together.  and  are also 3-dimensional vectors, where    and   . Finally, we concatenate these three vectors to enhance the region and distance information of the hybrid sentence representation of grid   . The overall process can be formulated as:where MLP is a multi-layer perception to reduce their dimensions and  represents concatenation operations.",
        " Convolutional neural network ",
        "Convolutional neural network is generally used in the field of computer vision for tasks such as image classification and detection. The core idea of CNN is to capture local features. For text, local features are sliding windows composed of several words, similar to N-gram. The advantage of CNN is that it can automatically combine and filter N-gram features to obtain semantic information at different levels of abstraction. This is beneficial for enriching the semantic information of . We use a single layer dilated convolutional neural network (SDConv) to capture the interactions between word pairs, denoted as:where S   and  is the GELU activation function.",
        "Our model mainly predicts the relationship of word pairs, that is, the probability that a directed graph edge belongs to a category. The vector  from SDConv represents the grid information of word pairs, and we use the MLP to calculate two separate relations scores of word pair (, ) and use the Softmax function to calculate the final relation probabilities, using ,where    is the scores of the relations pre-defined in . Finally, for the , our BioNER training target is to minimize the negative log-likelihood losses with regards to the corresponding gold labels, formalized as:where N is the number of words in the sentence,  is the binary vector that denotes the gold relation labels for the word pair (,), and  are the predicted probability vector. r indicates the r-th relation of the pre-defined relation set . As such, our total training target is to minimize the loss of BioNER and the loss of PAMDFGA, formalized as:where  is defined in Eq.\u00a010 and  can be seen as a regularization loss, which are regulated using , and this term works like L2 term which does not introduce any new parameters and only influence the fine-tuning of the standard model parameters.",
        "The five BioNER datasets used in our framework are all flat NER. For the word-pair relationship scores predicted by the framework, we decode our predictions as a directional graph. The decoding object is to find certain paths from one word to another word in the graph using NNW relations. THW is used to determine the boundaries and type of entities, especially for sentences without entities, our THW is empty, and we do not judge which category it belongs to. Specifically, the relationships  of all the word pairs serve as the inputs. The decoding object is to find all the entity word index sequences with their corresponding categories. First, since our dataset has no nested examples, in the lower triangle part of Fig.\u00a01, we can decode them out just using THW-. For multiple consecutive entities, we construct a graph to, in which nodes are words and edges are NNW relations. Then we use the deep first search algorithm to find all the paths from the head word to the tail word, which are the word index sequences of corresponding entities.",
        "We evaluate our model on five public and available datasets containing various biomedical entities: BC4CHEMD, BC5CDR (including two sub-datasets, BC5CDR-Disease and BC5CDR-Chem), NCBI-Disease, BC2GM and, all of which are pre-processed and provided by previos SoTA work. Table\u00a01 summarizes these datasets. Among them, BC4CHEMD has the most sentences and entities and NCBI-Disease has the least datasets. As the same with previous work, we merged the train and development sets, made the same data split, and evaluated our model on the test set for a fair comparison. We follow prior SoTA works, and adopt standard entity-level F1-score as evaluation metrics to measure the performance of the trained model. Specifically, a predicted entity is counted as true-positive if its token sequence and type match those of a gold entity. The corresponding metrics are Precision (), Recall (), and F1-score (), where .",
        "The BioBERTv1.1 (+PubMed, Cased) model was used, containing 12 layers of Transformers with a hidden size of 768. The dimensionality of the hidden state  in BiLSTM is 512, the channel size of the convolutional layer  is set to 128 and the size of distance embedding and region embedding is initialized to 20. All datasets are trained with the batch size of 8 except BC4CHEMD, which has a batch size of 4. We use the AdamW optimizer with a learning rate 1e-3 for all datasets. We select the sentence length of the largest sample in each batch for training. A linear learning rate decay schedule with warm-up over 0.1, and a weight decay of 0.1 applied to every epochs of the training. The  in Eq.\u00a019 are selected from the set {0.1, 0.01, 0.001, 0.0001} according to grid search. The temperature parameter is set to 2.0. On all the datasets, each experiment is repeated five times. We report the maximum F1-score (referred to \u201cMax\u201d), average F1-score (referred to \u201cMean\u201d), and standard deviation (referred to \u201cStd\u201d). Table\u00a02 demonstrates our work. The proposed attention guiding mechanism acts on all attention heads of BioBERT. The best results in all our datasets are obtained based on integrating PAMDFGA into the last four layers of BioBERT. The best training procedure contains 6 epochs for BC4CHEMD, 10 epochs for BC2GM, 41 epochs for NCBI-Disease, 34 epochs for BC5CDR-Disease, and 47 epochs for BC5CDR-Chem. All our ablation study and case study are performed under the same parameters and epochs. Because the model training is not complicated, we do not freeze the parameters of BioBERT. Finally, all models are trained on NVIDIA RTX 3090.",
        "We compare our model with a wide range of methods. These methods are based on sequence tagging. To be specific, we compare our model with the approaches based on neural network, approaches based on pre-trained language models, such as BERT and BioBERT, approaches based on external knowledge and the approaches based on mulit-task learning. As can be seen from Table\u00a03, multi-task learning in solving BioNER tasks is becoming more and more popular, among them, Chai et al. achieved SoTA performance on two datasets BC4CHEMD and BC5CDR-Chem by training a model on 14 datasets, which realizes the multi-level information fusion between the underlying entity features and the upper data features. Both multi-task learning and fine-tuning are applied to their model. Tong et al. design multiple auxiliary classification losses by incorporating multi-granularity information in the datasets to achieve the best performance in the BC4CHEMD, BC5CDR-Chem, and BC5CDR-Disease datasets. They all get the best performance without utilizing additional resources. It is worth noting that Tian et al injects a lot of external syntactic knowledge (i.e., POS labels, syntactic constituents, and dependency relations) into BioBERT in the form of a key-value pair that works best on the BC2GM, BC5CDR-Chem and NCBI-Disease datasets. Although additional knowledge and multi-task learning can alleviate the problem of insufficient data, the additional knowledge usually contains a lot of noise, and it is difficult for us to control how much additional information should be selected. The training process of multi-task learning is too complicated, so it is difficult for us to design a general multi-task framework for many BioNER datasets. Therefore, the current methods can only be effective on some specific datasets. However, surprisingly, we achieve the best performance on all five datasets by using a novel word-pair relation classification schema and the proposed PAMDFGA. As indicated in Table\u00a03, compared with the models without additional knowledge, the improvement effect of our model is more obvious. First, we can see that our model outperforms existing methods, regardless of whether they introduce external knowledge, which further confirms the validity of our innovation in enhancing BioNER feature extraction. Second, although some models utilize higher-level features, such as Tian et al. leverages POS tags, syntactic constituents, and dependencies rules, and Tong et al. employs multi-task learning to train the model, our model can achieve better results with a simple attention guiding. This means our proposed model can better mine semantic information and solve entity sparse problems in all datasets, especially when mining datasets of disease entities (NCBI-Disease and BC5CDR-Disease) and gene entities (BC2GM). We can conclude that the features extracted from the PAMDFGA module are effective in assisting biomedical text representation, and even show more potential than special designs in the biomedical field, with the whole attention map guidance being pushed.",
        "Since our proposed attention mechanism PAMDFGA performs an average fusion of the attention weights in the two dimensions of prefix and original text length. In order to analyze the impact of different components of our proposed attention mechanism on different datasets. As shown in Table\u00a04, we conducted ablation experiments under the same experimental parameters, and the pre-trained model we used was BioBERT. In this table, baseline refers to our model not using any attention mechanism. PAMDFGA w/o  refers to not using  but retaining , PAMDFGA w/o  refers to not using  but retaining , and PAMDFGA refers to the final attention proposed. It can be seen from the results in Table\u00a04 that different components have certain guiding roles for BioBERT. Among them, PAMDFGA is better than using a single component alone in BG2GM, BC5CDR-Disease, BC5CDR-Chem and NCBI-Disease datasets. In view of the dataset BC4CHEMD, our proposed guidance does not perform as well as using the original attention weights. The reasons are two-fold: (1) this dataset, BC4CHEMD, are large in scale and the sentences in training set without entities are too long and too large. (2) After random initialization of BioBERT, the prefix distribution is mapped to the high dimensional space, resulting in the entity distribution is too sparse. So the guiding effect of these words is not good. Besides, we can find that the impact without  is bigger than without . But for the fused attention mechanism,  plays a further guiding role. However, compared with the baseline, our proposed three-way guided attention mechanism has significantly improved the entity recognition of the BioBERT because this attention mechanism can better utilize the grid information of word pair relationships. Removing any of them will result in performance degradation for almost all metrics. The results of ablation experiments demonstrate that our proposed PAMDFGA can bring more valuable information to BioBERT, as PAMDFGA can push each head to focus on different locations of the input to capture diversity information.",
        "Prefix length (P in Eq.\u00a07) is a influential hyper-parameter for PAMDFGA because the length of the prefix will participate in the calculation of self-attention, which will affect the effect of the words that PAMDFGA pays attention to. As shown in Fig.\u00a05, we experimentally demonstrate that the length of the prefix is the best within 20. The prefix length are selected from the set {5, 7, 9, 11, 13, 15, 20} according to grid search in our experiments. Figure\u00a05 illustrates that prefix length shows a similar distribution across the datasets, with a prefix length of 11 performing best on all datasets. This may be related to the sentence length in BioNER. As for this, the prefix length of 11 was chosen for both our ablation experiment and the best experimental results to guide our model.",
        "To demonstrate that our proposed PAMDFGA can be better integrated into each layer of PLMs, we studied the effect of PAMDFGA on each layer of BioBERT. We conducted experiments on the NCBI-Disease test set. As shown in Fig.\u00a06, for NCBI-Disease dataset, most layers in BioBERT can benefit from the proposed PAMDFGA, and the improvement effect is more obvious in the last four to five layers. Among them, the F1-score of the eleventh layer increased from 89.87 to 90.82%. The F1-score of the last layes has decreased relative to the eleventh layer, which is understandable because PAMDFGA encourages pushing information of different heads of BioBERT. That is, our attention mechanism plays a better attention effect in other layers. It can also be argued that PAMDFGA makes the attention information of different heads more diverse compared to the patterns that traditional pre-trained models pay attention to. That\u2019s because traditional pre-trained models have incremental F1 in the last few layers. In all our final experiments, we integrate PAMDFGA into the last four layers of BioBERT. For comparison, the baseline system also uses the outputs of the last four layers of BioBERT.",
        "Our proposed attention machine can work in the fine-tuning phase without modifying the self-attention formula, which means we does not need to re-train the PLMs. Our entire model maintains a time complexity identical to the Transformer, which is . N is the length of the input sentence. Thus, PAMDFGA also has merits in terms of time cost. Nevertheless, the calculation process of it will take more time than directly fine-tuning the pre-trained model. Table\u00a05 shows the per-epoch training time of entire fine-tuned model on five datasets. As can be seen from Table\u00a05, the increased time cost is minor by adding PAMDFGA. Specifically, the external time cost by PAMDFGA per-epoch training is about 2.21\u00a0s, 1.11\u00a0s, 3.35\u00a0s and 1.68\u00a0s on BC2GM, BC5CDR-Disease, BC5CDR-Chem and NCBI-Disease datasets, respectively. We can see that our attention mechanism hardly adds to the computational cost of training because we don\u2019t introduce additional parameters. In term of this, the advantage of PAMDFGA is even more significant. For the dataset BC4CHEMD, the time of training time increases 24.74\u00a0s. We consider the time cost acceptable, since this dataset is inherently large and PAMDFGA improves the recognition of chemical disease terms.",
        "To show and prove the validity of our proposed attention mechanism, we plot the full attention heatmaps on NCBI-Disease dataset to verify the reason why the 11-th layer works well in Fig.\u00a06. And we perform qualitative analysis on BC5CDR-Chem dataset with their real labels and predicted labels from the method based on sequence labelling using BioBERT and our model.",
        "To show the validity of the PAMDFGA in Fig.\u00a06 and prove our attention can pay attention to more positions, we present examples of full self-attention maps of a set of fine-tuned models with/without the PAMDFGA to provide a better illustration of different head pattern in Figs.\u00a07 and 8. The selected token sequence from NCBI-Disease after the WordPiece tokenizer is \u201c[\u2019[CLS]\u2019, \u2019the\u2019, \u2019first\u2019, \u2019recognized\u2019, \u2019human\u2019, \u2019kind\u2019, \u2019##red\u2019, \u2019with\u2019, \u2019hereditary\u2019, \u2019deficiency\u2019, \u2019of\u2019, \u2019the\u2019, \u2019fifth\u2019, \u2019component\u2019, \u2019of\u2019, \u2019complement\u2019, \u2019(\u2019, \u2019c\u2019, \u2019##5\u2019, \u2019)\u2019, \u2019is\u2019, \u2019described\u2019, \u2019.\u2019, \u2019[SEP]\u2019]\u201d. Specifically, we first take the attention weight change of the first head of BioBERT layer 11 as an example, as shown in Fig.\u00a07, we can see that there are many informative tokens overlooked by the self-attention without PAMDFGA (Fig.\u00a07a) but captured by our method (Fig.\u00a07b). Looking further at the full attention map, as shown in Figure \u00a08, we can find that the repeated attention patterns like diagonal pattern of different heads in different layers of BioBERT after using PAMDFGA are significantly reduced, and different heads in the last four layers of BioBERT, the words that the attention head pays attention to are more diverse. In this way, the probability of the entity being noticed will naturally increase. We can conclude our attention mechanism pushes the diversity of the entire attention map. This is also explain why the F1-score of layer 11 in Fig.\u00a06 is better because layer 11 pays more attention to the words near the entity words. For example, more attention is paid to the token \u2019hereditary\u2019 and \u2019deficiency\u2019. In fact, these tokens constitute an important biomedical entity, which should be paid more attention. Most other heads have a similar effect. The comparison of the heatmaps between the eleventh and twelfth layers in Fig.\u00a08b also demonstrates why the F1-score of the eleventh layer is higher.",
        "We randomly sampled one sentence from the BC5CDR-Chem test set and compare the sequence tagging method using BioBERT with the word pair model with PAMDFGA. Figure\u00a09 shows that our model has certain advantages over BioBERT in terms of learning entity information and alleviate the problem of label inconsistency. For example, BioBERT model based on sequence tagging usually only recognizes entity composed of a single word such as \u201ccalcium\u201d, and the entity composed of multiple words such as \u201cCD-832\u201d tends to be identified incorrectly, causing the base model regards these words as two different entities. As we all know, \u201cCD-832\u201d is an important and complete chemical entity. But surprisingly, since word-pair relationship classification can better capture the relationship between adjacent entities, our model will recognize that \u201cCD\u201d and \u201c-\u201d and \u201c-\u201d and \u201c832\u201d are the relation of NNW. \u201c832\u201d and \u201cCD\u201d are the relation of THW. Our model thus decodes the identified relationship into a complete entity. To further verify how much our model pays attention to entities, we draw the attention heatmaps of the model from the average attention perspective in Figure \u00a010. We mainly focus on the interactions of tokens, except for \u2019[CLS]\u2019 and \u2019[SEP]\u2019. The selected token sequence after the WordPiece tokenizer is \u201c[\u2019[CLS]\u2019, \u2019Effects\u2019, \u2019of\u2019, \u2019a\u2019, \u2019new\u2019, \u2019calcium\u2019, \u2019antagonist, \u2019c\u2019, \u2019##D\u2019, \u201983\u2019, \u2019##2\u2019, \u2019[SEP]\u2019]\u201d. Then the attention scores are averaged over all heads and layers. This visualization validates the effectiveness of proposed attention compared with the traditional self-attention pattern. As shown in Fig.\u00a010, we can see that there are many informative tokens overlooked by the Transformer-based method (Fig.\u00a010a) but captured by our method (Fig.\u00a010b). For instance, the PAMDFGA allows the tokens \u201cCD\u201d to strongly attend to the token \u201c-\u201d and \u201c832\u201d, but these tokens are paid less attention in the Transformer-based attention. In addition, our model also strengthens the attention between sub-words such as \u201983\u2019, \u2019##2\u2019. These explain why our model can better capture the semantic information of neighbor words.",
        "In this work, we addresses the BioNER problem with a new prediction pattern for the first time. Experiments show that this prediction mode has better entity recognition ability and entity words modeling ability than sequence labeling. Empirically, to further improve the performance to recognize biomedical entities, we design a novel and efficient prefix and attention map discrimination fusion guided attention mechanism by changing the attention distribution to enhance BioBERT. And our method outperforms the four existing mainstream methods. This work also points to a promising direction and provides a new research angle for BioNER. As to future work, we plan to explore the effectiveness of PAMDFGA in different PLMs and different biomedical tasks, and explore how to incorporate more domain-specific knowledge to guide self-attention learning in other domains such as some biomedical low resource domains.",
        "The dataset is available on https://github.com/cambridgeltl/MTL-Bioinformatics-2016. Our code is released at https://github.com/Guan-cloud/PAMDFGA.",
        "Not applicable.",
        "Not applicable."
    ],
    "title": "A prefix and attention map discrimination fusion guided attention for biomedical named entity recognition"
}