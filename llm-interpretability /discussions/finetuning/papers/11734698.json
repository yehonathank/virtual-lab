{
    "content": [
        "Recent advancements in genomics, propelled by artificial intelligence, have unlocked unprecedented capabilities in interpreting genomic sequences, mitigating the need for exhaustive experimental analysis of complex, intertwined molecular processes inherent in DNA function. A significant challenge, however, resides in accurately decoding genomic sequences, which inherently involves comprehending rich contextual information dispersed across thousands of nucleotides. To address this need, we introduce GENA language model (GENA-LM), a suite of transformer-based foundational DNA language models capable of handling input lengths up to 36\u2005000 base pairs. Notably, integrating the newly developed recurrent memory mechanism allows these models to process even larger DNA segments. We provide pre-trained versions of GENA-LM, including multispecies and taxon-specific models, demonstrating their capability for fine-tuning and addressing a spectrum of complex biological tasks with modest computational demands. While language models have already achieved significant breakthroughs in protein biology, GENA-LM showcases a similarly promising potential for reshaping the landscape of genomics and multi-omics data analysis. All models are publicly available on GitHub (https://github.com/AIRI-Institute/GENA_LM) and on HuggingFace (https://huggingface.co/AIRI-Institute). In addition, we provide a web service (https://dnalm.airi.net/) allowing user-friendly DNA annotation with GENA-LM models.",
        "The encoding of genetic information by DNA is a principal subject in biology, involving both straightforward and complex systems of translation and epigenetic coding, respectively. While the translation of messenger RNA to amino acid sequences employs a widely accepted genetic code, other forms of encoding, notably the epigenetic code, are more challenging. DNA sequences dictate functional genome elements, including promoters, enhancers and transcription factor (TF) binding sites, among others. However, the diversity and redundancy of their underlying motifs challenge their detection within vast eukaryotic genomes, complicating insights into non-coding genome evolution and interpretations of human genomic variants, given the yet-to-be-fully-unraveled complexity of the epigenetic code.",
        "The advent of next-generation sequencing and additional high-throughput technologies has catalyzed the accumulation and public deposition of extensive databases, rich with functional genomic elements, enabling the broad application of computational methods to large-scale genomic data analysis. We, along with others, have successfully employed machine-learning methods, including ensemble learning and convolutional neural networks, for this purpose. However, while potent, these approaches encounter constraints in identifying long-range dependencies within DNA sequences, a common phenomenon in human and other eukaryotic genomes. Recent strategies employing transformer neural network-based approaches seek to surmount these constraints, with cutting-edge transformer architectures showcasing the capability to infer-specific epigenetic properties and gene expression levels from DNA sequences with exceptional precision. However, training models tailored to specific tasks demands substantial computational resources, and their inference capabilities are inherently constrained by the targets represented in the training dataset.",
        "Transfer learning, especially through pre-training, has been widely adopted in natural language processing (NLP) for its capacity to enhance computational efficiency and performance in scenarios with limited target data. Models pre-trained on substantial unlabeled datasets can be fine-tuned or utilized as feature extractors for new tasks, frequently outperforming models trained on task-specific datasets, particularly when those datasets are smaller. The application of this approach to bioinformatics is exemplified by the development of DNABERT, a BERT-like transformer neural network pre-trained on the human genome to predict subsequences from context, and subsequently fine-tuned for downstream tasks such as promoter activity prediction and TF binding. While DNABERT signifies a promising advance, its applicability is hindered by an input size cap of 500 base pairs (bp), and recent DNABERT extension DNABERT-2 also has an input length limit of \u223c1\u20134\u00a0kb. This input length limitation restricts the ability of the models to capture the extended contexts vital for various genomic applications.",
        "Enhancing input size for transformer models has recently been addressed through several developments, including sparse attention, effective attention and recurrence. Sparse attention techniques, which utilize either predefined or learned attention patterns like sliding window or block-diagonal, linearize the quadratic dependency of full attention on input length. Conversely, linear attention methods approximate full token-to-token interactions through softmax linearization. In the domain of recurrent models, inputs are segmented and sequentially processed, with intersegment information relayed through prior hidden states or specialized memory. Notably, the recently introduced recurrent memory transformer (RMT) architecture facilitates information aggregation from both long and extremely long input sequences, spanning thousands to millions of elements, respectively.",
        "In this work, we introduce GENA language model (GENA-LM), a family of transformer-based foundational DNA models. After fine-tuning for predictive analysis of various functional genomic elements\u2014including promoter activity, splicing, polyadenylation sites, enhancer annotations and chromatin profiles\u2014GENA-LM models demonstrate state-of-the-art performance for a significant fraction of tasks, achieving top average performance relative to other models. Moreover, our augmentation of GENA-LM with the RMT enables tackling genomic tasks that require substantial input sequence lengths. We also explore new applications of GENA-LM, such as identifying DNA motifs essential for TF binding and assessing mutation effects in promoters and splice sites to aid in the prioritization of clinical variants. To broaden the model\u2019s utility beyond human genomes, we have developed and released species-specific models for yeast, Arabidopsis and Drosophila, as well as a multispecies model. To facilitate sequence annotation for the thousands of unannotated sequences now available, we have developed GENA-Web (https://dnalm.airi.net), a web service that generates various annotations based on DNA sequence input. We contribute to the research community by open-sourcing the GENA-LM family on GitHub (https://github.com/AIRI-Institute/GENA_LM) and providing pre-trained models (prefixed with gena-lm-) on HuggingFace (https://huggingface.co/AIRI-Institute).",
        "Human T2T v2 genome assembly was downloaded from NCBI (acc. GCF_009914755.1). Genomic datasets used to train multispecies models were downloaded from ENSEMBL release 106 (https://ftp.ensembl.org/pub/release-106/). The list of species is provided in Supplementary Table\u00a0S4. For the 1000-genome dataset, we used gnomAD v3.1.2 data. For taxon-specific models, we used the following resources:",
        "Arabidopsis model: Data were obtained from and contain chromosome-level genomes of 32 Arabidopsis thaliana ecotypes.",
        "Yeasts model: Data were obtained from and include telomere-to-telomere assemblies of 142 yeast strains.",
        "Drosophila model: Data were obtained from Progressive Cactus alignment of 298 drosophilid species generated by.",
        "To prepare genomic datasets for our training corpus, we processed each record in the genomic FASTA files. We excluded contigs with the substring \u2018mitochondrion\u2019 in their identifiers and those shorter than 10\u00a0kb. From the remaining sequences, we divided them into \u2018sentences\u2019 spanning 500\u20131000 bp\u2014the sentence length being randomized\u2014and compiled \u2018documents\u2019 with 50\u2013100 consecutive sentences. This approach follows the data processing in BigBird\u00a0. Data augmentation incorporated reverse-complement sequences, and we applied a stochastic shift for some documents to include overlapping genomic sequences.",
        "For the 1000-genome\u00a0Single Nucleotide Polymorphism\u00a0(SNP) augmentation, nucleotide substitution was executed, replacing reference alleles with alternative ones sourced from individual samples of 1000-genome cohort. In order to maintain the haplotype structure, each sample was processed individually. This meant that for every genomic region, multiple sequences were derived, each resulting from swapping reference alleles with sample-specific alternative variants from singe individual. We limited our focus to genomic regions where the proportion of positions with a noted variant for a given sample exceeded 0.01. No allele frequency filter was applied.",
        "For our initial models, bert-base and bigbird-base-sparse, we hold out human chromosomes 22 (CP068256.2) and Y (CP086569.2) as the test datasets for the masked language modeling (MLM) task. In contrast, for subsequent models, identifiable by the \u2018t2t\u2019 suffix in their names, we hold out human chromosomes 7 (CP068271.2) and 10 (CP068268.2) for testing. All remaining data were used for training.",
        "Models focusing exclusively on human data were trained using the preprocessed Human T2T v2 genome assembly combined with its 1000-genome SNP augmentations, totaling \u2248480 \u00d7 109 bp. On the other hand, multispecies models incorporated both the human-only and multispecies data, aggregating to roughly \u22481072 \u00d7 109 bp.",
        "The data splitting strategy for downstream tasks was anchored to methodologies previously described in literature relevant for each particular downstream task. Comprehensive specifics for each task are provided in their respective dedicated sections.",
        "We employed Byte-Pair Encoding (BPE) tokenization\u00a0 for our models, setting the dictionary size to 32\u2005000 and initializing with a character-level vocabulary comprised of [\u2018A\u2019, \u2018T\u2019, \u2018G\u2019, \u2018C\u2019, \u2018N\u2019]. Our study utilized two distinct tokenizers:",
        "The first tokenizer, trained exclusively on the human T2T v2 genome assembly, is denoted as \u2018T2T split v1\u2019 in Table\u00a02.",
        "The second tokenizer, trained on a mixture of human-only and multispecies data sampled equally, is labeled \u2018T2T+1000G SNPs+Multispecies\u2019.",
        "Both tokenizers incorporate special tokens: CLS, SEP, PAD, UNK and MASK. Notably, the \u2018T2T+1000G SNPs+Multispecies\u2019 tokenizer integrates a preprocessing step to manage extensive gaps: sequences with over 10 consecutive \u2018N\u2019 characters are consolidated into a singular \u2018\u2013\u2019 token.",
        "A concise overview of the dataset parameters for downstream tasks is presented in Table\u00a01. A comprehensive description follows.",
        "For the task of predicting promoters, we sourced human sequences located upstream of TSS (transcriptional start sites) from the EPDnew database (https://epd.epfl.ch/EPDnew select.php). Sequences of lengths 300, 2000 and 16\u2005000\u00a0bp were extracted, with each dataset being processed and assessed independently. For negative samples generation, we randomly selected genomic locations outside promoter sequences, ensuring that negative and positive samples do not overlap for maximum promoter length (16\u00a0kb). The entire dataset was segregated by sequence into training, validation and testing sets. The objective of this task is a binary classification: determining the presence or absence of a promoter within a given region.",
        "To predict splice donor and acceptor sites, we replicated the dataset from, utilizing the original scripts provided by the authors. We adhered to the same training and testing splits as outlined in. In this dataset, a central 5000-bp target region is bracketed by 10\u2005000\u00a0bp of context, with 5000\u00a0bp on each side. Splice site annotations within the target region are aligned to token positions. Tokens overlapping with either splice-donor or splice-acceptor sites are designated as positive samples for their respective splicing annotation class. Subsequently, both the target and its context were tokenized independently. If the combined length diverged from the model\u2019s input size, adjustments were made through either padding or truncation. In the event of truncation, sequences furthest from the target region\u2019s midpoint were first removed. We demarcated the context and target sequences using SEP tokens. Through this procedure, the target\u2019s size matched the model\u2019s input token count. However, the computational loss did not account for tokens representing either context or padding. This challenge is a multiclass, token-level classification task encompassing three categories: splice donor, splice acceptor and none.",
        "Candidate sequences, along with their associated housekeeping and tissue-specific activity in Drosophila cells, were sourced from the Stark Lab repository (https://data.starklab.org/almeida/DeepSTARR/Data/). These datasets are partitioned into training, validation and testing sets, consistent with those used for training the DeepSTARR model. The task at hand involves a two-class regression, wherein each 249-bp sequence is predicted to produce two continuous scores: one for housekeeping enhancer activity and another for developmental enhancer activity.",
        "We gained the DeepSEA dataset\u00a0 from its original repository (http://deepsea.princeton.edu/media/co \u00a0de/deepsea train bundle.v0.9.tar.gz). This dataset outlines the chromatin occupancy profiles of various genomic features, encompassing histone marks (HMs), TFs and DNAse I hypersensitivity (DHS) regions. The dataset comprises DNA sequences of 1000\u00a0bp, with a central 200-bp target region flanked by 400-bp contexts on either side. Each feature\u2019s occupancy is quantified over this 200-bp target. Additionally, we trialed an expanded context of 7800 bp (yielding a total input length of 8000\u00a0bp). To elongate the DNA context, we aligned the input DNA segments to the hg19 genome using bwa fastmap. Surrounding sequences at mapped sites were then harvested. Sequences that either failed remapping or aligned too proximate to a chromosome\u2019s terminus to permit extension were omitted, though these comprised <1% of the dataset. Our partitioning for training, validation and testing adhered to the divisions presented in the original DeepSEA dataset. The challenge is a multilabel classification, with class count reflecting the unique epigenetic profiles identified in DeepSEA (919 in total).",
        "For predicting polyadenylation sites, we employed the APARENT dataset\u00a0 (available at https://github.com/johli/aparent). This dataset characterizes the frequency with which transcription machinery recognizes specific nucleotide sequences as polyadenylation signals. Utilizing the scripts published by the authors, we extracted the target values and delineated the training and testing datasets. Furthermore, we retrieved APARENT predictions (noted under the field iso_pred) to gauge the performance of the APARENT model. We tokenized the sequences from both upstream and downstream segments of the 5\u2019-untranslated regions individually, and they were demarcated using a SEP token. This study focuses on regression analysis targeting 256-bp sequences.",
        "The dataset and literature scores were obtained from the Nucleotide Transformer (NT version 2 scores)\u00a0.",
        "The dataset was reconstructed based on the description provided in\u00a0. Genomes from five species (human, lemur, mouse, pig and hippo) were downloaded from NCBI (RefSeq assemblies GCF_000001405.40, GCF_020740605.2, GCF_000001635.27, GCF_000003025.6 and GCF_030028045.1, respectively). Four chromosomes (chromosomes 1, 3, 12 and 13) were used for models evaluation, other chromosomes were utilized during training. We sampled sequences from chromosomes randomly, using the uniform distribution. We used a five-way classification and reported top-1 accuracy. For each task length, we collected a total of 50\u2005000 DNA subsequences from each species, ensuring a comprehensive dataset for our analysis.",
        "We trained and expanded upon several transformer models, drawing inspiration from both BERT\u00a0 and BigBird\u00a0 architectures. These adapted models are consistently referred to as GENA-LM throughout this manuscript. Key distinctions between these architectures can be found in Table\u00a02. A comprehensive breakdown of parameters and specific combinations for each model is available in Supplementary Table\u00a0S5. Additionally, we enhanced BERT-based models with pre-layer normalization\u00a0. In instances where the layer normalization is applied even to the final layer output, it is distinctly mentioned as lastln in the model names. For precise parameter details, refer to Supplementary Table\u00a0S5.",
        "All models were pre-trained using the MLM objective. During this process, the sequence was tokenized and flanked by the special tokens, CLS and SEP. In alignment with the BERT pre-training methodology, 15% of the tokens were randomly selected for prediction. Among these, 80% were replaced with MASK tokens, 10% were swapped with random tokens and the remaining 10% were retained unchanged. Training extended for 1\u20132 million steps, utilizing a batch size of 256 and operated on 8 or 16 NVIDIA A100 GPUs. We employed the FusedAdam implementation of the AdamW optimizer\u00a0, made available through Nvidia Apex (https://github.com/NVIDIA/apex). The initial learning rate was set at 1 \u00d7 10\u22124, including a warm-up phase. For most models, we adopted a linear learning rate decay, but in cases where pre-training diverged, we manually adjusted the learning rate.",
        "In our standard procedure, we tokenize input sequences and prepend and append them with the service tokens CLS and SEP, respectively. To ensure compatibility with the model\u2019s input requirements, sequences are either padded or truncated as needed. For datasets necessitating specialized tokenization, the specific preprocessing steps are detailed in the relevant dataset section.",
        "Tokenized sequences were provided as inputs to downstream models. These models utilized one of the pre-trained GENA-LM architectures, augmented with a single fully connected output layer. The dimensions of this layer are denoted by (hidden_size, target_size). Here, hidden_size refers to the hidden unit size specific to the GENA-LM model (refer to Supplementary Table\u00a0S5), while target_size is specified in the description of each downstream task dataset discussed earlier. For single-label, multiclass classification tasks, we implemented a softmax activation function on the final layer, paired with cross-entropy loss. In contrast, multilabel, multiclass classification tasks employed a sigmoid activation function on the last layer, combined with a binary cross-entropy with logits loss. Regression tasks did not necessitate any activation function on the last layer and utilized mean squared error as the loss function. To address sequence classification and regression tasks, we used the hidden state of the CLS token from the final layer. Meanwhile, for token-level classification tasks, such as splice site prediction, all hidden states from the ultimate layer were employed. Both the weights of the final fully connected layer and the parameters of the entire GENA-LM were fine-tuned during this process. Learning rate warm-up\u00a0 was consistently applied across all tasks. The optimal number of training and warm-up steps was determined empirically for each individual task.",
        "The recently introduced RMT presents a novel approach to extend the context length of pre-trained models\u00a0. Unlike traditional transformers, which exhibit quadratic computational complexity in their attention layers, the RMT employs a recurrent mechanism to efficiently manage elongated sequences. This recurrent design ensures constant memory consumption and linear computational scaling with context length. To process input, the RMT divides the sequence into distinct segments, processing them in a sequential manner. Special memory tokens are integrated into the input of each segment. For a given segment, the outputs linked to its memory tokens are subsequently utilized as input vectors for memory tokens for the succeeding segment. By this method, a multilayer transformer, such as the pre-trained GENA-LM, functions as a single recurrent cell, addressing one segment at a time.",
        "For both promoter and splice site prediction tasks, we segmented the input sequence into units, with each containing 512 tokens (\u223c4.5\u00a0kb). The initial 10 tokens of every sequence were allocated for memory tokens. Segments were processed in a sequential manner, where outputs from the memory tokens of one segment are used as the input memory tokens of the subsequent segment. During the training phase, gradients were allowed to propagate from the final segment to the initial one through these memory tokens. We did not impose any restrictions on the number of unrolls in backpropagation through time, allowing gradients to flow uninterrupted from the final to the initial segment. The initial states designated for memory tokens were randomly initialized, and further refined during the fine-tuning process. For the task of promoter prediction, we restricted loss computation to only the last segment. Conversely, for splice site prediction, the loss was determined for every individual segment. The training employed the AdamW optimizer and learning rates of {1e\u221204, 5e\u221205, 2e\u221205}. With a batch size set at 128, the training was terminated when there were no discernible improvements in validation scores. The results for the promoter prediction task are presented as averages over five folds. Meanwhile, the splice site prediction task results are averages across three runs, each employing a distinct random initialization. Training scripts are accessible within our provided codebase.",
        "For the species classification task, we used gena-lm-bert-base-t2t model that has been augmented with RMT (eight segments) during the pre-training phase. The processes of fine-tuning were enhanced through the application of a curriculum learning strategy. This meant that our initial step included fine-tuning the model on DNA subsequences of 1000\u00a0bp in length (single segment). Following this initial phase, we proceeded to extend the fine-tuning process to handle longer DNA subsequences while using the model weights from the 1000-bp fine-tuned model as initial weights, increasing the challenge to a length of 32\u00a0kb (eight segments). Continuing with this progressive training methodology, we further advanced our model\u2019s capabilities by eventually fine-tuning it to efficiently process and analyze DNA subsequences extending up to 50\u00a0kb in length (12 segments). This gradual fine-tuning approach, in line with the principles of curriculum learning, facilitated the model in sequentially mastering tasks of escalating complexity, thereby enhancing its analytical precision and performance on genetic classification tasks.",
        "For our phylogenetic analysis, we randomly sampled 500 subsequences from each genomic sequence, as detailed in Supplementary Table\u00a0S3. To ensure representative sampling across entire genomes, the probability of selecting a sequence from a specific chromosome was proportionate to the chromosome\u2019s length. In instances not otherwise specified, we utilized the embedding of the CLS token from the final layer. Sequences shorter than 5\u00a0kb were processed using the bert-large-t2t model, whereas sequences exceeding this length were analyzed with the big-bird-base-t2t model to accommodate the extended context. For classifying species, we employed the HistGradientBoostingClassifier from the sklearn library, retaining its default parameters.",
        "We curated records from the ClinVar database as of 1 July 2024, applying several filters to ensure data quality and relevance. Only records with >1 piece of evidence (i.e. filtering by the single_submitter field) and no conflicting interpretations of significance (as indicated in the conflicting_interpretations field) were included. We retained only variants with consequences classified as either benign or pathogenic. To focus on regulatory variants, we excluded variants overlapping exons, as defined by Gencode V45. Additionally, we filtered out variants located on sex chromosomes and mitochondrial variants. From the filtered dataset, we specifically selected those single-nucleotide substitutions that overlap with promoters within the 2\u00a0kb EPDnew promoter dataset described previously.",
        "For each variant, we selected all overlapping promoters and computed the log odds value as follows: , where p represents the promoter presence probability derived from the gena-lm-bert-large-t2t model, which was fine-tuned on a 2-kb length human promoter dataset. The OR was calculated for both the reference sequence and the sequence containing the mutation, and their absolute difference was utilized as the mutation score. If a single mutation overlapped several promoters, the highest score among them was used.",
        "For the cross-species analysis of H3K27ac and CTCF binding sites, we collected Chromatin Immunoprecipitation followed by Sequencing (ChIP-seq) data from NCBI, with accession numbers listed in Supplementary Table\u00a0S6, and uniformly processed them using the MACS3 software. For each dataset, we filtered out peak calls located on scaffolds shorter than 200\u00a0kb, and from the remaining data, we randomly selected 2000 binding sites as positive samples. All genomic regions located at least 8-kb away from any positive sample were designated as negative samples, and 2000 negative samples were randomly chosen for each dataset. We next computed the center of each sample and collected 2000\u00a0bp of the flanking sequences (\u00b11000\u00a0bp) to provide contextual information.",
        "Each genome was randomly split into five folds, ensuring that sequences from different folds did not overlap and that chromosomes were uniformly distributed across the folds. We fine-tuned the gena-lm-bert-base-t2t model using the human SRR10182244 dataset for H3K27ac and the human SRR26329064 dataset for CTCF. When assessing performance on human datasets, we utilized sequences exclusively from one fold (fold 1), which was held out during training. For non-human species, since their data were not included during the human model\u2019s fine-tuning, we performed evaluations on each of the five folds. Consequently, results for each human dataset evaluation are depicted by a single point in Figure\u00a03A and\u00a0B, while results for non-human dataset evaluations are represented by five points.",
        "We evaluated the gena-lm-bert-large-t2t model, which was fine-tuned on human promoter sequences, using data from seven species: macaque, mouse, rat, dog, zebrafish, chicken and Caenorhabditis elegans. For each species, we downloaded promoter sequences from EPDnew and prepared the data into five folds, following the same procedure used for the human dataset. For each species, we conducted 25 evaluation experiments by cross-applying the models trained on each of the five human dataset folds to each of the five species-specific dataset folds. For human data, we provide five evaluation results obtained on each of the human dataset folds.",
        "We employed the Integrated Gradients algorithm\u00a0 to conduct token attribution analysis. For epigenetic data analysis, we utilized the bigbird-base-sparse-t2t model, which was fine-tuned on the standard DeepSEA dataset with sequences of 1000\u00a0bp. Despite the dataset comprising over 900 features, our analysis specifically targeted six key features: ATF1, CTCF, GATA2, H3K27me3, H3K9me3 and H3K4me1 ChIP-seq profiles from untreated K562 cells. For each genomic feature, we randomly chose 3000 nucleotide sequences that encompassed ChIP-seq peaks. Subsequent tokenization of these sequences adhered to the same methodology as that applied in the chromatin profile fine-tuning task. With default parameters set, token attribution values were derived. For motif analysis, we leveraged the XSTREME tool\u00a0. Both FIMO and XSTREME assessments sourced motifs from the HOCOMOCO\u00a0v11 database\u00a0.",
        "For token importance analysis concerning promoter mutations, we utilized the gena-lm-bert-large-t2t model fine-tuned on the 2-kb length promoter dataset as previously described. For each mutation, we identified all overlapping promoter regions and calculated token importance scores using Integrated Gradients. For each sample, the top-1 percentile of tokens were designated as \u2018highly important\u2019, while the remainder were classified as \u2018not important\u2019. We then overlapped mutations with these tokens and categorized each mutation as either \u2018overlapping highly important token\u2019 or \u2018overlapping not important token\u2019. If mutation overlapped tokens from both classes, the \u2018highly important token\u2019 class was assigned. Finally, we compared the distribution of pathogenic and benign mutations across the \u2018overlapping highly important token\u2019 and \u2018overlapping not important token\u2019 classes using a chi-squared test.",
        "The code to generate the findings of this manuscript is available in the \u2018supplementary code\u2019 section\u00a0and on our GitHub repository: https://github.com/AIRI-Institute/GENA_LM. Additionally, our trained models can be found on HuggingFace under the prefix \u2018gena-lm\u2019: https://huggingface.co/AIRI-Institute/.",
        "In this study, we introduce a new universal transformer model tailored for nucleic acid sequences, which offers several improvements over existing models such as DNABERT\u00a0 and BigBird\u00a0 (as depicted in Figure\u00a01A). To ensure its versatility across various applications, we pre-trained our model using multiple datasets and diverse input sequence lengths.",
        "In the data preprocessing phase, we have extended established pipelines by integrating BPE for sequence tokenization (Figure\u00a01A, bottom panel). The essence of BPE is that it constructs a sequence dictionary to pinpoint the most frequently occurring subsequences within the genome. This results in tokens of diverse lengths, ranging from a single base pair up to 64 bp. In our tests, the median token length was determined to be 9 bp (Figure\u00a01D). Interestingly, our BPE vocabulary revealed tokens of significant biological relevance. For example, the longest tokens were often indicative of familiar repetitive elements, such as LINEs or simple repeats (Figure\u00a01E). The tokenization approach we adopted is greedy, starting with the longest sequences in the dictionary and tokenizing them first. Employing non-overlapping tokens, as opposed to the overlapping k-mers used in earlier studies, allows for the analysis of more extended sequence fragments while maintaining the same model input size. To put it in perspective, 512 overlapping 6-mers represent 512 bp, but 512 non-overlapping BPE tokens can represent \u223c4.5\u00a0kb. This is a crucial factor when dealing with expansive and intricate genomes like that of humans. Nevertheless, it is worth noting that the model\u2019s granularity is confined to the resolution of these individual tokens, which might pose constraints for certain applications.",
        "Our second enhancement pertains to the diversification in the implementation of the attention mechanism. The foundational GENA models utilize a conventional attention mechanism, which empowers the model to discern relationships between every pair of tokens in the input sequences. Conversely, sparse GENA models incorporate a sparse attention mechanism. This approach extends the permissible length of the input sequence by constraining the overall number of connections. Nevertheless, it retains the capability to understand relationships between distant sequence elements. In the case of recurrent GENA models (see \u2018Handling even longer sequences with recurrent memory\u2019 section), the transformer is supplemented with memory capabilities. This modification facilitates the processing of even longer inputs by segmenting them.",
        "Through the integration of BPE tokenization and the sparse attention mechanism, we are able to train models that can handle input sequences of \u223c4.5\u00a0kb (512 tokens with full attention) and 36\u00a0kb (4096 tokens with sparse attention). The incorporation of recurrent memory further expands this capacity, allowing for the processing of input sequences spanning hundreds of thousands of base pairs.",
        "For model training, we utilized the MLM task, a prevalent technique in NLP wherein the model predicts a masked token based on its surrounding sequence context. Unlike previous studies that used the hg38 genome assembly\u00a0, we trained all our models using the more recent human T2T genome assembly, setting our experiment apart. To mitigate the risk of overfitting to the reference genome, we incorporated common variants from the 1000-genome project database into some of our models. Additionally, we enriched our training dataset with genomes from diverse species, encompassing standard model organisms such as mice, fruit flies, nematode worms and baker\u2019s yeast, as well as others covering the entire spectrum of eukaryotic taxa. For a detailed methodology, see \u2018Materials and methods\u2019 section.",
        "Throughout the manuscript, we collectively refer to our suite of developed models as GENA-LMs. Each specific model is designated by its label as shown in Table\u00a02. While each model has its unique merits and constraints, we wish to highlight the following:",
        "The gena-lm-bert-base-t2t model: This model emulates the BERT transformer architecture, serving as a benchmark for subsequent models.",
        "The gena-lm-bert-base-t2t-yeast/fly/athaliana/multi models: These models include multispecies or taxon-specific data during pre-training while using the same BERT architecture as a model described above.",
        "The gena-lm-bert-large-t2t model: With the most significant parameter count (336M) and an input capacity of 4.5\u00a0kb, it stands out in terms of complexity.",
        "The gena-lm-bigbird-base-sparse-t2t models: These models, although having fewer parameters than the gena-lm-bert-large-t2t, boast a more extended input sequence length of 36\u00a0kb.",
        "Upon evaluating the performance of our models in the MLM task (Figure\u00a01F), we observed that models with sparse attention slightly outperformed their full-attention counterparts limited to 512 tokens. This underscores the role of contextual information in the training regimen. Nonetheless, it is imperative to note that while achieving commendable scores in the MLM task is encouraging, it does not necessarily guarantee optimal translation of the learned DNA representations to downstream applications. Consequently, our study delves into the comprehensive assessment of GENA-LMs across a spectrum of biologically relevant tasks to explore their merits and constraints.",
        "To evaluate the foundational GENA-LM models, we selected a range of genomic challenges that have recently been addressed using artificial intelligence (Figure\u00a01B). These challenges encompass (i) prediction of polyadenylation site strength; (ii) forecasting of chromatin profiles, which includes histone marks (HMs), DHS sites and TF binding sites, among others;\u00a0(iii) identification of splicing sites. In addition, we employed a comprehensive set of 18 benchmarks recently developed by. The datasets for these tasks are derived from human genomic data. To explore the performance of models when applied to non-vertebrate species, we introduced challenges including (iv) determining the activity of housekeeping and developmental enhancers in a STARR-seq assay in Drosophila cells and (v) estimation of DNA sequence promoter activity in humans, flies, yeasts and plants.",
        "The major feature of GENA-LMs is their ability to process long DNA sequences, ranging from 4 to 36\u00a0kb. Consequently, we benchmarked GENA-LMs on tasks where understanding long-range dependencies in DNA sequences is crucial for accurate prediction. In genomics, these long-range dependencies are particularly significant for various epigenetic features, which makes predicting a locus\u2019s epigenetic states based on its sequence a significant challenge. To assess the capabilities of the GENA-LM transformers in addressing this, we used DeepSEA dataset. This dataset encompasses over 900 cell-type-specific chromatin profiles, which are grouped into DHS sites, HMs and TF binding sites. In the foundational DeepSEA challenge, chromatin mark signals were predicted for each 200-bp genomic segment, informed by both its sequence and an additional 800-bp context derived from its flanking regions (\u00b1400\u00a0bp).",
        "When deploying GENA-LMs for this challenge (see DeepSEA section\u00a0of Table 3), we discovered that transformer models markedly surpassed the performance metrics previously achieved by the convolutional neural network, DeepSEA. Notably, for TF and DHS profiles, GENA-LMs delivered scores that eclipsed those reported for the BigBird architecture, even though BigBird utilized an expanded 8-kb context (leading GENA-LM average\u00a0Receiver Operating Characteristic Area Under the Curve\u00a0(ROC AUC) on a 1-kb context for TF: 96.81 \u00b1 0.1\u00a0versus BigBird\u2019s 96.1; for DHS: 92.8 \u00b1 0.03\u00a0versus BigBird\u2019s 92.3). Furthermore, the performance metrics for GENA-LMs were either on par with or exceeded those recently reported for the Nucleotide Transformer\u00a0. They also proved superior to the outcomes of the DNABERT architecture when trained on 1-kb input lengths.",
        "To ensure a more equitable comparison between the BigBird and GENA-LM architectures, we adapted the DeepSEA dataset to incorporate expanded context sequences. This adaptation allowed us to match the 8-kb input length characteristic of the BigBird architecture. Intriguingly, the augmented context had differential effects on the prediction of various epigenetic profiles. For HMs, a marked performance improvement was evident, with an AUC reaching 89.71 \u00b1 0.08. This was notably superior to the shorter context\u2019s score of 86.64 \u00b1 0.08, the original DeepSEA findings (85.6), and the BigBird\u2019s result (88.70). However, for TF and DHS predictions, the extension in input length yielded only marginal enhancements in performance.",
        "We analyzed the AUC variations across individual HMs to pinpoint which epigenetic profiles were responsible for the observed performance enhancement. Remarkably, there was a distinct divergence between narrow and broad HMs. While the narrow marks demonstrated marginal AUC improvements, the broad marks exhibited a pronounced increase when the context length was extended (Supplementary Figure\u00a0S1). These observations reinforce our prior observation that broad HMs necessitate an expansive context for precise prediction. This highlights the importance of handling extended input lengths for such tasks.",
        "The varying performance metrics of distinct GENA-LMs across diverse epigenetic profiles and context lengths underscore that no singular model universally excels across all challenges. For TFs, the gena-lm-bigbird-base-sparse-t2t stands out on 1-kb inputs, with performance diminishing marginally when the input size increases. In contrast, for DHS, the gena-lm-bert-large-t2t model, boasting the highest parameter count, emerges as the optimal choice. Surprisingly, extending the context for this model results in a notable performance dip. For HMs, the optimal approach hinges on processing extended contexts with the gena-lm-bigbird-base-t2t model. Collectively, GENA-LMs outstrip competing models like BigBird, DNABERT and Nucleotide Transformer, marking a new performance state of the art for this task.",
        "Promoter activity is an essential characteristic of genomic sequences, allowing them to drive the expression of genes. Although the basal promoter is a relatively short genomic region of\u00a0300\u00a0bp, surrounding context can substantially modify promoter activity. We assessed whether our models can discriminate human promoter sequences using promoter instances from the EPD dataset and juxtaposed them against non-promoter control samples. We observed that when the input sequence length was extended from 300\u00a0bp to 2\u00a0kb, there was a significant improvement in performance, as shown in the EPDnew section\u00a0of Table 3. With 300-bp sequences, the DNABERT architecture emerged superior, registering an F1 score of 93.26, compared with the top-performing GENA-LM\u2019s F1 score of 90.62. However, when evaluating 2-kb sequences, the GENA-LM performance matched DNABERT results, recording an F1 score of 94.28 \u00b1 0.65 and 94.16 \u00b1 0.19 for DNABERT and gena-lm-bert-large-t2t, respectively (no significant difference, Wilcoxon test P-value=0.0625). This result was markedly higher than the DNABERT-2 model\u2019s score, which, when fine-tuned for the same input sequence length, achieved an F1 score of 92.98 \u00b1 0.25.",
        "In assessing the performance of GENA-LMs for predicting promoter activity, we observed the following: (i) Models with a greater number of parameters outperformed those with fewer. For instance, the gena-lm-bert-large-t2t surpassed the gena-lm-bert-base-t2t. (ii) The ability to handle longer input sequences due to the sparse attention mechanism gave certain models an edge over traditional full-attention BERT models. As a result, the gena-lm-bigbird-base-sparse outperformed the gena-lm-bert-base-t2t. Interestingly, models with shorter inputs but more parameters, such as the gena-lm-bert-large-t2t, still had superior performance over the gena-lm-bigbird-base-sparse. (iii) Incorporating multispecies training by using genomic sequences beyond just human data during pre-training did not result in improved performance, as seen when comparing the gena-lm-bert-base-t2t-multi with the gena-lm-bert-base-t2t.",
        "We further optimized GENA-LMs to predict splice-donor and splice-acceptor sites within the human genome (Splice section\u00a0of Table 3). The task required analyzing large contexts: a 15-kb input comprised of a central 5-kb target flanked by 5-kb sequences on either end. Notably, the task-specific convolutional neural network, SpliceAI, marginally surpassed GENA-LMs, registering a mean PR (precision-recall) AUC of 0.960 compared with 0.947 \u00b1 0.002 for GENA-LMs.",
        "For this task, models designed for longer sequence inputs, such as gena-lm-bigbird-base-t2t, outperformed those tailored for shorter inputs, even if the latter were equipped with more parameters, as in gena-lm-bert-large-t2t. This aligns with our earlier findings, suggesting that extending contextual information could be more beneficial than merely increasing the number of parameters. Consistent with our promoter analysis, multispecies models, like gena-lm-bert-base-t2t-multi (mean PR AUC of 0.914), did not enhance performance compared with their single-species counterparts, such as gena-lm-bert-base-t2t (mean PR AUC of 0.926).",
        "To compare GENA-LM with with several recently developed DNA language models, including Nucleotide Transformer, DNABERT, DNABERT-2, HyenaDNA and fine-tuned versions of Enformer, we adopted a recent series of 18 benchmarks, which include relatively short sequence inputs ranging from 300 to 600\u00a0bp. According to the results presented in the Table\u00a04, gena-lm-bert-large-t2t outperformed all other models, achieving the highest average score and the second-best average ranked score. Notably, gena-lm-bert-large-t2t outperformed Nucletide Transformer in multiple tasks, despite having substantially less parameters (330M versus 2500M). Similarly, GENA-LMs demonstrated performance on par with these models in another series of benchmarks focused on prediction of the human polyadenylation sites and Drosophila enhancer activity, as detailed in Supplementary Note 1 (Supplementary Figures S9, S10 and S11, respectively).",
        "Modern techniques for analyzing deep neural networks allow us to assess the contribution of each input element to a model\u2019s downstream task performance. Such analyses offer valuable insights into the underlying mechanisms of biological processes. Take the ChIP-seq technique, for instance, a prevalent method for chromatin profiling. Its resolution is \u223c100\u2013200\u00a0bp. However, recognition motifs for the majority of DNA-binding proteins are significantly shorter, typically between 4 and 10\u00a0bp. Consequently, deducing precise binding locations from ChIP-seq data is challenging and often necessitates supplementary experiments.",
        "To ascertain if GENA can enhance the resolution of experimental ChIP-seq data, we employed token importance scoring on the bigbird-base-sparse-t2t model, which was fine-tuned using the DeepSEA dataset. This methodology assigns a significance value to each token within the input, based on its relevance to the prediction outcome. Here, we concentrate on the binding of three TFs: ATF1, CTCF and GATA2 in human K562 cells. Each of these factors possesses well-established DNA recognition motifs (Figure\u00a02A). This allows for a comparison between tokens important for GENA\u2019s predictions and the recognized sequence determinants associated with TF binding.",
        "First, we examined the distribution of importance scores across the sequence length, as depicted in Figure\u00a02B. It should be noted that during the fine-tuning process, the input consists of the DNA sequence from the 200-bp target region (where TF binding is anticipated) accompanied by an 800-bp contextual sequence. Our analysis, presented in Figure\u00a02B, reveals a consistent pattern: the importance attributed to a token diminishes as its distance from the target region increases, a trend observed for all three TFs.",
        "We subsequently sought to discern which sequences garnered high token importance scores. Tokens exceeding the 95th percentile of the importance score distribution were designated as \u2018highly important.\u2019 We then compiled tokens that consistently featured on this \u2018highly important\u2019 list. Upon visual examination (Figure\u00a02C), we observed that these tokens frequently encompassed full or fragmented motifs of the target TFs. For instance, ATF1, which has a core motif of TGACG, prominently displayed a token matching this exact sequence among its highly important tokens. In the case of the GATA2 factor (core motif: GATAA), the token AGATAAG, incorporating the GATA2 motif, was most prevalent among the highly important tokens. As for CTCF, which boasts a motif more intricate and extended than its counterparts, the most recurrent highly important tokens primarily featured GC-rich subsequences of the motif.",
        "To more comprehensively assess the congruence between known TF motifs and \u2018highly important\u2019 tokens, we employed the FIMO tool to annotate all DNA samples. FIMO is a bioinformatics software designed to identify specific motifs by leveraging the motif\u2019s position weight matrix (PWM). As depicted in Supplementary Figure\u00a0S2, there is a discernible overlap between significant tokens and motifs detected by FIMO. Our statistical evaluation establishes a relationship between FIMO motif scores and token importance scores. Both robust motifs (FIMO q-value < 0.01) and more tenuous motifs (0.01 < FIMO q-value < 0.05) manifest markedly elevated token importance scores compared with tokens absent of any discerned motif (FIMO q-value > 0.01) (Figure\u00a02D). It is worth noting, in the context of the GATA2 TF which is characterized by a shorter motif length, sequences with high FIMO q-values are absent. Nonetheless, we observed that the majority of the \u2018highly important\u2019 tokens encompass the core GATA2 motif, as delineated in Supplementary Figure\u00a0S3.",
        "While the results affirm that token importance mirrors the presence of established motifs for DNA-binding TFs, the congruence between FIMO-detected motifs and tokens with high scores is not absolute. This observation prompted us to delve into the nature of motifs encompassed by tokens vital for GENA model prediction, yet devoid of the target TF\u2019s annotated motif as per FIMO. Utilizing the de novo motif discovery tool XSTREME, we analyzed a subset of important tokens lacking a canonical motif (with FIMO target factor motif q-value > 0.05) and examined the enriched motifs therein. Intriguingly, for both CTCF and GATA2, XSTREME predominantly identified their respective motifs. In the case of important ATF1 tokens sans ATF1 motif, the secondary most abundant motif discerned belonged to the ATF family. This suggests that the rudimentary PWM statistics employed by FIMO might overlook biologically pertinent motif variants that diverge notably from the consensus represented by the PWM. Conversely, GENA-LM exhibits a promising potential in recognizing these variant motifs. When consolidated during XSTREME analysis, these diverse motif representations converge to echo the canonical motif\u2019s PWM. Moreover, our analysis revealed a significant presence of GATA2 motifs within tokens deemed essential for the ATF1 factor, hinting at a possible functional synergy between these TFs in K562 cells\u2014a nuance discerned by GENA-LM. Given that ATF1 is an integral part of the AP-1 complex, our findings resonate with, and potentially elucidate, prior experimental data evidencing cooperation between GATA2 and the AP-1 complex.",
        "Having ascertained GENA\u2019s capacity at pinpointing DNA motifs crucial for TF binding, we turned our attention to discerning sequence determinants associated with HMs, which currently lack identifiable binding motifs. Our focus centered on H3K4me1, H3K9me3 and H3K27me3. These factors represent HMs with distinct and well-documented functional implications: H3K4me1 delineates active genomic regions, H3K9me3 signifies heterochromatin and H3K27me3 demarcates the suppressed \u2018facultative heterochromatin\u2019 including genes under developmental regulation, bound by polycomb group proteins. In our examination of these factors, we evaluated the distribution of importance scores relative to sequence length and accumulated tokens that frequently exhibited high importance values.",
        "As depicted in Supplementary Figure\u00a0S4, the distribution of token importance scores across sequence lengths for these epigenetic markers mirrors that of the previously discussed factors. Notably, specific tokens consistently emerged as highly significant in predicting these HMs, reminiscent of the motif-rich tokens previously identified as important for predicting associated TFs (see Supplementary Figure\u00a0S5).",
        "Prompted by our observations, we sought to determine if motifs enriched among the tokens with high importance scores were shared across these three factors. Extending the sequences of our selected highly important tokens by 4\u00a0bp, we then undertook a rigorous motif analysis using XSTREME. Our examination revealed several motifs of significance (see Supplementary Table\u00a0S1), each aligning with known TFs. For the active promoter mark, H3K4me1, the significant tokens were found to encompass motifs corresponding to the GATA, JUN and FOSL TFs. These findings align with the documented roles of these factors in regulating transcription and influencing the oncogenic transformation of K562 cells. In the context of the H3K27me3 mark, which signifies facultative heterochromatin and designates functional elements repressed in specific cell lineages, our data from hematopoietic K562 cells indicated an enrichment of TF motifs not typically associated with blood cells. Examples include SNAI2, pivotal in epidermal cell differentiation, and ASCLI, a critical regulator of neurogenesis. This suggests that within this setting, GENA discerned genomic motifs that might be activated in alternative cell types but are designated for repression in the K562 lineage. Lastly, for the H3K9me3 mark indicative of constitutive heterochromatin, our analysis highlighted an enrichment of the ZNF274 TF motif. This is in agreement with its established role as a transcriptional repressor.",
        "In summary, our findings indicate that beyond predicting the epigenetic profiles of a specific locus, GENA models can effectively identify the distinct subsequences that drive observed epigenetic signals. Such analysis holds potential to substantially augment the resolution of prevailing experimental approaches, like ChIP-seq, and to pinpoint TFs linked to particular HMs.",
        "The capability of GENA-LMs to accurately predict promoter activity inspired us to investigate whether model predictions could functionally characterize variants in human promoters. We compiled ClinVar mutations that overlap with promoter sequences and evaluated them using the odds ratio of promoter presence probability for wild-type versus mutated sequences. While not all pathogenic ClinVar variants overlapping promoters impact predicted promoter activity, a pathogenic-versus-benign classifier based on gena-lm-bert-large-t2t predictions achieved an AUC of 0.66 and an average precision (AP) of 0.59 (Supplementary Figure\u00a0S6). Further, we applied the Integrated Gradients method\u00a0 to identify input sequences tokens that are the most important for predicting promoter presence. Our analysis revealed that pathogenic mutations are enriched \u223c2.5\u00a0times in the top percentile of the most important tokens (P-value < 1e\u221215, Supplementary Table\u00a0S2). Comparable results were obtained with the gena-lm-bigbird-base-sparse-t2t model.",
        "Similar to promoters, mutations at splice sites often have clinical implications in human diseases. To determine whether GENA-LMs can detect the impact of single-nucleotide perturbations at splice donor and acceptor sites, we conducted comprehensive in silico mutagenesis, evaluating the effects of every possible single-nucleotide substitution within a \u00b120-bp sequence surrounding splice sites. Despite the token-level resolution of the inputs, predictions from gena-lm-bigbird-base-t2t proved sensitive to single-nucleotide substitutions. We observed that the model distinctly identifies substitutions within canonical splice site motifs from other single-nucleotide variants (Supplementary Figure\u00a0S7). While the latter rarely alter the model\u2019s prediction, mutations within canonical splice sites almost invariably abolish the predicted acceptor or donor site. Notably, the predicted effects of single-nucleotide substitutions align precisely with known splice site motifs: changes in the constant motif positions have more significant effects compared with the substitutions in the more variable regions of the motif. These findings highlight the potential of GENA-LMs for clinical interpretation of human genetic variants.",
        "While the results discussed previously demonstrate the high efficacy of fine-tuned GENA-LM models in reconstructing and analyzing genomic features such as promoters or protein binding sites, task-specific training of these models necessitates the availability of experimentally measured data, which is often lacking for non-model species. We hypothesize that features relatively conserved across different species and cell types could be effectively predicted by a model that has been fine-tuned using human (or other reference) data. To explore this hypothesis, we collected experimentally measured profiles of insulatory protein CTCF binding sites, H3K27 acetylation (H3K27ac) HMs, and promoter activity across various animal species.",
        "We initiated our study by evaluating a human-based promoter activity prediction model on data from seven species: macaque, mouse, rat, dog, zebrafish, chicken and C. elegans. For mammals (macaque, mouse, rat and dog), the model\u2019s performance closely mirrored that observed with human promoters, achieving an F1 score of \u223c0.95, despite the absence of species-specific data during the model\u2019s fine-tuning (Figure\u00a03A). The evaluation score decreased by 10 pt when applying the same model to phylogenetically more distant vertebrate species such as chicken and zebrafish, but it still achieved a relatively high F1 score of around 0.85. However, for more distantly related species like the fruit fly or the flatworm C. elegans, the model\u2019s performance dropped significantly, resulting in an F1 score of \u223c0.7. These results suggest that closely related mammals share similar promoter grammars, which can be captured by training on a human dataset, while more distant species exhibit gradual modifications in the DNA encoding of their promoters.",
        "Then gena-lm-base-t2t was trained to differentiate between genomic sites with H3K27ac and CTCF binding sites and those without binding on human data, achieving F1 scores between 0.7 and 0.8 on held-out human sequences (Figure\u00a03B and\u00a0C). Applying this model to other species revealed that its performance correlates with the evolutionary distance between the target species and humans. For CTCF binding prediction, the model demonstrated similar performance in closely related species, such as mice, with a gradual decrease in other vertebrates and a more significant drop in invertebrate species such as Drosophila (Figure\u00a03B and\u00a0C).",
        "While similar cross-species analyses for CTCF can be performed using a known motif, this is infeasible for H3K27ac due to the lack of any recognized motif. When evaluating the human H3K27ac model across species, we observed a substantial performance decline outside tetrapod species, and for evolutionarily distant species like Hydra F1 score drops to \u223c0.4. The decline in model performance across species may serve as a measure of the interspecies differences in H3K27ac encoding. Thus, the use of GENA-LMs enables explorations into the evolution of sequence grammar. Furthermore, these results suggest that GENA-LMs can be used to infer epigenetic marks from genomic sequences in the absence of experimental data. Although the performance of such inferences is inferior to that of species-and cell type-specific models and may be limited to taxonomically close groups, it presents extensive opportunities for annotating available genomes, such as using the human model to annotate H3K27ac across several thousand available mammalian genomes.",
        "Based on the aforementioned experiments, it is clear that promoter grammar varies significantly across evolutionarily distant species such as humans, flies, yeasts and plants. As a result, learning species-specific information during pre-training could prove beneficial for accurate promoter presence prediction and other species-specific genomic tasks. However, pre-training species-specific GENA-LMs requires sufficient amount of data and computational resources. Thus, it is important to know whether transfer learning, when a model pre-trained on one data distribution is reused for tasks in similar domain, can substantially improve performance. Another viable strategy in this case involves generation of a universal foundational model trained on the mixture of species.",
        "To study generalization capabilities of GENA-LM models, we utilized EPD promoter annotations for yeast, fly and Arabidopsis species to fine-tune universal, multispecies or species-specific models. We discovered that for all these species, gena-lm-bert-base-lastn-t2t pre-trained on human data can be effectively fine-tuned to provide reasonably accurate promoter classification (Figure\u00a03D). When comparing datasets with varying promoter lengths (300\u00a0bp versus 2\u00a0kb), we observed a significant impact of contextual information on prediction accuracy, mirroring the dependency noted in human data. Compared with the model pre-trined on human data, multispecies pre-training proved to be benefitial: in five out of six experiments, we observed performance improvements ranging from 0.2 to 1.5 points when fine-tuning gena-lm-bert-base-t2t-multi model. It is important to note that the gena-lm-bert-base-t2t-multi training dataset includes dozens of genomes; hence, the amount of data from each species available during the pre-training phase was limited. Consequently, we pre-trained new taxon-specific models for yeast, flies and Arabidopsis. Fine-tuning these models to predict promoter activity in their respective taxa resulted in performance improvements of 2\u20137 points compared with the model pre-trained on human data (Figure\u00a03D). Therefore, we conclude that taxon-specific models can significantly enhance DNA annotations by learning the unique DNA grammar of each species during pre-training. We have publicly released these taxon-specific models to facilitate further applications within the selected species.",
        "The universality of GENA-LMs in addressing various biological challenges suggests that the DNA embeddings of the pre-trained model encapsulate significant biological insights. Evolutionary distant species are known to exhibit divergence in their regulatory code and codon usage patterns. If GENA-LMs effectively capture these inherent biological characteristics during pre-training, one would expect that their embeddings could differentiate DNA sequences sourced from varying species without any additional fine-tuning. To evaluate this premise, we curated a set of 27 species spanning diverse taxonomic classifications, ranging from bacteria to humans (refer to Supplementary Table\u00a0S3). These species also represent a broad spectrum of evolutionary divergence times, spanning from millions to over a billion years (as depicted in Figure\u00a04A and\u00a0B). We then examined the embeddings generated by inputting genomic DNA subsequences into pre-trained GENA-LMs. Our investigations encompassed a range of sequence lengths, beginning with the typical length of shotgun sequencing reads (100\u00a0bp) and culminating at 30\u00a0kb, a size consistent with reads from third-generation sequencing platforms.",
        "Initially, we employed tSNE to project sequence embeddings derived from all genomes into a 2D space. This visualization reveals discernible clusters that mirror the phylogenetic relations between the species. Notably, distinct groupings emerged for bacteria, plants and yeasts, each isolated from the clusters representing animal genomes. Within the realm of animals, we could discriminate invertebrate species and various vertebrate classes. This evidence underscores that GENA-LM embeddings encapsulate nuances allowing for the differentiation of species based on their genomic sequences.",
        "To deeply study these capabilities, we employed a Gradient Boosting algorithm for each of the 27 species pairs. Our aim was to achieve binary species classification leveraging the sequence embeddings.",
        "The data in Figure\u00a04C show the richness of information contained in GENA embeddings, enabling species differentiation based on their genomic DNA subsequences. The accuracy of classification is influenced by both the divergence time and the length of the input sequence, with the latter exerting a more pronounced effect. For species that are closely related (with divergence times \u2264 20 MYA), classification accuracy remains constrained (F1 score \u2248 0.7). However, for species diverging around 60\u2013100 MYA\u2014equivalent to the evolutionary separation among all mammalian species\u2014employing the model that accepts longer sequence inputs boosts our classification capability, yielding an F1 score exceeding 0.8. Remarkably, for extensive divergence times (\u2265200 MYA, reflecting the era of the last common ancestor of vertebrates), the classification\u2019s precision approaches perfection.",
        "We next evaluated the classification efficacy of sequence embeddings derived from various layers and architectures of GENA-LMs. Across all models, embeddings sourced from the initial layers consistently delivered subpar performance. This performance incrementally improved, peaking around layers 9 to 12. Notably, for both gena-lm-bert-large-t2t (comprising 24 layers) and gena-lm-bigbird-base-t2t (with 12 layers), a minor performance dip was observed when utilizing embeddings from the final layers. This trend resonates with prior studies in NLP and protein modeling\u00a0. Such studies have posited that in transformer-based language models, the terminal layer embeddings encapsulate information tailored to the specific model training task. In contrast, embeddings from intermediary layers are more versatile, proving advantageous in knowledge transfer for tasks not explicitly addressed during the pre-raining phase. The depth of the layer is also indicative of the abstraction degree of the representations. While preliminary layers prioritize local level representations, the advanced layers capture intricate global features, such as binding sites and contact maps\u00a0.",
        "Collectively, our findings demonstrate that the sequence embeddings from pre-trained GENA-LMs encapsulate abundant biological insights, enabling the resolution of genomic challenges without the necessity for fine-tuning.",
        "Given the demonstrated potential of GENA-LMs in various genomic tasks, we aim to extend their accessibility through GENALM-Web, a web service designed for sequence annotation using DNA language models (Figure\u00a01C). GENALM-Web incorporates several downstream tasks developed in this paper, such as promoter activity prediction, chromatin annotation, splice site inference and enhancer activity prediction for Drosophila sequences. Key features of the web service include the capability to handle exceptionally long inputs (up to 1 Mb), utilize extensive contextual information and conduct token importance analysis in real time. This last feature allows users to identify sequence regions responsible for specific features, even if these regions are located at distant genomic locations. The web service is accessible at https://www.dnalm.airi.net, and documentation is available at service pages and in the supporting manuscript.",
        "While the integration of sparse attention techniques and BPE tokenization in GENA-LMs has substantially expanded the permissible DNA input length, the current limit (about 36\u00a0kb) may not sufficiently capture certain biological dependencies. Notably, the prediction of chromatin interactions, enhancer\u2013promoter associations, gene expression and other genomic phenomena necessitate the processing of contexts that extend beyond 30\u00a0kb. Additionally, our empirical analyses show improvements in promoter and splice site predictions as the context size expands from 512 to 4096 tokens (see \u2018GENA-LM performance on different genomic tasks\u2019 section). This indicates the potential benefits of further enhancing sequence length for these biological tasks.",
        "To enhance the input capacity of GENA-LMs, we incorporated recurrent memory mechanisms. The RMT has been demonstrated as an efficient, plug-and-play method to handle extended input sequences using pre-trained transformer models\u00a0. In this recurrent strategy, the input sequence is partitioned into segments which are processed one after the other (Figure\u00a05A). Special memory tokens are introduced to each segment to pass information between consecutive segments, allowing them to use information from all previous segments. Thus, the entire pre-trained transformer effectively functions as a single recurrent unit.",
        "RMT can be optionally incorporated during pre-training, enabling the model to learn the use of memory tokens at this stage. Alternatively, memory tokens can be introduced during the fine-tuning phase, using a model that was pre-trained without RMT. To assess these two training strategies and determine the optimal approach, we conducted pre-training experiments with gena-lm-rmt-base-t2t and gena-lm-rmt-large-t2t models. During pre-training, we evaluated how RMT augmentation influences MLM accuracy. As illustrated in Supplementary Figure\u00a0S8, RMT augmentation enhances accuracy for the base-size model when provided with 8\u201310 segments of contextual information. However, the large-size model pre-trained without RMT augmentation achieves a substantially better score. Extending the large-size model\u2019s input with RMT does not improve the score (Supplementary Figure\u00a0S8). These findings corroborate our previous observation that gena-lm-bert-large-t2t outperforms gena-lm-bigbird-base-sparse in the MLM task Figure\u00a01F, despite the latter\u2019s longer input length. Therefore, we propose that long-range dependencies beyond 4\u00a0kb are not necessary for accurate masked token prediction. However, these dependencies are important for downstream tasks.",
        "For a comparative evaluation between RMT and other GENA models, we focused on tasks with inputs of moderate length (15\u201316\u00a0kb), which can be processed by sparse models. The gena-lm-bert-large-t2t model, when integrated with RMT, was fine-tuned on sequences of 16\u00a0kb for promoter prediction and 15\u00a0kb for splice site prediction. Inputs were divided into segments, with each segment comprising \u223c512 tokens or about 4.5\u00a0kb. These segments also included memory tokens as part of the input, with 10 memory tokens used for each task. When contrasted with the original gena-lm-bert-large-t2t model, the sequence length processed by the gena-lm-bert-large-t2t + RMT increased substantially: from three to eight times (rising from 4.5 to 15\u00a0kb for the splice site prediction and from 2 to 16\u00a0kb for other tasks).",
        "The expansion in input length significantly enhanced the performance of the gena-lm-bert-large-t2t model, as depicted in Figure\u00a05B. Notably, models employing the RMT outperformed all other GENA-LMs, including those sparse variants of GENA-LM. While these sparse models can accommodate the input lengths featured in the aforementioned tasks, they have fewer parameters compared with gena-lm-bert-large-t2t. Thus, RMT allows combining models with the higher number of parameters and longer sequence inputs, achieving the best performance on the common biological tasks. Furthermore, RMT has no limit in a sequence length and could be used for even longer sequences. Sparse GENA-LMs, on the other hand, are limited to the lengths on which they were trained.",
        "We next used splice sites and promoter activity prediction tasks to benchmark the effects of the RMT application during pre-training. This benchmark yields mixed results: for promoter activity prediction, limiting the RMT approach to the fine-tuning stage does not diminish performance compared with its use at both the pre-training and fine-tuning stages. However, for splice site annotation, pre-training with RMT proved beneficial, improving performance by \u223c0.5 points.",
        "Recently, the HyenaDNA team introduced a specific benchmark for DNA language models that process long input sequences. The authors demonstrated that HyenaDNA can classify five mammalian species (human, mouse, lemur, pig and hippo) based on their genomic sequences. Compared with the classification of a broader range of species using GENA-LM embeddings, as previously described, the species in this benchmark are phylogenetically closer, making classification more challenging. Original research illustrated that classification accuracy is heavily dependent on the input DNA length, which increases gradually from 61.1 to 99.84 as sequence length scales from 1\u00a0kb to 1 Mb.",
        "We compared HyenaDNA results with gena-lm-bert-base-t2t augmented with RMT. For sequence inputs of 1\u00a0kb, the classification accuracy of both models was relatively low (61.45 \u00b1 0.91 for GENA and 61.1 for HyenaDNA). A significant enhancement in classification accuracy was observed when the sequence length was increased to 32\u00a0kb, with rmt+gena-lm-bert-base-t2t achieving 99.24 \u00b1 0.06, thereby surpassing the performance of HyenaDNA (93.4), as shown in Figure\u00a05 (right panel). Further extending the sequence length to 50\u00a0kb elevated the classification accuracy of rmt+gena-lm-bert-base-t2t to 99.67 \u00b1 0.059, exceeding the accuracy HyenaDNA attained with 1000\u00a0kb sequences. This indicates that, within this experimental framework, RMT and the associated model architecture extract and leverage information from extended DNA sequences more efficiently than other technologies designed for processing long input sequences such as Hyena layers underlying HyenaDNA.",
        "Transformer architectures have garnered significant interest across diverse research domains, including genomics. They consistently achieve exemplary results in various biological tasks such as deciphering gene expression regulation in mammals and Escherichia coli, predicting phenotypes from gene expression, deducing DNA methylation and filling in missing genotypes, to name a few. Nevertheless, the challenge lies in training task-specific models for each distinct biological question. This process demands substantial time and resources. DNABERT, DNABERT-2 and similar foundational DNA models such as BigBird and Nucleotide Transformer provide a solution by offering a platform for refining universally applicable models without starting from scratch. The Nucleotide Transformer v2\u00a0 has incorporated rotary embeddings and gated linear units paired with swish activations, distinguishing it from its predecessor. This model has an input size of 12\u00a0kb. DNABERT\u00a0v2, while drawing upon the foundational DNABERT architecture, expands in terms of model parameters and employs BPE tokenization. However, its sequence input length remains below 4000\u00a0bp. Contrarily, HyenaDNA\u00a0 introduces a novel architecture capable of handling vast DNA sequences, extending up to 1 million base pairs. Yet, benchmark results suggest an inverse relationship between HyenaDNA\u2019s performance and the input size used during its training, as noted by. Moreover, our benchmarking GENA-LMs augmented with RMT against HyenaDNA in species classification task indicate better performance of the former model.",
        "A unique feature of HyenaDNA is its decoder-only configuration. Unlike the encoder-centric GENA-LMs, HyenaDNA does not generate sequence embeddings directly. Instead, it produces DNA sequences, making the derivation of class labels (for classification purposes) or quantitative targets (for regression) from its outputs a complex task. To predict specific DNA states with the HyenaDNA model, the authors utilized a DNA-alphabet encoding, obliging the model to understand this biologically unrelated nucleotide sequence interpretation.",
        "We introduce GENA-LMs, a collection of open-source models boasting the most extensive input capacity among all accessible DNA transformers (for models starting with the gena-lm- prefix, visit https://huggingface.co/AIRI-Institute/) (10). The GENA-LM collection encompasses a spectrum of publicly accessible architectures, catering to researchers by offering tailored solutions for unique challenges. Moreover, GENA-LMs include several taxon-specific models that can improve performance in species-specific setups. Our rigorous benchmarking affirms that GENA-LMs not only surpass earlier pre-trained models but occasionally even rival the precision of task-specific convolutional neural networks.",
        "In our comparison of various GENA-LMs, we investigated the influence of context length and the total number of model parameters on predictive accuracy. We found that the optimal balance between these two factors varies depending on the specific task. For instance, an extended context is vital for predicting promoter activity or deciphering widespread HM distributions, as previously indicated by. However, for certain tasks, a more concise context is adequate, making it more advantageous to augment the model\u2019s parameter count. The broad spectrum of GENA-LMs available offers researchers the flexibility to select a model best suited for their particular objective.",
        "While GENA-LMs accommodate extensive input sizes, they occasionally fall short of the lengths required for peak accuracy in specific biological tasks. For example, research has shown that gene expression can be influenced by variants situated hundreds or even millions of base pairs distant from the promoter. This can be attributed to processes such as loop extrusion and other 3D-genomic mechanisms. There are several strategies to address this constraint in GENA.",
        "First, the RMT technique facilitates the processing of extensive sequence inputs using powerful models with a large number of parameters. Our benchmarks reveal that this approach delivers superior results for tasks where the biological signal spans a lengthy context. Notably, unlike transformer layers that exhibit a quadratic memory dependence on the number of tokens, the computational resources needed for RMT training and inference scale linearly with sequence length. RMT can be integrated with GENA-LMs not only during the fine-tuning phase of downstream tasks but also throughout the MLM pre-training stage. This could enhance learning operations on extended sequences, particularly for downstream task datasets that are of limited size. Furthermore, RMT models pre-trained on multiple segments can be utilized for a greater number of segments during inference. As such, RMT models are versatile enough to address a variety of downstream tasks, even for teams without access to cutting-edge computational infrastructure.",
        "Second, the 3D proximity of chromatin can be determined using specialized models. This information can then be directly incorporated into transformer models, enabling them to capture long-range associations between functional genomic elements.",
        "One limitation of GENA-LMs arises from the granularity imposed by the use of BPE tokenization, which confines predictions to specific tokens. To overcome this, exploring alternative DNA tokenization methods and developing low-level nucleotide embeddings could offer solutions for certain applications.",
        "Beyond merely predicting specific biological signals, we demonstrate that GENA-LMs can also be harnessed to decipher and understand the functions of sequences underpinning these signals. An analysis of token importance revealed that GENA-LM accurately detected motifs corresponding to known TFs. Furthermore, it pinpointed TF binding sites crucial for specific HMs. There exists an array of factors that modify histones, termed histone \u2018writers\u2019, many of which are cell-type specific. Determining these factors and their corresponding genomic binding sites is a complex endeavor. In this context, we illustrate how GENA-LMs can aid in this task by discerning motifs deemed \u2018essential\u2019 for a specific HM within a particular cell type. However, it is pivotal to approach this method judiciously. The presence of enriched motifs may only indicate an association rather than a direct causal relationship. As an instance, while the enrichment of recognized activator factor motifs within H3K4me3-important tokens aligns with the understood biological roles of these factors, the presence of motifs specific to neural or dermal TFs within H3K27me3-important tokens in lymphoid K562 cells likely does not signify a direct causal role of these proteins in establishing the repressive H3K27me3 mark. We posit that these factors\u2019 targets were suppressed in blood lineage progenitors, implying that the enrichment of their motifs is a reflection of the developmental trajectory of these cells.",
        "To sum up, our study provides compelling evidence that large language models trained on DNA sequences have the capability to generate useful biological insights. This not only presents an innovative method for solving an array of genomic challenges but also forges a pathway for a more nuanced understanding of genetic data. The transformative impact of language models has already been witnessed in protein biology, where they have brought about remarkable progress in predicting protein properties and engineering novel peptides with tailored functions. This is indicative of the potential these models hold, suggesting that their capabilities go beyond mere sequence analysis. With the exponential increase in multi-omics data\u2014spanning genomics, transcriptomics, proteomics and metabolomics\u2014it is imperative to have advanced analytical tools that can seamlessly integrate and interpret these vast and complex datasets. Language models, as demonstrated by our findings, appear poised to fill this role. As the nexus between computational techniques and biology strengthens, it is foreseeable that language models will be pivotal in ushering in a new era of DNA-based technologies."
    ],
    "title": "GENA-LM: a family of open-source foundational DNA language models for long sequences"
}