{
    "content": [
        "An increasing number of studies have reported using natural language processing (NLP) to assist observational research by extracting clinical information from electronic health records (EHRs). Currently, no standardized reporting guidelines for NLP\u2010assisted observational studies exist. The absence of detailed reporting guidelines may create ambiguity in the use of NLP\u2010derived content, knowledge gaps in the current research reporting practices, and reproducibility challenges. To address these issues, we conducted a scoping review of NLP\u2010assisted observational clinical studies and examined their reporting practices, focusing on NLP methodology and evaluation. Through our investigation, we discovered a high variation regarding the reporting practices, such as inconsistent use of references for measurement studies, variation in the\u00a0reporting location (reference, appendix, and manuscript), and different granularity of NLP methodology and evaluation details. To promote the wide adoption and utilization of NLP solutions in clinical research, we outline several perspectives that align with the six principles released by the World Health Organization (WHO) that guide the ethical use of artificial intelligence for health.",
        " WHAT IS THE CURRENT KNOWLEDGE ON THE TOPIC? ",
        " WHAT QUESTION DID THIS STUDY ADDRESS? ",
        "An increasing number of natural language processing (NLP) applications have been leveraged to assist observational research.  ",
        " WHAT DOES THIS STUDY ADD TO OUR KNOWLEDGE? ",
        "To understand the reporting practices in NLP\u2010assisted observational studies.   ",
        " HOW MIGHT THIS CHANGE CLINICAL PHARMACOLOGY OR TRANSLATIONAL SCIENCE? ",
        "The study reviews reporting practice of NLP\u2010assisted observational studies and discusses the lessons learned from existing evaluation and reporting practices.  ",
        "The study emphasizes the importance of reproducibility and scientific rigor when using NLP for clinical research. It also defines recommendations to ensure translation into practice.",
        "Advancement in the digital transformation of health care has reshaped the documentation, representation, and management of electronic health records (EHRs). This transformation increases the need for innovative informatics solutions for facilitating and accelerating the secondary use of EHRs for clinical research defined as patient\u2010oriented research, epidemiological and behavioral studies, or outcomes and health services research. Observational research is a common type of clinical research that measures outcomes or observes individuals following cohort, cross\u2010sectional, or case\u2010control research study design. Not all EHR data are represented in structured format, so it is necessary in many observational studies to leverage unstructured data to comprehensively capture the \u201ccomplete\u201d representation of patient profiles. Natural language processing (NLP) has been leveraged to assist chart review by automatically extracting clinical information from unstructured text. For example, in our previous study, we assessed the agreement among International Classification of Diseases (ICD) codes, flowsheets, and clinical notes within the EHR documentation of patients with delirium. The unstructured clinical notes and semi\u2010structured flowsheet data can improve phenotyping sensitivity by approximately 20% over a purely structured data (ICD\u2010based) approach. Similar results were achieved with incidental findings, diseases with multifactorial causes, diseases with no singular and conclusive diagnostic tests, or surgical information. Furthermore, to maximize the detection accuracy, studies have leveraged the combination of both structured (e.g., laboratory and medication) and unstructured data to determine patient's disease status, such as silent brain infarction, type 2 diabetes mellitus, and rheumatoid arthritis. These findings strongly suggest that text information can improve the detection of conditions such as rare diseases, which are not routinely coded and/or are underdiagnosed in clinical practice. ",
        "There are many approaches to the derivation of knowledge from EHRs for conducting observational research, one of which is chart review, a common, manual process of extracting or reviewing information from EHRs and assembling a data set for various research needs, such as case ascertainment, data element extraction, risk stratification, and case matching. Because a significant portion of clinical information is represented in textual format, execution of such a human\u2010operated approach is time\u2010consuming, non\u2010standardized, and not very scalable. The availability of digital information demands systematic solutions for high\u2010throughput knowledge extraction. As illustrated in Figure\u00a01, NLP can be integrated into the research workflow for observational studies by providing information extraction and knowledge conversion at multiple stages of study implementation. Such a scalable solution promotes a new era of research that would not have been feasible decades ago. As a result, an increasing number of observational studies from disease study areas to drug\u2010related studies benefit from state\u2010of\u2010the\u2010art NLP solutions.",
        "Despite the notable advantages of NLP\u2010assisted observational research, there are remaining challenges \u2013 one of which is the inconsistency between community standards in observational research and the standards in clinical NLP communities. In the clinical NLP community, studies that focus on the development and evaluation of NLP solutions are commonly referred to as measurement studies, defined as research projects designed to develop and refine methods for determining how much error of the targeted attribute can be measured in a population of objects. For example, Wyles et al. conducted a measurement study to examine the accuracy of two NLP systems for the identification of common data elements in operative notes for total joint registry. Wang et al. evaluated a comprehensive NLP algorithm for extracting lung cancer data elements, including stage, histology, tumor grade, and therapies. McCoy et al. developed an NLP\u2010based phenotype algorithm to extract five different cognitive and psychopathologic symptoms. Such study involved iterative processes to establish gold standard corpora \u2013 the benchmark language resources for the development, evaluation, and deployment of NLP methods. In observational studies, NLP is typically viewed as a specific measurement for patient characteristics through EHR data. For example, through applying the previously validated NLP algorithm for estimating cognitive symptomatology, McCoy et al. conducted a survival analysis to examine the association of cognitive symptoms with incidence of dementia diagnosis. Similarly, Kent et al. designed a study to estimate the incidence of future stroke in patients with incidentally discovered silent cerebrovascular disease (SCD). In this study, NLP was used to measure patients\u2019 status of SCD from neuroimaging reports.",
        "The quality of such a measure can potentially impact the validity, reproducibility, and explainability of the research outcome. As indicated by the current version of STrengthening the Reporting of OBservational studies in Epidemiology (STROBE) reporting criteria, there is, however, no standardized reporting guideline for NLP\u2010assisted observational studies as of today. The absence of detailed reporting guidelines can create ambiguity and a lack of reproducibility in current research reporting practices (e.g., not defining the original cohort used in measurement study when using NLP in an observational study). This issue is particularly important for NLP applications due to their methodology complexity. For example, most NLP applications contain (i) a generic text processor (i.e., NLP framework) that is usually comprised of a set of information extraction pipelines, such as sector detector, sentence breaker, tokenizer, part\u2010of\u2010speech tagger, chunk annotator, information extractor, and context annotator and (ii) task\u2010specific NLP algorithms (e.g., heart failure, stroke, and breast cancer). In general, the text processor serves as a generic pipeline to pre\u2010process clinical documents into a standard representation before disease\u2010specific information extraction occurs. Well known generic text processors include MedLEE, MetaMap, KnowledgeMap, cTAKES, HiTEX, and MedTaggerIE. In addition to that, an NLP algorithm is often referred to as a specific set of rules (rule\u2010based or statistical) that will be used to solve one or many defined problems. These algorithms are defined by specific NLP tasks and thus are closely dependent on various contextual factors, such as disease types, domain experts, EHR data, and evaluation environments. Because the final NLP output is dependent on the aggregated performance of all prior text processing components and algorithms, non\u2010explicit and ambiguous reporting practices can cause unintended data reproducibility issues.",
        "Considering NLP is becoming a vital part of EHR\u2010based observational research and closely affects overall research reproducibility and scientific rigor, it is important to have a systematic understanding of the existing reporting practices such as the metadata, degree of granularity, and confidence of NLP methodology and evaluation in observational studies. In response to that, we conducted the scoping review to examine the manuscript reporting of NLP\u2010specific methodological standards and evaluation processes in observational studies to provide potential evidence for strengthening the future reporting of NLP\u2010assisted observational research. We reviewed all studies published from 2009 to 2021 that met the inclusion criteria. Based on our findings, we propose recommendations for the future development of NLP research standards for the clinical and translational science (CTS) community.",
        " Why the review should be conducted: existing studies focus on NLP methodologies and evaluations in measurement studies. Little\u2010to\u2010no investigation on the methodologic standards and evaluation practices has been conducted when NLP is applied to support observational studies.",
        " What the review will add to the reader's knowledge in the field: understanding current reporting variability, lessons learned from NLP\u2010assisted observational studies, and indications of what a more standardized NLP\u2010reporting structure could or should look like.",
        "Are important NLP methodology and evaluation details reported?",
        "Where are they reported?",
        "What is the degree of granularity and confidence (e.g., data source definition, data abstraction and annotation process, NLP pre\u2010processing pipeline, NLP method, evaluation statistics)?",
        "What is the current reporting practice of the NLP component in the NLP\u2010assisted observational research?  ",
        "What are the potential barriers to adopting NLP for observational research?",
        "What are the lessons learned from the existing evaluation and reporting practices?",
        " What specifically is being investigated about the topic under investigation:  ",
        "The review was conducted following the scoping review guide proposed by Pham et al. The initial process includes identification of key research questions and defining the scope (population) of research.  ",
        "An experienced librarian (LP) retrieved the studies written in English and published from January 2009 through September 2021 from Ovid MEDLINE In\u2010Process & Other Non\u2010Indexed Citations, Ovid MEDLINE, Ovid EMBASE, Scopus, and Web of Science. The implementation of search patterns was consistent across different databases. Detailed search strategies are provided in Supplementary Material S1. A total of 1347 articles were retrieved from five libraries and after de\u2010duplication, 471 articles remained for evaluation.",
        "The title and abstract screenings were conducted by two reviewers (authors S.F. and R.R.) from the previously identified 471 articles. The main objective of the screening was to assess and identify observational studies that focused on the subsequent use of the NLP solutions to address clinical questions. In addition, the studies needed to comply with the standard design of the EHR\u2010based observational research method. A total of 390 articles were then removed during the screening process. Among them, we excluded 205 measurement studies of NLP that concentrated on developing and refining methods for making measurements. We also removed 161 non\u2010observational studies, 21 studies without NLP applications, and three studies with non\u2010English EHR data. After that, 81 studies underwent full\u2010text review, performed by five reviewers (authors S.F., R.R., N.Z., L.W., and S.M.). The full\u2010text review further excluded 31 articles. Among them, seven studies were measurement design, five studies have no description of NLP in the full text, 15 studies did not comply with the standard design of the EHR\u2010based observational research method, three studies had no full text or abstract, and one study was non\u2010peer reviewed.",
        "To assess the reporting, methodologic, and evaluation standards of the identified articles, we considered Findable, Accessible, Interoperable, and Reusable (FAIR) and Reproducible, Implementable, Transparent, and Explainable (RITE) principles (Figure\u00a02). The FAIR data principles define the best practices of data stewardship and dissemination through emphasizing four criteria: findable, accessible, interoperable, and reusable. After translating the FAIR principles into the use case of clinical NLP, the provenance of information resources for developing NLP applications include corpus location, corpus sharing criteria, metadata of corpus, definition of corpus (i.e., annotation guideline), and standard (semantic and syntactic). In addition to FAIR, the RITE implementation principles place an emphasis on process transparency and implementability, because the variability and explainability of the result are dependent on the process. Even in a situation that does not require the process or results to be replicated, all important steps and details need to be documented and made available to ensure the traceability of the process and explainability of the result. This view implies the importance of the documentation and reporting of methodology (i.e., processes of NLP evaluation and rule refinement) and implementation (i.e., accessibility of key information resources for system deployment) details. We also considered the evaluation principles suggested by Friedman et al. to guide the reporting practice: (1) the study team is obligated to identify the best possible way of measuring a standard given the research context and conduct measurement studies to estimate the error; and (2) the error estimated from the measurement studies need to be incorporated into analyses (observational study).",
        "The key data elements that needed to be extracted include study metadata, NLP methodology (e.g., context, certainty, and normalization), gold standard development and evaluation (e.g., evaluation environment and sampling strategy), and overall reporting practice. Three categories (retrospective cohort, cross\u2010sectional, and case\u2010control) of study design were included based on Man's definition. Based on the research question, study hypothesis, and clinical perspectives, we summarized the studies into five application domains: disease, drug, risk factors, social determinant of health, and other categories. The NLP methodology and evaluation\u2010related data elements and definitions are provided in Table\u00a01.",
        "To ensure quality, we performed consensus development during the article screening and data abstraction. During the consensus development phase, two batches of articles were randomly sampled from the screening pool. Each batch contains 20 articles. Two reviewers (authors S.F. and R.R.) annotated the same set of articles. After each round, disagreements were measured using kappa statistics. The agreement scores in the two rounds were 0.573 (confidence interval [CI] 0.210\u20130.936) and 0.700 (CI 0.389\u20131.000), respectively. Two consensus meetings were organized to resolve the disagreements. During the meeting, the same reviewers (authors S.F. and R.R.) present the cases with disagreements to two adjudicators (authors H.L. and A.W.). Each case was discussed for reaching a consensus. In a rare situation when a consensus cannot be reached, the adjudicators provided the final decision. The full\u2010text data abstraction was performed by five reviewers (authors S.F., R.R., N.Z., L.W., and S.M.). During the data abstraction, two reviewers (authors S.F. and L.W.) verified the abstraction quality of other reviewers by randomly sampling 40% of the data. Conflicting results were discussed and adjudicated between the primary reviewer and the validation reviewer. The abstracted data from the included studies were charted using frequencies of the following variables: year of publication, study period, geographic region of conduct, and study design. Descriptive statistics were used in the analysis of the data collected in this study.",
        "Among the total 471 articles after the de\u2010duplication, we identified 205 (44%) NLP\u2010related measurement studies and 161 (34%) non\u2010observational studies without NLP methodology to be excluded based on title and abstract screening. After the screening process, 81 articles were considered for full text review, of which we identified 50 articles for a comprehensive full\u2010text review and subsequent data abstraction. A flow chart of this article selection process is shown in Figure\u00a03.",
        "Based on the comparison of our previous review on clinical NLP methodologies (Figure\u00a04a), we observed an upward trend in published research on NLP use for observational research, especially within the past 5\u2009years. This finding not only affirms the need for leveraging NLP to extract important clinical information from EHRs, but also suggests an increasing number of translational efforts in applying NLP solutions in the context of clinical research. Among these methods being translated into clinical research, rule\u2010based methodology was the most reported in the demonstration studies. On the other hand, although we observed an upward trend in the published NLP\u2010assisted observational studies, compared with the distribution of overall EHR\u2010based observational research (Figure\u00a04b), utilization of NLP is still low. Since the HITECH Act of 2008, the growth of observational studies leveraging EHR data was exponential. However, such a pattern was not observed for NLP\u2010assisted studies, despite the plethora of NLP methods papers. This number suggested an implementation gap between traditional NLP research and NLP\u2010assisted observational research.",
        "The summaries of primary study design, application domain, and disease classification are provided in Figure\u00a05. Among the 50 studies, the primary research design was retrospective cohort (66%), followed by cross\u2010sectional (28%) and case\u2010control (6%; Figure\u00a05a). We identified that 24 (48%) studies used NLP to extract risk factors, 21 (42%) studies extracted outcome, two (4%) studies extracted exposure, and three (6%) studies applied NLP for extracting multiple types of research variables. The study distribution of disease, drug, other categories, risk factors, and social determinant of health was 41 (82%), 4 (8%), 2(4%), 2 (4%), and 1 (2%), respectively (Figure\u00a05b). The \u201cDiseases\u201d sub\u2010domain was further classified based on the ICD\u201010 classification system. Top classification systems include Mental, Behavioral, and Neurodevelopmental disorders (20%), Diseases of the genitourinary system (8%), Diseases of the respiratory system (8%), and Neoplasms (8%; Figure\u00a05c).",
        "Among the 50 surveyed articles, the rule\u2010based method (72%) was the most reported NLP method, followed by statistical machine learning (12%), and hybrid (6%; Figure\u00a06a). Five (10%) studies did not specify the NLP method used. Compared with the previous study reviewing NLP measurement studies that reported using the rule\u2010based approach, utilization of this approach increased from 48% to 72% among the demonstration studies. Figure\u00a06b presents the reporting adherence of NLP methodologic definitions. Fifty\u2010eight percent of the studies did not report model definition (e.g., lexicon, dictionary, code, etc.), 74% of the studies did not report normalization techniques (e.g., UMLS, SNOMED CT), and 58% of the studies did not report context definition (e.g., certainty, patient status [historical vs. present], experiencer [patient vs. family member]). Only six (12%) of the studies reported all three definitions. In summary, the reporting of NLP methodology needs to be enhanced for demonstrating method reproducibility and confidence in technical soundness.",
        "In assessing the utilization and reporting of NLP\u2010related evaluation methodologic standards (Figure\u00a06c\u2013f), most studies (66%) conducted the evaluation in a single EHR environment. Eleven (22%) of the studies involved multiple EHRs, one (2%) study utilized benchmark datasets, one (2%) study utilized both EHRs and benchmark datasets, three (6%) studies utilized the VA environment, and one (2%) study did not report evaluation environment. In addition, we discovered that 17 (34%) studies reported the use of sampling methods for establishing the evaluation cohort and preparing gold standard datasets. Furthermore, 25 (50%) studies reported the definition of the level of evaluation. Among these studies, 12 (24%) reported that evaluations were conducted at patient level, 12 (24%) at document level, and one (2%) at concept level. The median, minimal, and maximal evaluation size was 40, 716, and 2000 at document level and 18, 245, and 1042 at patient level, respectively (Figure\u00a06d). Evaluation metrics, such as sensitivity and specificity, quantify the performance of NLP and can provide a direct estimate of the validity models. Based on the review, 14 (28%) studies did not report evaluation metrics, 11 (22%) studies reported one metric, 11 (22%) reported two metrics, and 14 (28%) reported more than two. The most\u2010reported metric was sensitivity, followed by precision, specificity, and f1\u2010score (Figure\u00a06e). Post evaluation refinement is an important process for optimizing the NLP before using it in an observational study. We observed that 12 (24%) studies reported that NLP rules and keywords were iteratively revised prior to being used for research analyses.",
        "Among the total 50 studies, 18 (36%) studies reported NLP evaluation in the body of the manuscript, nine (18%) provided the reference of a prior study, two (4%) reported in the supplementary appendix, nine (18%) reported in the body of the manuscript and provided reference, four (8%) reported in the body of the manuscript and supplementary appendix, one (2%) provided reference and appendix, and seven (14%) did not report NLP evaluation (Figure\u00a06g).",
        "To better understand the current translation effort of NLP applications to clinical research, we conducted a scoping review on NLP\u2010assisted observational studies and examined (1) whether important NLP methodology and evaluation details are reported; (2) where they are reported, and (3) the degree of granularity and confidence with which they are reported. Through our investigation, we discovered a high variation in reporting practices, such as missing details about measurement studies and inconsistent reporting of the location and granularity of NLP methodology and evaluation details. Fourteen percent of studies did not report NLP methodology and evaluation, 22% of studies did not report evaluation design, and 10% of studies did not report NLP methodology. A few studies claimed that \u201c\u2018NLP has been evaluated prior to release,\u201d however, no additional details could be found to justify the validity of the NLP results and clinical findings. In addition, among the 14 studies with both measurement and demonstration studies reported, we found no study evaluated the difference between the two environments. As suggested by our previous investigation, if there is a substantial variation in the study setting, EHR environment, or cohort definition between measurement study and observational study, NLP may suffer from a portability issue. This issue could also occur within a single institution due to EHR system migration, cohort definition, change of definition, and variation in the extract, transform, and load (ETL) process.",
        "We further observed ambiguity and inconsistency when authors used the terms \u201cframework,\u201d \u201csystem,\u201d \u201ctool,\u201d and \u201calgorithm.\u201d We observed that only 42% of the studies provided definitions of the NLP algorithm, and 18% of studies provided the reference of a prior measurement study. The lack of direct access to the original text processors, machine learning models, and pattern definitions may create substantial reproducibility challenges and may impact clinical validity. These findings highlight the strong need for more community efforts, such as partnering with international initiatives such as EQUATOR Network to establish reporting standards for observational studies empowered by NLP methods. As an initial effort, we emphasize several crucial points that need to be carefully considered when conducting NLP\u2010assisted observational studies (Table\u00a02).",
        "One potential barrier to the wide adoption of NLP may be due to the high\u2010stake nature of clinical research because findings could be translated into routine care delivery through practice\u2010based knowledge discovery and evidence\u2010based medicine. Based on our review, we discovered that most data elements extracted by NLP applications are either risk factors (48%) or outcomes (42%), which are the primary variables used for the statistical analyses for discovering associations. Consequently, invalid NLP results will cause systematic bias, measurement error, and misclassification, which can ultimately impact the overall validity of any produced research and subsequent clinical guidelines derived from these studies. Therefore, when translating and applying NLP solutions to observational studies, the process must require necessary assessment and evaluation efforts for mitigating various quality issues during the real\u2010world implementation within the EHR environment to ensure research reproducibility and scientific rigor. To align with the six guiding principles (human autonomy, human safety, transparency, explainability and intelligibility, responsibility and accountability, inclusiveness and equity, and responsive and sustainable) released by the World Health Organization (WHO) to guide the ethical use of artificial intelligence for health, we outlined two key perspectives to promote the wide adoption and utilization of NLP solutions in clinical research in the following discussion sections.",
        "Reproducibility is crucial to NLP\u2010assisted observational research because the validity of NLP models is dependent on the data from which they are derived. Solid data understanding and documentation can promote a good data curation plan and solutions for mitigating potential biases or confounders prior to model development and re\u2010deployment. From the implementation perspective (RITE principle), deploying NLP for a real\u2010world research application would require substantial translational efforts (e.g., rigorous evaluation, optimization, and dissemination) to ensure the model (1) is successfully deployed and executed and (2) can produce valid and explainable outcomes. According to the review, the utilization of the rule\u2010based method (72%) for observational research was much higher than all other reported methods combined. The unique characteristic of the rule\u2010based approach is that the lexicons and rule patterns are interpretable and able to be modified easily. On the other hand, no deep learning methods were reported in the observational research studies. Similarly, despite the recently increasing adoption of deep learning methods for NLP in the general domain, we discovered a delay of such application in the cancer domain in another scoping review for cancer NLP. Such findings point out potential misaligned research objectives between translational research and informatics research \u2013 translational research emphasizes quality\u2010driven objectives (e.g., practicality, customizability, and explainability) and informatics research focuses more on methodological innovation (i.e., sophisticated deep learning methods). To better bridge this gap, it is important for us to have a stronger engagement in translational research as well as community\u2010wide interdisciplinary collaboration.",
        "Open science carries the criteria of transparency, openness, and reproducibility, which are recognized as vital features of science. NLP technologies, on the other hand, have a strong presence in both commercial industry and academic research. Consequently, the protection of intellectual property may prevent disclosing enough details about NLP solutions (e.g., methodology and evaluation) and become a potential barrier to open\u2010science collaboration and knowledge sharing. When non\u2010open and non\u2010cross\u2010validated NLP solutions are leveraged in clinical research, outcomes could potentially face limited utilization, external validation, and result in generalizability and trust issues. In contrast, open solutions can promote collaborative, and trustworthy environments for NLP development, evaluation, and deployment, foster partnership with diverse communities such as the National COVID Cohort Collaborative (N3C), PCORnet, Observational Health Data Sciences and Informatics (OHDSI), and Precision Medicine Initiative (PMI), and bring positive impacts to future workforce development. Therefore, there is a strong need for open collaboration across different disciplines and among the CTSA community. There are some early efforts for promoting open NLP collaboration including the National Center for Advancing Translational Sciences (NCATS) funded national consortium Open Health Natural Language Processing (OHNLP) consortium and National Center for Data to Health (CD2H) funded federated evaluation platform NLP Sandbox. ",
        "In 2010, the National Cancer Institute (NCI) published its first field guide entitled Collaboration and Team Science to foster the education, practice, and professional development of team science and interdisciplinary research. Due to the increasing specialization and complexity of research methods and domain expertise, there is a strong need for adopting collaborative approaches among scientists across discipline\u2010based fields. The development, evaluation, and application of NLP for observational research is an example of team science collaboration (TSC) across the disciplines of biomedical informatics, medicine, statistics, and epidemiology. For example, the development and evaluation of a research cohort is a process of operationalizing clinical definition to EHR data. This process requires extensive collaboration and communication among clinical research scientists and informaticians to ensure the validity and reliability of EHR\u2010based measurements (e.g., ICD, CPT, and LOINC). Furthermore, developing gold standard resources requires expertise in clinical domain knowledge, data, infrastructure, and model. Failure to adhere to the TSC approach can affect the robustness of NLP models and the validity of research outcomes. This issue can be illustrated by not engaging domain experts and scientists when drawing sample populations, matching control cases, estimating sample size, conducting corpus annotation, etc.",
        "The study does have some limitations. First, the review is limited by the defined scope. Studies which had relevant content but did not meet the inclusion and exclusion criteria were not included (e.g., written in non\u2010English). In addition, the study could miss relevant articles due to the search strings and databases selected in the review. Finally, abstraction tasks themselves are often subjective in nature, and manual data abstraction may therefore introduce data quality issues. Supporting this is the fact that the first round of inter\u2010abstractor agreement only had a kappa of 0.57. This moderate agreement suggests that several challenges were encountered when differentiating NLP related measurement studies and demonstration studies during the article screening process. Despite adjudication of any disagreements, misclassification is still possible. Our results should therefore be better viewed as an exploratory investigation, and we hope that this study can motivate future studies in a similar scope to provide additional evidence.",
        "As NLP solutions are increasingly integrated into the research workflow for observational studies, the validity and reproducibility of these solutions take on high importance. Our study reviewed the reporting of NLP\u2010specific methodological standards and evaluation processes in EHR\u2010based observational studies. Our assessment reveals a high variation in the reporting practices, such as inconsistent use of references for measurement studies, variable reporting locations, and differing granularity of NLP methodology and evaluation details, which entails the need for continuously improving research reproducibility and scientific rigor for EHR\u2010based observational studies. These critical issues must be addressed through fostering partnerships with clinical NLP and clinical and translational science communities through promotion of collaborative and transparent environments for the development, evaluation, and deployment of NLP solutions."
    ],
    "title": "Recommended practices and ethical considerations for natural language processing\u2010assisted observational research: A scoping review"
}