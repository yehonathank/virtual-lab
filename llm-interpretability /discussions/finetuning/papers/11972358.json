{
    "content": [
        "Parkinson\u2019s disease (PD) is a neurodegenerative disorder affecting motor control, leading to symptoms such as tremors and stiffness. Early diagnosis is essential for effective treatment, but traditional methods are often time-consuming and expensive. This study leverages Artificial Intelligence (AI) and Machine Learning (ML) techniques, using voice analysis to detect early signs of PD. We applied a hybrid model combining Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), Multiple Kernel Learning (MKL), and Multilayer Perceptron (MLP) to a dataset of 81 voice recordings. Acoustic features such as Mel-Frequency Cepstral Coefficients (MFCCs), jitter, and shimmer were analyzed. The model achieved 91.11% accuracy, 92.50% recall, 89.84% precision, 91.13% F1 score, and an area-under-the-curve (AUC) of 0.9125. SHapley Additive exPlanations (SHAP) provided data explainability, identifying key features driving the PD diagnosis, thus enhancing AI interpretability and trustability. Furthermore, a probability-based scoring system was developed to enable PD patients and clinicians to track disease progression. This AI-driven approach offers a non-invasive, cost-effective, and rapid tool for early PD detection, facilitating personalized treatment through vocal biomarkers.",
        "The online version contains supplementary material available at 10.1038/s41598-025-96575-6.",
        "Parkinson\u2019s disease\u00a0(PD) is a disorder of the central nervous system. It causes unintentional and uncontrollable bodily movements such as shaking, stiffness, or difficulty with balance and control. PD is a neurodegenerative disorder, meaning the symptoms gradually worsen over time. Due to this, people suffering from PD may develop behavioral or mental changes such as depression or a decrease in memory. Currently, there is no cure for PD, but there are medications that can alleviate symptoms. Regardless, it is best to intervene and prevent the gradual onset of PD rather than treating it at its most vicious state. However, traditional diagnostic methods often rely on clinical evaluations and imaging techniques, which can be invasive, costly, and require specialized medical expertise. In recent years, the advent of AI has opened new opportunities for diagnosis, particularly through voice analysis. This paper explores the use of AI and ML techniques to diagnose early-stage PD by analyzing vocal characteristics. This study aims to develop an AI-powered, non-invasive, and cost-free PD screening tool using vocal biomarkers, allowing for early detection of PD before symptoms manifest. This diagnostic tool will primarily be a proof of concept that vocal data alone can train a model to diagnose PD. In our next phase, we will go into implementation science to translate our model to a production environment. Traditional PD diagnostic methods rely on costly imaging techniques and subjective physician assessments. Our work introduces a novel hybrid deep learning pipeline (MLP\u2009+\u2009CNN\u2009+\u2009RNN\u2009+\u2009MKL) and a data explainability framework (SHAP) to enhance clinical interpretability. Additionally, we propose a probability-based scoring system that allows for continuous monitoring of disease progression, a unique feature absent in many diagnostic settings.",
        "Recent advancements in AI and ML have demonstrated significant potential in diagnosing Parkinson\u2019s disease using voice analysis. Various studies have utilized the extracted acoustic features of voice recordings to distinguish between healthy individuals and individuals with PD. While traditional statistical methods have been employed, the field is rapidly evolving towards the use of deep learning techniques that automatically extract relevant features from raw voice data.",
        "Little et al. used support vector machines (SVM) to classify voice recordings of PD patients with an accuracy of 91.4%, establishing themselves as one of the first pioneers in this field. Their study demonstrated the viability of using acoustic voice features for PD diagnosis and laid the groundwork for further research. However, this study lacked MFCCs, which are instrumental for projects using voice to diagnose PD. This paper will incorporate MFCCs alongside traditional acoustic features to ensure a thorough diagnosis. Building on this, Tsanas et al. developed a decision support system using MKL to replicate the unified Parkinson\u2019s disease rating scale (which requires the patient\u2019s presence in the clinic) remotely. Their approach underscored the importance of integrating multiple learning features and robust ML techniques when transitioning to noninvasive and self-administered PD tests. More recent studies, however, focus on deep learning models\u2014automatic extraction of relevant features from raw voice data. For example, Alhanai et al. employed a Long-Short Term Memory (LSTM) neural network to analyze speech patterns with an 89% accuracy in detecting early PD symptoms. Similarly, Alissa et al. used a CNN to extract and analyze voice features, achieving a diagnosis accuracy of 93.5%. These studies highlight a transition from traditional methods to more sophisticated AI models. The broader implications of deep learning models, specifically CNNs, have highlighted the significance of transitioning from traditional machine learning to deep learning methods to enhance diagnostic accuracy. Researchers have also explored using ensemble models like Boosted Decision Trees or XGBoost in Parkinson\u2019s contexts. These approaches have shown promising results, with accuracy rates ranging from 90 to 95%. These types of gradient-boosted models have been found to outperform some Random Forest and Logistic Regression models. These methods offer the potential for early, cost-effective PD diagnosis, addressing the challenges of traditional diagnostic approaches.",
        "Integrating voice analysis with other modalities, such as data from wearable devices, has shown promise in improving diagnostic accuracy. For example, Guo et al. demonstrated that combining voice data with other physiological signals improved their overall accuracy of PD diagnosis to around 96.06%. Aich et al. did something similar by pairing a machine-learning algorithm with wearable devices to track gait. They used AI to analyze statistical features and spatiotemporal gait features to reach an accuracy of 96.72%. Yang et al. developed an AI model to detect PD and track its progression from nocturnal breathing patterns. Their AI model can detect PD with an AUC of 0.90 and 0.85 on held-out and external test sets, respectively. Their study provides a non-invasive method of detecting and analyzing PD through sleep biomarkers. Apart from tracking external biomarkers, a very promising field of study is using AI to analyze protein expression. An approach done by Roshanbin et al. used an antibody-based positron emission tomography (PET) tracer for in vivo imaging of alpha-synuclein (aSYN). H\u00e4llqvist et al. also used a machine-learning approach to analyze the expression of eight different protein expressions. By looking at specific blood panels, they were able to indicate molecular events in the early stages and identify at-risk participants. Their model is able to predict PD 7 years before symptom onset. If voice were to participate in any of these multimodality-based studies, diagnostic accuracy would significantly increase due to various biomarkers being analyzed in conjunction with one another.",
        "However, this area of study is relatively novel, and researchers are still experimenting with ways to accurately combine voice and locomotive movement. Our paper only uses voice as data input, isolating the model\u2019s accuracy so it disregards any other biomarker. Removing confounding variables lets us properly gauge how important vocal biomarkers are for diagnosing PD.",
        "Historically, AI model applications in medical analysis have used decoupled model architectures. This means the model does not leverage multiple networks concurrently. A notable exception in recent literature is a pipeline AI model that uses SVM, adaboost classifier, and bagged random forest, as well as two different variants of deep learning model RNN known as LSTM and Bi-directional LSTM. Their model was specifically applied to analyze handwritings from patients with PD. In this paper, we will explore the performance of our novel pipeline model on vocal biomarkers, a different yet equally important domain for PD diagnosis.",
        "Explainable AI has been effectively implemented to explain various model outputs for diagnosing conditions like myocardial infarction (MI). Salih et al. applied Local Interpretable Model-agnostic Explanations (LIME) to 4 classification models and generated plots similar to Fig.\u00a04a. According to LIME, all four models agreed that high cholesterol, hypertension, and sex were the three most important factors determining an MI diagnosis, thus proving explainable AI to be reliable. Shinde et al. extracted radiomic features in order of their importance and plotted them against their corresponding f-scores. This method uses quantitative information extracted from diagnostic images. We used the same dataset Iyer et al. (2023) had used but we filled their experimentation gap they had acknowledged with difficulty in identifying feature importance in spectrogram images (they used CNN with transfer learning approach and had difficulty in determining the features importance but we were able to identify them using SHAP). We chose SHAP over other data explainability software like LIME because of the nature of our hybrid model. LIME excels over SHAP when analyzing a simple, standalone machine learning model with a straightforward structure. However, because our model integrates four different machine learning architectures, SHAP provides a more robust and consistent estimation of feature importance across multiple inputs, rather than relying on a local approximation like LIME. Furthermore, unlike LIME, which alters individual samples and builds a local surrogate model, SHAP assigns global feature attributions that remain stable across various predictions, ensuring a more reliable analysis of how different acoustic features contribute to PD diagnosis.",
        "We intend to use SHAP to effectively showcase the importance of each acoustic feature in our model\u2019s decision-making, thus removing the ambiguity of our AI\u2019s results. By transparently quantifying the impact of each feature, our model will promote greater trust among clinicians and patients regarding the AI diagnostic process, setting this project apart from less interpretable models.",
        "One major challenge is that while deep learning models have achieved high precision levels, most lack data explainability. This is particularly concerning in medical contexts where understanding the decision-making process of AI is crucial for gaining trust among healthcare professionals and patients. Furthermore, the generalizability of these models across diverse populations is limited because they are only trained on specific demographic groups\u2019 audio recordings. To enhance their robustness, there is a need for diverse datasets and training across various cohorts. \u201cToday, the much-needed personalization of medicine for PD patients still depends largely on the abilities, experience and intuition of treating physicians, nurses and allied healthcare professionals to adjust evidence-based medicine to individual decision making\u201d. This paper will utilize a large language model (LLM) to attempt to provide explainable AI that could personalize PD treatment.",
        "This section will report the model evaluation results. The primary metrics for model evaluation were accuracy and cross-entropy loss, which were assessed during both the training and validation phases. Accuracy indicates how well the model correctly predicts the inputted data\u2019s labels. A high accuracy indicates that the model can adequately distinguish between HC and PD recordings. On the other hand, low accuracy suggests a higher number of misclassifications. Cross-entropy loss measures how well or poorly the model\u2019s predictions match the actual labels during training. A high cross-entropy loss value (40% or higher) indicates the predictions significantly deviate from the actual labels. In contrast, a cross-entropy low loss value (20% or lower) shows the predictions are closely aligned with the actual labels. We utilized a five-fold CV in which the stratified data was split into 5 subsets. Each fold further trains the model on 4 subsets and validates it on the remaining subset. This process was repeated for each of the 5 folds, ensuring that every data point was used for training and validation to report a consistent average of evaluation indices. We also ensured that each subset of the data was used for validation precisely once. This approach mitigates the risk of overfitting and provides a more reliable estimate of the model\u2019s performance.",
        "The most optimal model was the MLP\u2009+\u2009CNN\u2009+\u2009RNN\u2009+\u2009MKL model. Its performance was evaluated based on accuracy, precision, recall, F1 score, and AUC metrics. Its average accuracy was 0.9111, indicating that around 91.11% \u00b1 1 of all predictions made with this model will be correct. For reference, a 2016 meta-analysis of 11 pathologic examinations (the gold standard for PD diagnosis) for PD had a pooled diagnostic accuracy of 80.6%. The precision was 0.8984, meaning roughly 89.84% \u00b1 1 of the positive predictions (Parkinson\u2019s disease) were correct. This high precision value indicates that the model is reliable when it predicts a patient with Parkinson\u2019s, as it produces a low rate of false positives. This low rate of false positives can save patients and hospitals resources by preventing healthy patients from testing positive for Parkinson\u2019s. It will also prevent false anxiety from being instilled within the patient. The recall was 0.9250, indicating that the model\u2019s ability to identify positive cases correctly was 92.50% \u00b1 0.5. The F1 score, which was 91.13% \u00b1 0.1, balances high precision and recall. This score reflects the model\u2019s ability to accurately identify Parkinson\u2019s patients while keeping false positives to a minimum. The MLP\u2009+\u2009CNN\u2009+\u2009RNN\u2009+\u2009MKL model outperforms all other models on every metric as seen in Fig.\u00a02a.",
        "The loss values of our champion model remained consistently low, as seen in Fig.\u00a01b. The loss value ranged from a high of 9.88% in Folds 4 and 5 to a low of 7.41% in Fold 3. The average loss value of the champion model is 8.89% \u00b1 1.",
        "Our champion model\u2019s consistency in both accuracy (Fig.\u00a01a) and cross-entropy loss (Fig.\u00a01b) shows that it is extremely good at predicting unseen data.",
        "The Receiver Operating Characteristic (ROC) shows the trade-off between the true positive rate (TPR) and false positive rate (FPR) for different machine learning models. The higher the AUC, the better the model is at distinguishing between positive and negative cases. The dashed line represents random chance (AUC\u2009=\u20090.5), and models performing above this line indicate better-than-random performance.",
        "Our champion model achieved an AUC value of 0.9125, as shown in Fig.\u00a03. This means it has strong discriminative power when distinguishing between individuals with Parkinson\u2019s disease and healthy individuals across different classification thresholds. This value is comparable to Iyer et al.. They used a CNN with transfer learning approach on the same 81 audio files we used. However, they did not report accuracy, recall, precision, or F1 score.",
        "The outcomes of the implemented scoring system demonstrate a distinct separation in the probability assessments for PD across the 81 analyzed audio samples. There is a clear demarcation of which files were considered HC and PD based on the system. For example, 40 of the 41 HC files scored between 0 and 0.30. However, File AH_678A_2E7AFA48-34C1-4DAD-A73C-95F7ABF6B138.wav, classified as HC, was assigned a higher score of 0.39. According to Table\u00a03, this file has a moderate likelihood of developing PD, suggesting that such a case would require careful monitoring in a clinical setting. Conversely, 38 of the 40 PD files scored between 0.70 and 0.90. Notably, the files AH_545812846-0C14B32A-6C50-4B62-BC89-0A815C2DEEFA.wav and AH_545880204-EE87D3E2-0D4C-4EAA-ACD7-C3F177AFF62F.wav registered scores of 0.69 and 0.62, respectively. Upon further analysis of the files scoring 0.39 and 0.62, their acoustic features closely resemble those of patients in the early stages of PD. This observation validates our scoring system by confirming that the vocal biomarkers in the audio files accurately correspond with their assigned scores.",
        "On the more definitive end, File AH_322A_C3BF5535-A11E-498E-94EB-BE7E74099FFB.wav was scored at 0.06, indicating a virtually nonexistent likelihood of PD, and File AH_545789670-C297FD53-BF71-4183-86A0-58E5E1EB0DF8.wav received a score of 0.89, strongly suggesting PD presence. Subsequent analyses confirmed that their acoustic features are highly representative of their respective scores, thereby validating our scoring system even in extreme cases.",
        "The entire list of 81 audio files and their corresponding scores can be found as Supplementary Table S1.",
        "In this juncture, we want to evaluate where the machine misclassified the predictions. We would also want to generate insights if possible to inform medical practitioners in the diagnosis and prognosis of PD using voice. We will also use an LLM to investigate the important vocal characteristics in the data.",
        "Of the 41 HC audio files, an average of 36.8 were correctly classified as HC, while 4.2 were incorrectly classified as PD. This resulted in a precision rate of 89.84% \u00b1 1 for HC label predictions. The misclassification of an average of 4.2 HC files could be attributed to various reasons. The most likely explanation for the observed overlap in acoustic features between HC and early-stage PD patients can be attributed to the subtler distinctions between these groups compared to those between HC and late-stage PD. For example, examples of mean HNR in HC patients include 15.32, 23.15, 18.04, 15.83. Examples of mean HNR in PD patients include 13.65, 15.48, 18.68, 17.20. This overlap in numerical data due to acoustic similarity is a possible reason why our model may experience issues when predicting borderline cases. However, it is also possible the feature extraction software did not adequately capture variations in speech patterns, thus resulting in too much leniency in the model\u2019s decision boundary when distinguishing between the two classes.",
        "As seen in Table\u00a01, the AI model correctly classified 37 out of the 40 PD files, with 3 files incorrectly classified as HC. This makes the recall rate 92.50% \u00b1 0.5 for PD predictions, indicating that few PD instances were missed. These false negatives are particularly concerning in a clinical context because failure to identify PD could delay treatment. The variability in symptom severity among patients may have contributed to the misclassification of PD. The model may have also been overly conservative when labeling borderline cases as PD, resulting in such prediction errors.",
        "Table 1 represents the average actual frequency values of the seven AI Models tested across 5 CV folds. Each model was evaluated on the original 41 HC files and the original 40 PD files. A higher count in the \u201cCorrectly Predicted\u201d section indicates stronger model performance.",
        "In this study, we employed SHAP to interpret the model\u2019s predictions. SHAP generations offer insight into the extent to which each feature contributed to the final predictions, allowing us to validate the model\u2019s decision-making process and reliability.",
        "The SHAP summary plot (Fig.\u00a04a) provides a thorough visualization of the most influential features used by our pipelined composite champion model to distinguish between HC and PD patients. Each feature\u2019s impact on the data output is displayed along the x-axis. Positive SHAP values indicate a higher likelihood of the prediction being PD, and negative values indicate a higher likelihood of HC. The left-hand-side y-axis shows the features that had the most influence on model output (top) and the least influence on model output (bottom). The features generated by SHAP were done so for post-training data explainability. They were not used for algorithm training.",
        "Among the most impactful features were MFCCs, with mfcc_3, mfcc_11, and mfcc_5 showing significant influence (Fig.\u00a04a). MFCCs encapsulate the spectral properties of voice, which are known to be altered in PD patients due to the neurodegenerative nature of the disease on speech production. The efficacy of MFCCs in speech recognition, speaker biometry, or voice pathology detection is universally recognized. In fact, a 2023 study isolated MFCCs from other speech features and analyzed sustained vowels similar to this paper. Their performance metrics ranged from 70 to 79%, highlighting the relevance of MFCCs in this field.",
        "Notably, mfcc_3 had a strong positive SHAP value, signifying that higher values of this feature were associated with an increased likelihood of a PD diagnosis. An absence of mfcc_3 would indicate a likelihood of HC. Some characteristics residing at the center of Fig.\u00a04, such as mfcc_2, are more neutral when predicting both values of our binary target. Other characteristics such as mfcc_6, which is found at the bottom of Fig.\u00a04, indicate less prominence in model decision-making with reference to either values of our binary target.",
        "Incorporating jitter and shimmer measurements provided deeper insight into the fine vocal variations associated with PD. Local shimmer and local jitter were extremely influential because they allowed the model to recognize sensitivity in amplitude and frequency variations. For instance, high values of local shimmer were linked to a higher likelihood of a PD diagnosis, as shown by the positive SHAP values. Similarly, rap_jitter and local_jitter, which measure relative frequency perturbations, were also crucial in the model\u2019s predictions.",
        "Although not as significant, \u2018mean HNR\u2019 still contributed to the models\u2019 predictions. Lower HNR values, which suggest a noisier voice signal, were more often associated with PD. This supports the clinical observations of PD patients tending to have a breathier voice due to impaired control of vocal fold vibration.",
        "This study highlights the efficacy of AI, particularly a hybrid model combining MLP, CNN, RNN, and MKL in diagnosing early PD through voice analysis. The model demonstrated a robust ability to distinguish between HC and PD patients with significant accuracy by leveraging key vocal biomarkers such as MFCCs, jitter, and shimmer.",
        "Our champion model had an accuracy of 91.11%, a precision of 89.84%, a recall of 92.50%, an F1 score of 91.13%, and an AUC of 0.9125. These evaluation metrics are all around the 90% mark, indicating high consistency in distinguishing PD patients.",
        "Furthermore, the use of SHAP for data explainability reinforced the reliability of the diagnostic tool by providing transparent insight into how individual acoustic features impacted model decision-making. Features like MFCCs have been well-documented in existing literature as strong indicators of vocal abnormalities in PD, which is why they were among the most prominent in Fig.\u00a04. Also, jitter and shimmer significantly contributed to model decision-making, aligning with well-tested clinical characteristics of PD-related speech disorders. Extrapolating from just the raw data, LLMs such as SHAP can provide insights that were otherwise latent, potentially enabling physicians to tailor treatment plans more effectively by identifying the most prominent acoustic features in a patient\u2019s voice data. In Fig.\u00a04, for instance, features such as mfcc_3 or local_shimmer are more pronounced, indicating different aspects of disease progression that can guide individualized treatment planning. By using SHAP, our model provides data explainability findings that could inform future research. For instance, if, hypothetically, 90% of voice recordings show that mfcc_3 is the most impactful acoustic feature in diagnosis and mfcc_6 is the least impactful, researchers can use this insight in several ways. First, it can guide feature selection and optimization in future models, allowing them to focus on the most relevant vocal biomarkers and reduce noise from less significant features. Second, it can aid in targeted clinical research by prompting speech pathologists and neurologists to investigate why certain vocal characteristics are more strongly associated with PD. This form of explainability is consistent with inferential analysis as it allows us to identify high-information-value variables that play a key role in AI-driven diagnostics. By identifying these critical vocal biomarkers, the algorithm could significantly advance precision medicine approaches by enabling personalized PD treatment plans based on individual vocal feature profiles.",
        "Also, implementing a scoring system proves advantageous over similar works because it allows for a quantifiable, objective measurement of disease markers, which is crucial for early diagnosis and management of PD. Using a random selection of voice recordings, we validated our scoring system and it was consistent with the prediction results because the HC voice recordings were scored 0\u20130.40, and the PD voice recordings were scored 0.60\u20130.90, which are the correct ranges for the HC and PD recordings. This system facilitates longitudinal monitoring of disease progression, offering a valuable tool for assessing treatment efficacy and adjusting therapeutic interventions accordingly.",
        "This study\u2019s findings suggest that hybrid sequential pipeline models like ours offer a promising and noninvasive approach for early PD diagnosis and hold significant clinical implementations. Specifically, because our method requires only a short voice recording, it eliminates the need for costly imaging, lengthy clinical evaluations, or invasive diagnostic procedures. This allows screenings to be highly accessible for patients, allowing for frequent, convenient monitoring. This is particularly useful in remote or telehealth settings. Furthermore, the speed of analysis means patients and clinicians can receive immediate diagnostic feedback. This will decrease the number of cases that progress to latent stages. This increase in early intervention and timely adjustments for treatment plants will ultimately improve patient outcomes and quality of care.",
        "To effectively integrate this model into clinical practice, several key steps must be taken. First, increasing the dataset size is imperative to enhance the model\u2019s generalizability. The purpose of this paper is not to necessarily create a fully-functioning diagnostic tool, but rather as a proof of concept to demonstrate vocal data alone can train an AI algorithm to diagnose PD. The next stage of our model would be to focus on implementation science, translating the model into a production environment for real-world use. Second, regulatory compliance must be addressed to ensure adherence to healthcare data privacy standards such as the Health Insurance Portability and Accountability Act (HIPAA) or General Data Protection Regulation (GDPR). Third, the model\u2019s hardware requirements are minimal, requiring only a microphone, making it a cost-effective and easily scalable screening tool. Fourth, clinician training is straightforward as they would only need to learn how to record and upload audio clips. Finally, integration requires alignment with diagnostic guidelines and healthcare infrastructure. This knowledge translation is crucial for implementation science. Integrating this AI-based diagnostic tool into clinical workflows could enhance remote or telehealth settings, ensuring patients in underserved areas receive early PD screenings.",
        "Of course, the ethical implications of using AI for voice-based PD diagnosis are significant and have been considered carefully. First, data privacy is paramount, especially as voice recordings can be personally identifiable. To address this, our framework deliberately strips voice recordings of identification. As well, when implementing in clinical settings, compliance with established standards such as HIPAA or GDPR would be of utmost priority. Informed consent is another critical aspect, as patients must be fully informed about data usage, storage, and potential risks.",
        "One limitation would be the practicality of using this model in real-world clinical settings. Patients may experience challenges when trying to record high-quality voice recordings, thus skewing the model\u2019s ability to analyze the voice recordings effectively. Another limitation would be the model\u2019s performance in handling longitudinal data. This AI model is designed for early PD detection and is trained on static voice recordings. However, it is uncertain whether the model can track PD progression over time with the same level of accuracy as it achieves in early detection. As well, we acknowledge our small dataset is a limitation, which is why this study serves as a proof of concept to evaluate the feasibility of using voice alone to diagnose PD. We have taken rigorous measures such as five-fold cross-validation and file identification removal to mitigate the effects of this limitation. For example, the standard deviation (SD) of extracted acoustic features demonstrates a significant range in variability, particularly in pitch (SD\u2009=\u200950.08\u00a0Hz (HZ)) and MFCCs (SD up to 43.82). This suggests that the dataset effectively captures vocal diversity in PD and HC patients, mitigating concerns about overfitting and generalizability. As well, even though we have demonstrated that AI can extract meaningful vocal biomarkers for PD classifications, future research should explore synthetic data generation techniques to increase the training dataset. Specifically, Synthetic Minority Over-sampling Technique (SMOTE) can be applied to generate artificial voice recordings that retain the statistical properties of real recordings, mitigating the concern of data scarcity. As well, semi-supervised learning could be leveraged to enhance model training by incorporating unlabeled data alongside our existing annotated recordings. This two-front approach of improvement that leverages SMOTE and semi-supervised learning is an effective way of combatting the small dataset size.",
        "Using MLP\u2009+\u2009CNN\u2009+\u2009RNN\u2009+\u2009MKL in the AI model introduced layers of complexity. While highly sophisticated and extremely powerful, this model style may have introduced new layers of depth that were not properly synthesized, thus potentially contributing to data overfitting or poor generalization for specific test cases. This warrants future work to develop methods of balancing the complexity of the AI with the data.",
        "Furthermore, it would be prudent to attempt to pair this MLP\u2009+\u2009CNN\u2009+\u2009RNN\u2009+\u2009MKL model with other means of physical analysis. For example, creating a smartwatch that can record the wearer\u2019s speech and track physical movements such as tremors and gait would be a great way to introduce a multimodal, non-invasive, early PD diagnosis method. Han et al. explored the idea of using wrist-worn devices to capture the unique vocal characteristics of an individual. They achieved a 92.85% in identifying the correct participant from a voice recording. Although their primary purpose was to create an anti-spoofing defence mechanism, the model\u2019s ability to detect acoustic features unique to a person can be transferable to PD diagnosis.",
        "Finally, we have considered the possibility of implementing this hybrid diagnostic framework for other neurological diseases. While our voice-based AI framework has demonstrated strong performance in diagnosing PD, its applicability to other voice-related or neurological disorders remains uncertain. The model\u2019s core strength lies in detecting distinctive vocal characteristics of PD. However, its ability to generalize to other conditions, such as laryngitis or Alzheimer\u2019s Disease, presents challenges. For voice disorders primarily affecting vocal cords, the model\u2019s transferability may be higher due to a clear presence of vocal biomarkers. In contrast, neurological conditions like Alzheimer\u2019s may not exhibit the same distinct vocal traits, making classification more complex. Furthermore, differentiating between disorders with overlapping vocal biomarkers may lead to misclassification. However, a multi-modal approach could help overcome these limitations. Complete schemes for multi-character classifications have been generated using electroencephalography signals from speech imagery. This integration of multi-modal data\u2014such as combining voice analysis with brain activity measurements\u2014could improve classification accuracy and prevent diagnostic confusion between distinct conditions. By incorporating additional input modalities like gait analysis, clinical history, or wearable health data, future iterations of our model could improve diagnostic accuracy and adaptability for a wider range of diseases.",
        "The dataset for training this AI model consists of 81 distinct voice recordings sourced from a publicly accessible dataset. Of these recordings, 41 were taken from healthy patients in the HC group, and the other 40 were taken from patients with PD who comprise the PD group. To maintain consistency among the data, the recordings were modified to remove background noise, equalize decibels based on sex, and retain intervals of silence before and after the audio. This dataset was compiled by Iyer, et al.. They applied Audacity\u00ae to their voice recordings to remove background noise. They also filtered the recordings using floor and ceiling values of 75 decibels (dB) and 300 dB, respectively, for males and 100 dB and 600 dB for females. Then, Iyer et al. rescaled the speech signals to the range [\u2212\u00a01,1]. They also trimmed and removed the intervals of silence at the start and end of the recording to ensure the silence did not impede model analysis.",
        "Iyer, et al. created the dataset shown in the Table\u00a02. This table presents the demographic and clinical data of the Healthy Control and Parkinson\u2019s Disease groups, including sex ratio, mean age at data collection, Hoehn and Yahr stage for PD severity, and disease duration.",
        "This dataset was chosen because of its effective representation of real-world scenarios. It has a balanced sex ratio and diversity in age, PD development, and length of disease, as seen in Table\u00a02.",
        "The AI model excels at processing audio files, demonstrating superior performance with .wav formats and .zip archives. The script uses the Parselmouth library, a Python wrapper for Praat\u2014a software tool for speech analysis. Parselmouth library automatically numerically digitalizes audio files into their respective features for ease of analysis. The primary function, extract_voice_features, takes an audio file as input and extracts several key acoustic features. First, the audio is converted into a parselmouth.Sound object, allowing for various analyses. Then, using Praat\u2019s \u201cTo Pitch\u201d method, the AI retrieves the mean, minimum, and maximum pitch values. Next, the model calculates local jitter, which measures frequency variation, by converting the sound to a point process and applying the \u201cGet Jitter (local)\u201d method. Similarly, local shimmer, which measures amplitude variation, is extracted using the \u201cGet Shimmer (local)\u201d method. Finally, the script calculates the harmonicity-to-noise ratio (HNR) using Praat\u2019s harmonicity analysis method to determine the mean HNR.",
        "Figure 5 showcases an amalgamation of each acoustic feature extracted and collected.",
        "Raw data values for the HC group and the PD group can be found as Supplementary Table S2 and Supplementary Table S3, respectively.",
        "Continuous model refinement involved leveraging insights from acoustic feature analysis and interpreting various graphical plots. Although these graphical visualizations were not directly used in the model, they served as a valuable tool for clinicians and researchers to better understand the underlying data. For example, the spectrograms provided visual cues about the voice recordings\u2019 frequency content and temporal dynamics. Violin plots, box plots, and histograms illustrated the distribution of the acoustic features, highlighting blatant and nuanced differences between the HC and PD groups. Scatter plots depicted relationships between mean pitch and HNR, revealing distinct clusters of the two groups, further aiding the model\u2019s training. Overall, the graphical data allowed for a more informed approach to model improvement by enhancing our understanding of the data\u2019s complexities.",
        "All graphs can be found as Supplementary Figures S1.",
        "The Fourier transform (FT) is beneficial in speech analysis because different aspects of the voice can be analyzed more effectively in the frequency domain. Furthermore, the precise data of acoustic feature analysis is too complex for human scrutiny (Fig.\u00a06a vs. b), especially in real-time. This gives a legitimate case for using machine learning to ensure proper analysis.",
        "The 81 speech recordings were initially captured as time-domain signals, representing how the audio amplitude varies over time. The FT converts these time-domain signals into the frequency domain, representing the signal in terms of its frequencies and their respective amplitudes. This helps to isolate and identify different frequency components indicative of vocal characteristics.",
        "The frequency domain representation obtained through FT allows the AI to extract the key acoustic features: pitch, jitter, shimmer, and HNR. The extracted features are then standardized using Python\u2019s \u2018StandardScaler\u2019 from \u2018sklearn\u2019 to ensure they are on a similar scale, improving the model\u2019s performance. The AI is trained to automate these extractions, allowing future researchers to efficiently process large datasets of voice recordings while maintaining consistency in feature extraction.",
        "The AI model generates spectrograms by applying the Short-Time Fourier Transform (STFT) to the audio signal. The STFT divides the signal into short, overlapping segments and then applies FT to each segment, resulting in a time-frequency representation. The color scale adjacent to each spectrogram represents the magnitude of its frequency components, which are measured in decibels. The colors range from bright yellow, signifying higher amplitude components, to dark purple, indicating lower amplitude components. The color scale represents 10 log(|S|/max(|S|)), where S denotes the complex numbers obtained from the output of the FT. This logarithmic scaling underscores the differences in intensity across various frequencies, allowing the AI model to easily discern subtle variations that could be crucial for diagnostic purposes.",
        "In Fig.\u00a06a, which depicts the HC group, the frequency bands are clearly defined and consistent across the time axis, indicating stable vocal tract function and regular vocal fold vibration. Furthermore, harmonics are visible at regular intervals\u2014characteristics of a healthy vocal system. On the other hand, the PD group in Fig.\u00a06b has less distinct frequency bands and exhibits more variability. These irregularities suggest vocal instability, something commonly seen in PD patients. Such instability may be due to tremors affecting vocal fold vibration, leading to the scattered and less defined harmonic structure. We only used vocal data in our AI decision-making, but these graphs exist as a foundational representation to aid clinicians in understanding the granular details of the voice recordings.",
        "The experimentation phase of this model involved constantly improving a rudimentary MLP and CNN model designed to diagnose Parkinson\u2019s disease from voice recordings. This model utilized Python\u2019s robust ML and audio processing libraries for rigorous training and validation to achieve optimal performance. Eventually, MLP and CNN were paired with RNN and MKL to create a unified PD diagnosis model that harnesses each approach\u2019s strengths.",
        "The MLP\u2009+\u2009CNN\u2009+\u2009RNN\u2009+\u2009MKL (our champion model) is a hybrid model whose architecture was designed to discern intricate patterns in voice signals. MLP is useful when applied to structure learning, meaning it is strong when detecting and learning patterns in HC and PD recordings. CNN excels at capturing local acoustic patterns with spectrograms, such as mean pitch. RNN effectively models the temporal dynamics of speech, which is critical for the model to correctly identify sequential anomalies associated with PD. Finally, MKL enriches the AI model by enabling the integration of diverse feature modalities, thus making for a more comprehensive analysis and prediction.",
        "Mahmood et al. also used a similar approach to us, but they applied it to medical imaging. They used a model that integrates two segmentation networks and a multimodal registration network. This allows the model to run like a sequential pipeline while allowing the segmentation and registration processes to work in parallel. Furthermore, because PD manifests differently in each individual, hybrid models like our champion model are even more critical. These models can generalize more effectively because they extract information from both spatial and temporal domains, creating a more holistic understanding of PD-related vocal changes. This accounts for variations in patients, thus providing a more encompassing diagnostic tool.",
        "The model comprises multiple convolutional layers that extract hierarchical feature representations from the input spectrograms. These layers were followed by pooling layers to downsample the feature maps, reducing computational complexity and preventing data overfitting. Finally, dense layers were used to combine the extracted features and make a final prediction. We used k-fold cross-validation (CV) to report more accurate evaluation results, which means we averaged final performance metrics across all runs.",
        "Due to the relatively smaller size of our dataset, we have implemented rigorous validation techniques to mitigate this. We use five-fold CV to ensure model generalizability. Each trial run includes 150 epochs where training data is 75% of the total data and testing data is 25% of the total data. In each epoch, the files used for training and testing are randomized to reduce overfitting. Furthermore, file identification numbers were removed to prevent bias from the model memorizing recordings and their diagnosis labels.",
        "In addition to randomizing training and testing data, we have more complex methods of reducing overfitting in place. In our codebase, we used L1/L2 regularization (kernal_regularizer\u2009=\u2009l1_l2 (l1\u2009=\u20090.01, l2\u2009=\u20090.01)), which penalizes complex model weights, discouraging overfitting. Additionally, we incorporated Dropout layers (rates of 0.3\u20130.4) to deactivate neural network neurons during training randomly. To further prevent overfitting and optimize training duration, we implement automated callbacks: EarlyStopping (to monitor validation loss with a patience of 7 epochs) and ReduceLROnPlateau (to reduce the learning rate by a factor of 0.3 if validation loss plateaued for 3 epochs).",
        "We also have many hyperparameter optimizations in place to ensure the optimal specifications are in use. Hyperparameter optimization was achieved through a combination of empirical selection and dynamic in-training adjustments. For our model, we manually set critical parameters. This includes a learning rate of 0.0003, a batch size of 16, and 150 epochs. These were done based on preliminary experiments. Additionally, for the Random Forest component, the number of estimators was manually chosen. The Recursive Feature Elimination with Cross-Validation (RFECV) was utilized to automatically optimize feature subsets based on model feedback.",
        "This model can be considered a sequential pipeline structure with multiple branches where CNN and RNN might operate in parallel, and their outputs are later combined using MKL.",
        "Input is fed into the CNN layers.",
        "Input Data:",
        "Extracts feature maps from the input data.",
        "Output: Feature maps.",
        "CNN Layer:",
        "Takes feature maps as input and learns temporal sequences.",
        "Output: Temporal feature representation.",
        "RNN Layer:",
        "Takes inputs from both CNN and RNN.",
        "Learns a kernel-based representation.",
        "Output: Combined representation.",
        "MKL Layer:",
        "Processes the combined representation to learn higher-level, non-linear relationships between acoustic features.",
        "Output: A refined, non-linear representation of the data suitable for prediction.",
        "MLP Layer:",
        "The final representation is passed to one or more dense layers.",
        "Output: Prediction distribution.",
        "Fully Connected Layers:",
        "Provides the classification output.",
        "Output Layer:",
        "Figure 7 is a graphical representation of the champion model\u2019s architecture flow.",
        "We have an inference time of 0.06\u00a0s on standard hardware and a training duration of around 12.35\u00a0s on standard hardware. This efficiency ensures suitability for deployment in real-time diagnostic settings by enabling rapid and convenient PD assessments. While deep learning models demand more resources than traditional ML (e.g., SVM), our pipeline remains computationally feasible for real-world diagnostic applications.",
        "HC and PD diagnoses exist on a continuum, meaning that not all files labeled as \u201cHC\u201d are equally distant from a potential PD diagnosis; some may be closer to being classified as PD than others. In conjunction with binary labeling, we want a more granular means of distinction among different HC and PD cases.",
        "We created a scoring system to quantify the likelihood of a patient having PD based on key acoustic features extracted from their voice recordings. This system is derived from model probabilities, allowing clinicians to interpret the likelihood of PD more effectively and set individualized thresholds for diagnosis. By enabling precision medicine, this approach ensures that diagnostic decisions are tailored to the unique characteristics of each patient rather than relying on a one-size-fits-all binary system.",
        "PD exists on a spectrum of severity, rather than being a binary condition. Traditional diagnostic methods classify patients as either having PD or not, making it difficult to assess PD progression over time. Our scoring system addresses this challenge by providing a continuous target variable, allowing for more granular assessments of PD severity. By implementing a progressive, interval-based screening approach, our system enables individuals to test periodically and monitor fluctuations in their numerical score. Clinicians can use these scores to evaluate treatment effectiveness by tracking whether a patient\u2019s condition is stable, improving, or worsening. Furthermore, this method enhances patient understanding of their condition, fostering greater engagement in disease management. These practical advantages make it particularly valuable for clinical settings focused on long-term PD treatment and monitoring.",
        "Our scoring system also offers several advantages over other numerical and graph-based methods used for quantifying disease progression. There are many existing methods of tracking and predicting the development of PD in patients. For example, Naranjo et al. utilized Hidden Markov for analyzing longitudinal data, employing regression techniques to predict the development of patients\u2019 PD by displaying the Hoehn and Yahr stages relative to time. However, Naranjo et al. only used PD files in their study because they were simply predicting a trajectory of disease development. In contrast, our scoring system is predicting if a patient has PD or not and generating a score if PD is detected. Although Naranjo et al.\u2019s model provides useful insights into predictions about when a patient may transition to new PD stages, it lacks a crucial aspect inherent to our scoring system: the ability for patients to interpret their own results meaningfully.",
        "Our scoring system offers a description context for each score by linking numerical values to specific symptoms. This allows patients to self-assess and verify if the score corresponds to their real-life experiences. For example, if a user who wants to conduct self-screening at home receives a score of 0.20 but does not notice changes in their everyday speech, they are more likely to trust and accept this score because it aligns with their personal observation. As a result, they may be more inclined to seek medical treatment, potentially enabling physicians to detect PD at an earlier stage and take proactive measures to mitigate its impact. In clinical settings, our model empowers clinicians to track scores systematically over time. An increase in score can indicate the need to initiate or adjust treatment, while a stable score may suggest that current therapies are effective at delaying PD progression. This can inform healthcare professionals in their decision-making process when treating each patient.",
        "Our scoring system assigns probabilities of the likelihood of an individual being diagnosed with PD on a scale from 0 to 1. The system uses probabilities generated by a Random Forest model trained on acoustic vocal features such as mean pitch, MFCCs, local jitter, local shimmer, and HNR. This model is integrated in a sequential pipeline with our champion model. This means that once the champion model completes its diagnosis, the results are input into the Random Forest model for scoring. Although the Random Forest model\u2019s code is computationally simpler than our champion model\u2019s code, it shows a strong correlation with the champion model\u2019s results, thus providing an interpretable layer for clinical decision-making.",
        "Table 3 presents the AI model\u2019s probability-based scoring system, which assesses the likelihood of PD based on vocal biomarkers. Scores range from 0.00 to 1.00, with higher values indicating a stronger presence of PD-related vocal characteristics. Each range is accompanied by a description that provides interpretability for both clinicians and patients, enabling early detection, monitoring, and potential clinical evaluation."
    ],
    "title": "Explainable artificial intelligence to diagnose early Parkinson\u2019s disease via voice analysis"
}