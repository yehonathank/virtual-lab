{
    "content": [
        "Large neural language models have transformed modern natural language processing (NLP) applications. However, fine-tuning such models for specific tasks remains challenging as model size increases, especially with small labeled datasets, which are common in biomedical NLP. We conduct a systematic study on fine-tuning stability in biomedical NLP. We show that fine-tuning performance may be sensitive to pretraining settings and conduct an exploration of techniques for addressing fine-tuning instability. We show that these techniques can substantially improve fine-tuning performance for low-resource biomedical NLP applications. Specifically, freezing lower layers is helpful for standard BERT- models, while layerwise decay is more effective for BERT- and ELECTRA models. For low-resource text similarity tasks, such as BIOSSES, reinitializing the top layers is the optimal strategy. Overall, domain-specific vocabulary and pretraining facilitate robust models for fine-tuning. Based on these findings, we establish a new state of the art on a wide range of biomedical NLP applications.",
        "Systematic exploration of fine-tuning stability in biomedical NLP",
        "Domain-specific vocabulary and pretraining facilitate robust models for fine-tuning",
        "PubMedBERT-large and PubMedELECTRA models advance state-of-the-art in biomedical NLP",
        "Large neural language models have transformed modern natural language processing (NLP) and have recently become a focus of public attention. However, fine-tuning these models for specific tasks of interest remains challenging as model size increases, especially with small labeled datasets, which are common in biomedical NLP.",
        "This study conducts a systematic exploration of fine-tuning stability in biomedical NLP and identifies techniques that address instability and improve performance. The findings highlight the importance of domain-specific vocabulary and pretraining for creating robust models and establish a new state of the art on a wide range of biomedical NLP applications in the Biomedical Language Understanding and Reasoning Benchmark (BLURB).",
        "Large neural language models have transformed modern natural language processing (NLP). However, fine-tuning these models for specific tasks remains challenging as model size increases, especially with small labeled datasets, which are common in biomedical NLP. This systematic exploration of fine-tuning stability in biomedical NLP highlights the importance of domain-specific vocabulary and pretraining for creating robust models and establishes a new state of the art on a wide range of biomedical NLP applications.",
        "Biomedical text is growing at an explosive rate. PubMed adds thousands of scientific papers every day and more than a million every year. Simultaneously, digitization of patient records has created steadily growing resources of clinical text. For example, every year there are about 2 million new cancer patients in the United States alone, each with hundreds of clinical notes, such as pathology reports and progress notes. Curating knowledge and longitudinal patient information from this text stands to accelerate clinical research and improve clinical care. Manual curation, however, does not scale to the rapid growth of biomedical text because manual curation often requires hours for each paper or patient and may require domain-specific expertise, such as clinical knowledge, that precludes common techniques in crowd-sourcing.",
        "Natural language processing (NLP) has emerged as a promising direction to accelerate curation by automatically extracting candidate findings for human experts to validate. However, standard supervised learning often requires a large amount of training data. Consequently, task-agnostic self-supervised learning is rapidly gaining traction. By pretraining on unlabeled text, large neural language models facilitate transfer learning and have demonstrated spectacular success for a wide range of NLP applications.",
        "Fine-tuning these large neural models for specific tasks, however, may be unstable and prone to overfitting, as has been shown in the general domain. For biomedicine, the challenge is further exacerbated by the scarcity of task-specific training data because annotation requires domain expertise and crowd-sourcing is harder to apply. For example, the Biomedical Semantic Similarity Estimation System (BIOSSES) dataset, a semantic similarity task in the biomedical domain, contains only 100 annotated examples in total. By contrast, STS, a similar dataset in the general domain, contains 8,628 examples.",
        "In this paper, we conduct a systematic study on fine-tuning stability in biomedical NLP. We focus this effort on two popular models, Bidirectional Encoder Representations from Transformers (BERT) and Efficiently Learning an Encoder that Classifies Token Replacements Accurately (ELECTRA). We ground\u00a0our study in BLURB, a recently proposed comprehensive benchmark for biomedical NLP comprising six tasks and 13 datasets.",
        "We first studied how pretraining settings impact fine-tuning performance. We show that for all applications, skipping next-sentence prediction (NSP) in pretraining has negligible effect, thus saving significant compute time, a finding consistent with general-domain observations by Liu et\u00a0al. and Aroca-Ouellette and Rudzicz. However, modeling segment IDs during pretraining may have a large impact on certain semantic tasks, such as\u00a0text similarity and question answering, especially when training data are scarce. Larger models (e.g., BERT-) significantly increase fine-tuning instability, and their use often hurts downstream performance. Interestingly, changing the pretraining objective from the masked language model (MLM) to ELECTRA has demonstrated improved performance in general-domain applications, but it may exacerbate fine-tuning instability in low-resource biomedical applications.",
        "We then conducted a comprehensive exploration of stabilization techniques to establish the best practice for biomedical fine-tuning. We show that conventional general-domain techniques, such as longer training and gradient debiasing, help but layerwise adaptation methods are key to restoring fine-tuning stability in biomedical applications. Interestingly, their efficacy may vary with pretraining settings and/or end tasks. For example, freezing lower layers is helpful for standard BERT- models, whereas layerwise decay is more effective for BERT- and ELECTRA models. For low-resource text similarity tasks, such as BIOSSES, reinitializing the top layers is the optimal strategy. Overall, we find that domain-specific vocabulary and pretraining produce more robust language models. Based on these findings, we attain new state-of-the-art performance on a wide range of biomedical NLP tasks.",
        "Finally, we showed that the best biomedical language models not only cover a much wider range of applications, but also substantially outperform off-the-shelf biomedical NLP tools on their currently available tasks. To facilitate biomedical research and applications, we released our state-of-the-art pretrained and task-specific fine-tuned models.",
        "We conduct a systematic study on fine-tuning stability and mitigation methods in the presence of various pretraining settings and large models. Prior work studying fine-tuning stability and mitigation methods tends to focus on general domain\u2014e.g., using BERT models pretrained on general-domain corpora and evaluating on GLUE or SuperGLUE. Table\u00a01 summarizes representative recent work and common stabilization techniques.",
        "We ground our study on the Biomedical Language Understanding\u00a0& Reasoning Benchmark (BLURB). BLURB is a comprehensive benchmark for biomedical NLP, spanning six tasks and 13 datasets, including applications with very small training datasets, such as text similarity and question answering. To facilitate a head-to-head comparison, we followed the train/dev/test setup from BLURB in all our experiments.",
        "We first conducted an ablation study to evaluate the impact of pretraining settings on fine-tuning stability. Prior work on fine-tuning stability focuses almost exclusively on  models; we showed that  models also suffer instability if we deviate from standard BERT pretraining settings.",
        "Specifically, we experimented with skipping NSP during pretraining. Standard BERT pretraining inputs two text sequences (with two distinct segment IDs). We also experimented with inputting a single sequence at a time (with same segment ID). For a head-to-head comparison, we pretrained all language models from scratch on PubMed abstracts (i.e., using the same settings as PubMedBERT) and adopted the same fine-tuning settings as in Gu et\u00a0al.",
        "Table\u00a02 shows the results. In general, NSP has relatively little impact on end task performance. However, pretraining with single sequences leads to a substantial performance drop in the sentence similarity task (BIOSSES). Presumably performance degrades because this task requires comparison of two sentences and the training set is very small, therefore pretraining\u00a0with two text segments helps. Surprisingly, pretraining with single sequences substantially improves test performance on PubMedQA, even though the task also inputs two text segments. Interestingly, even with the original pretraining setting (with NSP and two segments), simply using a single-segment ID in fine-tuning for PubMedQA would result in a similarly large gain in test performance (F1 63.92, not shown in the table). However, the standard setting (using separate segment ID) is still better for BIOSSES and BioASQ.",
        "We also evaluated using the ELECTRA objective. Unlike in the general domain, ELECTRA does not show clear improvements over the MLM objective, when fine-tuned in an identical manner, in the biomedical domain. In fact, ELECTRA performs worse on most tasks and suffers a catastrophic performance drop in text similarity. We note that these tasks also happen to have relatively small training sets. This may appear contradictory with some recent work that demonstrates superior results using ELECTRA in biomedical NLP. Later, we showed that it is indeed possible to attain higher performance with ELECTRA, but doing so requires various techniques to stabilize and improve fine-tuning. Compared with BERT with the MLM objective, ELECTRA is generally more difficult to fine-tune and demonstrates no significant advantage for biomedical NLP.",
        "As previously mentioned, prior studies conclude that small optimization adjustments often suffice to restore fine-tuning stability in  models. In biomedical NLP, however, we found that such adjustments are necessary to prevent catastrophic performance drops, but are not always sufficient for stabilizing fine-tuning, even with  models. Table\u00a03 shows an ablation study on BIOSSES. In this case, forgoing either adjustment leads to a significant performance drop. But, as noted in the last subsection, even if both are used, fine-tuning remains unstable with alternative pretraining settings, which requires more advanced stabilization techniques.",
        "Next, we studied various layer-specific adaptation methods in fine-tuning. Given that most models suffer from high instability on sentence similarity (BIOSSES) and question answering (BioASQ and PubMedQA), we focused on those tasks. For question answering, we reported the mean performance. Table\u00a04 shows the results. All three methods are broadly beneficial, but their effects vary substantially with tasks and pretraining settings. Freezing lower layers is helpful for BERT models with the standard MLM objective, whereas layerwise decay is more effective for ELECTRA models. For sentence similarity, reinitializing the top layers is the optimal strategy. We focused our study on sentence similarity and question answering tasks, as other datasets in BLURB are relatively large and do not suffer from\u00a0stability issues. We explored a combination of layer-specific adaptation methods but found little gain in preliminary experiments.",
        "In addition, we consider the task of relation extraction and simulate low-resource settings by subsampling training instances (100/500/1000 from ChemProt and DDI that contain 18,035 and 25,296 training instances, respectively). Table\u00a05 shows the results. Not surprisingly, test performance is lower with fewer training instances. However, the simulated results confirm that layer-specific adaptation generally increases fine-tuning stability and test performance (except in the extremely low-resource setting of 100 training instances).",
        "It is well known that larger models can be finicky to fine-tune. Again, we focused on sentence similarity (BIOSSES) and question answering (BioASQ and PubMedQA). Indeed, we observed a substantial drop in test performance on sentence similarity and question-answering tasks for most large models (see Table\u00a06). Note that to avoid clutter, we only show the average scores for the question-answering tasks.",
        "Surprisingly, PubMedBERT- is a notable exception because it does not suffer any catastrophic performance drop. In fact, it actually gains slightly on the question-answering tasks. This stands in stark contrast with other models such as BioBERT and BlueBERT. We hypothesize that its robustness stems from domain-specific vocabulary and pretraining. Interestingly, although PubMedELECTRA- is also pretrained in the same domain-specific fashion, it suffers a similar performance drop, which provides further evidence that the ELECTRA pretraining objective may exacerbate fine-tuning instability.",
        "Optimization adjustments (longer training time and ADAM bias correction) have been used in all these experiments. Unlike in the general domain, they are not sufficient to restore stability. As in the case of  models, layer-specific adaptation methods can substantially reduce fine-tuning instability, in some cases enabling  models to attain even higher performance than  (e.g., PubMedBERT- on QA and PubMedELECTRA- on SS). See Table\u00a06.",
        "Like Gu et\u00a0al., we also observed that domain-specific vocabulary and pretraining are far superior, as PubMedBERT- substantially outperforms BioBERT- and BlueBERT-. Again, while ELECTRA models can perform reasonably well with advanced stabilization techniques, they are still finicky to fine-tune and are not superior over BERT models with the standard MLM pretraining objective. As with  models, reinitializing the top layers is still the optimal strategy for sentence similarity. However, for question answering, layerwise decay is superior for  models.",
        "Table\u00a07 compares overall BLURB test performance for  models with both improved optimization and layer-specific adaptation. They help stabilize fine-tuning, with no  model suffering significant instability issues. With domain-specific vocabulary and pretraining, PubMedBERT- and PubMedElectra- benefit the most and attain significant gain over .",
        "Rising concerns about computation cost of large pretrained models have spawned research in model pruning, such as removing top layers of a BERT model. We thus conducted an ablation study on BLURB tasks to assess the impact of removing top layers from PubMedBERT. Table\u00a08 shows the results. Indeed, pruning barely impacts fine-tuning efficacy for many tasks, such as named entity recognition (NER), evidence-based medical information extraction, sentence similarity, and document classification. Test performance does not substantially drop even when the top half of the layers were removed, suggesting that these tasks are relatively easy and do not require deep semantic modeling. By contrast, test performance in relation to extraction and question answering was substantially impacted by layer removal, dropping up to 3 to 4 absolute points for the former and up to 6 to 13 points for the latter. This suggests model pruning may make sense for simpler tasks, but not for semantically more challenging tasks. Further, this study suggests that the upper encoder layers are crucial for semantically challenging tasks, such as question answering. This illustrates why stabilization techniques, such as layerwise decay and layer\u00a0freezing, are particularly beneficial for question-answering tasks\u2014both techniques emphasize retraining these upper layers, which may have overfit to the pretraining objective.",
        "To further improve test performance for low-resource tasks, a common technique is to combine the training set and development set to train the final model\u2014after hyperparameter search is done. We found that for most biomedical NLP tasks, this was not necessary, but it had a significant effect on BIOSSES. This is not surprising given that this dataset is the smallest.",
        "By combining our findings on optimal fine-tuning strategy, we establish a new state-of-the-art in biomedical NLP. Table\u00a09 shows the results. PubMedBERT with the MLM pretraining objective remains the best model, consistently outperforming ELECTRA in most tasks, although the latter does demonstrate some advantage in question-answering tasks, as can be seen in its superior performance with  models. With more extensive hyperparameter tuning, the gap between  and  is smaller, compared with more standard fine-tuning (Table\u00a06), which is not surprising. Overall, we were able to significantly improve the BLURB score by 1.6 absolute points, compared with the original PubMedBERT results in Gu et\u00a0al. (from 81.35 to 82.91).",
        "While there are many off-the-shelf tools for general-domain NLP tasks, there are few available for the biomedical domain. Two recent exceptions are scispaCy and Stanza, both with a limited scope focusing on NER. Table\u00a010 compares sciSpaCy and Stanza performances with PubMedBERT on BLURB NER tasks. scispaCy comes with two versions, trained on JNLPBA and BC5CDR, respectively. Stanza comes with eight pretrained biomedical models, among which four overlap with or are related to BLURB NER tasks, namely JNLPBA, BC5CDR, NCBI-disease, and BC4CHEMD. We compare individually and to an oracle version of sciSpaCy and huggingface versions of Stanza that pick the optimal between the three for each evaluation dataset. As Stanza does not provide any gene/protein extraction model, its performances on BC2GM task are empty. While scispaCy and Stanza perform well, PubMedBERT fine-tuned models attain substantially higher scores. We note that many scispaCy errors stem from imperfect entity boundaries. We thus further compare the two using a lenient score that regards overlapping predictions as correct (Table\u00a011). As expected, the gap shrinks but PubMedBERT models still demonstrate overwhelming improvement, raising the average score by more than 10 points compared with sciSpacy. In both settings, PubMedBERT models outperform Stanza across tasks.",
        "Studies on pretraining and fine-tuning large neural language models originated in the general domain, such as newswire and the web. Recently, there has been increasing interest in biomedical pretraining and applications. In particular, Gu et\u00a0al. conducted an extensive evaluation of pretrained models on wide-ranging biomedical NLP tasks. However, they focus on domain-specific pretraining, whereas we study fine-tuning techniques and explore how they might interact with tasks and pretraining settings.",
        "Prior studies on fine-tuning stability focused on the general domain and  models, and often conclude that simple optimization adjustments, such as longer training time and ADAM debiasing, suffice for stabilization. By contrast, we show that in biomedical NLP, even  models may exhibit serious instability issues and simple optimization adjustments are necessary, but not sufficient, to restore stabilization. We systematically study how fine-tuning instability may be exacerbated with alternative pretraining settings such as using single sequences and the ELECTRA objective. We show that layer-specific adaptation methods help substantially in stabilization and identify the optimal strategy based on tasks and pretraining settings.",
        "In this work, we identify several strategies that are effective for biomedical applications, but we have not found a single strategy or combination that works well across all models and tasks. Our exploration is also bounded by computational resources and time. While our study is relatively large and thorough, we have not exhaustively explored all possible settings or methods. Below we list some relevant directions that are beyond the scope of our study.",
        "Multi-task learning can also mitigate the challenge presented by low-resource biomedical tasks. This process generally requires applications with multiple related datasets, such as NER. As discussed previously, NER tasks are relatively easy, and domain-specific pretrained models can\u00a0already attain high performance without specific adaptation.",
        "Other relevant methods include Mixout, which has not been found to consistently improve performance, and fine-tuning on intermediate tasks, which is not always applicable and incurs substantial computation. Instead, we focus on layer-specific adaptation techniques that are generalizable and easily implemented.",
        "Finally, adversarial training can also help instability issues and prompt-based learning has been shown to work well in low-resource settings. They are worth exploring in future work.",
        "Further information and requests for resources should be directed to the lead contact, Hoifung Poon (hoifung@microsoft.com).",
        "This study did not generate any physical materials.",
        "In this paper, we focus our study on BERT and its variants, which have become a mainstay of neural language models in NLP applications. Here, we begin with a brief review of the evaluation metrics for each of the datasets comprising BLURB, and then review core technical aspects in neural language model pretraining and fine-tuning, providing a basis for the key research questions of our fine-tuning study.",
        "The six tasks in BLURB use evaluation metrics that are appropriate for each task. For reference, we identify each evaluation metric here, but refer to readers to Gu et\u00a0al. and the corresponding works describing each task for additional details.",
        "Named-entity recognition (NER), including BC5-chem, BC5-disease, NCBI-disease, BC2GM, and JNLPBA, use F1 score at the entity-level. Evidence-based medical information extraction (PICO), including EBM PICO, uses macro F1 score at the word-level. Relation extraction, including ChemProt, DDI, and GAD, use micro F1 score. Sentence similarity, including BIOSSES, use Pearson correlation between the gold standard scores and the scores produced by the model. Document classification, including HoC, uses micro F1 score. Question answering, including PubMedQA and BioASQ, uses accuracy.",
        "The BLURB score is the macro average of average tests results for each of the six tasks (NER, PICO, relation extraction, sentence similarity, document classification, and question answering).",
        "The input to a neural language model consists of text spans, such as sentences, separated by special tokens . To address the problem of out-of-vocabulary words, neural language models generate a vocabulary from subword units, using Byte-Pair Encoding (BPE) or variants such as WordPiece. Essentially, the BPE algorithm tries to greedily identify a small set of subwords that can compactly form all words in a given corpus. It does this by initializing the vocabulary with all characters and delimiters found in the corpus. It then iteratively augments the vocabulary with a new subword that is most frequent in the corpus and can be formed by concatenating two existing subwords, until the vocabulary reaches the pre-specified size\u2014e.g., 30,000 in standard BERT models or 50,000 in RoBERTa. In this paper, we use the WordPiece algorithm, which is a BPE variant that augments the vocabulary using likelihood in an unigram language model rather than frequency in choosing which subwords to concatenate.",
        "The text corpus and vocabulary may preserve the original case  or convert all characters to lower case . Prior work, such as Gu et\u00a0al., finds that case does not have significant impact on downstream tasks, so we simply use  in our work.",
        "BERT is a state-of-the-art neural language model based on a transformer architecture. The transformer model introduces a multi-layer, multi-head self-attention mechanism, which has demonstrated superiority in leveraging GPU computation and modeling long-range text dependencies. Standard BERT pretraining inputs two text spans (e.g., sentences) and assigns a distinct segment ID to each. The input token sequence is first processed by a lexical encoder, which combines a token embedding, a position embedding, and a segment embedding by element-wise summation. This embedding layer is then passed to multiple layers of transformer modules. In each transformer layer, a contextual representation is generated for each token by summing a non-linear transformation of the representations of all tokens in the prior layer, weighted by attention computed using a given token\u2019s representation in the prior layer as query. The final layer outputs contextual representations for all tokens, which combines information from the whole text span.",
        "BERT models come with two standard configurations:  uses 12 layers of transformer modules and 110 million parameters,  uses 24 layers of transformer modules and 340 million parameters. Prior work applying BERT to biomedical NLP focuses on  models. By contrast, in this work, we conduct a systematic study on  models as well, which reveals additional challenges for fine-tuning neural language models in biomedical NLP.",
        "Similar to other language models, the key idea of BERT pretraining is to predict held-out words in unlabeled text. Unlike most prior language models, BERT does not adhere to a generative model. Instead, Devlin et\u00a0al. introduces two self-supervised objectives: Masked Language Model and Next Sentence Prediction. MLM randomly replaces a subset of tokens by a special token (e.g., ), and asks the language model to predict them. The training objective is the cross-entropy loss between the original tokens and the predicted ones. Typically, 15% of the input tokens are chosen, among which a random 80% are replaced by , 10% are left unchanged, and 10% are randomly replaced by a token from the vocabulary. NSP is a binary classification task that determines for a given sentence pair whether one sentence follows the other in the original text. While MLM is undoubtedly essential for BERT pretraining, the utility of NSP has been called into question in prior work. As such, we conduct ablation studies to probe how NSP and the use of segment IDs in pretraining might impact downstream fine-tuning performance.",
        "Aside from standard BERT pretraining objectives, we also consider ELECTRA, which has shown good performance in general-domain datasets such as GLUE and SQuAD. ELECTRA introduces an MLM-based generator to help pretrain a discriminator for use in end tasks. Specifically, given sample masked positions, first the generator predicts the most likely original tokens as in MLM, then the discriminator classifies, for all tokens, whether each is the original one. While ELECTRA shares some superficial similarity with generative adversarial network GAN, the roles of generator and discriminator are very different. After pretraining, the generator in ELECTRA is discarded and the discriminator is used for downstream fine-tuning, whereas GAN typically discards the discriminator and uses the generator. The training objective is not adversarial, but a weighted combination of MLM for the generator and classification accuracy for the discriminator. By classifying on all tokens rather than just the masked ones, ELECTRA can potentially learn more from each example while adding little overhead as the majority of compute lies in transformer layers before classification. The generator, on the other hand, does incur additional compute. Also, if the generator becomes very accurate early on, there will be little learning signal for the discriminator. Therefore, ELECTRA typically uses lower capacity in the generator compared with the discriminator (e.g., one-third in  and one-fourth in  for contextual representation dimension and attention head number).",
        "The study of neural language model pretraining originates in the general domain, including newswire and web. For example, the original BERT model was pretrained on Wikipedia and BooksCorpus. RoBERTa, another representative BERT model, was pretrained on a larger web corpus. Biomedical text is quite different from general-domain text and domain-specific pretraining has been shown to substantially improve performance in biomedical NLP applications. In particular, Gu et\u00a0al. conducted a thorough analysis on domain-specific pretraining, which highlights the utility of using a domain-specific vocabulary and pretraining on domain-specific text from scratch. We build on their work and study how domain-specific pretraining might impact fine-tuning stability, especially for larger models and/or with alternative pretraining settings. To facilitate our investigation, we pretrained PubMedBERT- and PubMedELECTRA ( and ) following the same setting of PubMedBERT  in Gu et\u00a0al. All  models use 12 layers, 768-dimension latent vectors, and 12 attention heads, with 110 million parameters. All  models use 24 layers, 1,024-dimension latent vectors, and 16 attention heads, with 336 million parameters.",
        "Prior work studying fine-tuning stability and mitigation methods tends to focus on the general domain\u2014e.g., using BERT models pretrained on general-domain corpora and evaluating on GLUE or SuperGLUE. Table\u00a01 summarizes representative recent work and common stabilization techniques. Small adjustments to the conventional optimization process may have surprisingly significant effect. For example, Mosbach et\u00a0al. and Zhang et\u00a0al. show that simply training for a longer time helps reduce fine-tuning instability with small training datasets. They also show that bias correction, which was proposed in the original ADAM algorithm but was not used in fine-tuning from the original BERT paper, can enhance fine-tuning stability by effectively reducing learning rates in the first few iterations.",
        "Such minor adaptations are already highly effective for general-domain applications. However, biomedical datasets are often much smaller than their\u00a0general-domain counterparts. For example, as aforementioned for text similarity, the biomedical dataset BIOSSES is much smaller than the general-domain dataset STS. Similarly, the question-answering datasets in BLURB have only a few hundred instances, compared with more than 100,000 in SQuAD.",
        "Therefore, we systematically study advanced layer-specific adaptation techniques previously studied in the general domains: freezing pretrained parameters in the lower layers, adopting layerwise learning-rate decay, and reinitializing parameters in the top layers. See Figure\u00a01. Essentially, these techniques represent various ways to alleviate the vanishing gradient problem in training deep neural networks, where optimization suffers from severe ill conditioning and requires adapting learning rates for individual layers. Interestingly, we find that their efficacy may interact with the pretraining setting and the end task.",
        "longer training: use up to 100 epochs in fine-tuning (vs. up to five epochs in standard setting)",
        "ADAM debiasing: adopt bias correction in ADAM during fine-tuning",
        "layer freeze: fix pretrained parameters in the lower half layers of BERT models during fine-tuning (six layers for  models and 12 for  models)",
        "layerwise decay: adopt layerwise learning-rate decay during fine-tuning (we follow ELECTRA implementation and use 0.8 and 0.9 as possible hyperparameters for learning-rate decay factors)",
        "layer reinit: randomly reinitialize parameters in the top layers before fine-tuning (up to three layers for  models and up to six for  models)",
        "Below is a list of all the methods we have explored with more details."
    ],
    "title": "Fine-tuning large neural language models for biomedical natural language processing"
}